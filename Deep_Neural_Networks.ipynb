{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jigjid/github_task/blob/main/Deep_Neural_Networks.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Preprocessing"
      ],
      "metadata": {
        "id": "xocPLPMsMzd3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np \n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from keras.datasets import mnist\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.metrics import accuracy_score\n",
        "import math"
      ],
      "metadata": {
        "id": "Yw6IPM0dM6UJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class tanh():\n",
        "  \"\"\"\n",
        "  Activation function - tangent\n",
        "  \"\"\"\n",
        "  def __init__(self):\n",
        "      pass\n",
        "  \n",
        "  def forward(self, A):\n",
        "    self.A = A\n",
        "    self.Z = np.tanh(self.A)\n",
        "\n",
        "    return self.Z\n",
        "\n",
        "  def backward(self, dZ):\n",
        "    ret = dZ * (1-self.Z**2)\n",
        "    return \n",
        "\n",
        "\n",
        "\n",
        "class sigmoid():\n",
        "  \"\"\"\n",
        "  Activation function - sigmoid\n",
        "  \"\"\"\n",
        "  def __init__(self):\n",
        "      pass\n",
        "  \n",
        "  def forward(self, A):\n",
        "    self.A = A\n",
        "    self.Z = 1/(1 + np.exp(-self.A))\n",
        "\n",
        "    return self.Z\n",
        "\n",
        "  def backward(self, dZ):\n",
        "    ret = dZ * (1-self.Z)*self.Z\n",
        "    return ret"
      ],
      "metadata": {
        "id": "Qy4fia7vM7ia"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class GetMiniBatch:\n",
        "    \"\"\"\n",
        "    Iterator to get a mini-batch\n",
        "    Parameters\n",
        "    ----------\n",
        "    X : ndarray, shape (n_samples, n_features)\n",
        "      training data\n",
        "    y : ndarray, shape (n_samples, 1)\n",
        "      Label of training data\n",
        "    batch_size : int\n",
        "      batch size\n",
        "    seed : int\n",
        "      NumPy random seed\n",
        "    \"\"\"\n",
        "    def __init__(self, X, y, batch_size = 20, seed=0):\n",
        "        self.batch_size = batch_size\n",
        "        np.random.seed(seed)\n",
        "        shuffle_index = np.random.permutation(np.arange(X.shape[0]))\n",
        "        self._X = X[shuffle_index]\n",
        "        self._y = y[shuffle_index]\n",
        "        self._stop = np.ceil(X.shape[0]/self.batch_size).astype(np.int64)\n",
        "\n",
        "    def __len__(self):\n",
        "        return self._stop\n",
        "\n",
        "    def __getitem__(self,item):\n",
        "        p0 = item*self.batch_size\n",
        "        p1 = item*self.batch_size + self.batch_size\n",
        "        return self._X[p0:p1], self._y[p0:p1]        \n",
        "\n",
        "    def __iter__(self):\n",
        "        self._counter = 0\n",
        "        return self\n",
        "\n",
        "    def __next__(self):\n",
        "        if self._counter >= self._stop:\n",
        "            raise StopIteration()\n",
        "        p0 = self._counter*self.batch_size\n",
        "        p1 = self._counter*self.batch_size + self.batch_size\n",
        "        self._counter += 1\n",
        "        return self._X[p0:p1], self._y[p0:p1]"
      ],
      "metadata": {
        "id": "KD-61oSHNF6Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**【Problem 1】Classification of fully connected layers**"
      ],
      "metadata": {
        "id": "e2EQPiDM6jTb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class FC:\n",
        "    \"\"\"\n",
        "    Number of nodes Full connection layer from n_nodes1 to n_nodes2\n",
        "    Parameters\n",
        "    ----------\n",
        "    n_nodes1 : int\n",
        "      Number of nodes in the previous layer\n",
        "    n_nodes2 : int\n",
        "      Number of nodes in the next layer\n",
        "    initializer : Instance of initialization method\n",
        "    optimizer : Instance of optimization method\n",
        "    \"\"\"\n",
        "    def __init__(self, n_nodes1, n_nodes2, initializer, optimizer, activation):\n",
        "        self.optimizer = optimizer\n",
        "        self.initializer = initializer\n",
        "        self.n_nodes1 = n_nodes1\n",
        "        self.n_nodes2 = n_nodes2\n",
        "        self.activation = activation\n",
        "        # Initialization\n",
        "        # Initialize self.W and self.B using the #initialr method\n",
        "\n",
        "        self.W = self.initializer.W(self.n_nodes1, self.n_nodes2)\n",
        "        self.B = self.initializer.B(self.n_nodes2)\n",
        "        \n",
        "    def forward(self, X):\n",
        "        \"\"\"\n",
        "        forward propagation\n",
        "        Parameters\n",
        "        ----------\n",
        "        X : ndarray, shape (batch_size, n_nodes1)\n",
        "            input\n",
        "        Returns\n",
        "        ----------\n",
        "        A : のndarray, shape (batch_size, n_nodes2)\n",
        "            output\n",
        "        \"\"\"        \n",
        "        self.X = X\n",
        "        self.A = np.dot(self.X, self.W) + self.B\n",
        "        A = self.activation.forward(self.A)\n",
        "        return A\n",
        "    def backward(self, dA):\n",
        "        \"\"\"\n",
        "        backward propagation\n",
        "        Parameters\n",
        "        ----------\n",
        "        dA : ndarray, shape (batch_size, n_nodes2)\n",
        "            Gradient descent from behind\n",
        "        Returns\n",
        "        ----------\n",
        "        dZ : ndarray, shape (batch_size, n_nodes1)\n",
        "            Gradient descent forward\n",
        "        \"\"\"\n",
        "        dA = self.activation.backward(dA)\n",
        "        self.dB = np.mean(dA, axis = 0)\n",
        "        self.dW = np.dot(self.X.T, dA)/len(self.X)\n",
        "        dZ = np.dot(dA, self.W.T)\n",
        "         \n",
        "        self = self.optimizer.update(self)\n",
        "        return dZ\n"
      ],
      "metadata": {
        "id": "CAVyo-GCYAj5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**【Problem 2】Classification of initialization method**"
      ],
      "metadata": {
        "id": "u0eghI9O8r4i"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class SimpleInitializer:\n",
        "    \"\"\"\n",
        "    ガウス分布によるシンプルな初期化\n",
        "    Parameters\n",
        "    ----------\n",
        "    sigma : float\n",
        "      ガウス分布の標準偏差\n",
        "    \"\"\"\n",
        "    def __init__(self, sigma):\n",
        "        self.sigma = sigma\n",
        "\n",
        "    def W(self, n_nodes1, n_nodes2):\n",
        "        \"\"\"\n",
        "        Weight initialization\n",
        "        Parameters\n",
        "        ----------\n",
        "        n_nodes1 : int\n",
        "          Number of nodes in the previous layer\n",
        "        n_nodes2 : int\n",
        "          Number of nodes in the later layer\n",
        "        Returns\n",
        "        ----------\n",
        "        W : ndarray shape with (n_nodes1, n_nodes2)\n",
        "          weights of hidden layer\n",
        "        \"\"\"\n",
        "        W = self.sigma * np.random.randn(n_nodes1, n_nodes2)\n",
        "        return W\n",
        "\n",
        "    def B(self, n_nodes2):\n",
        "        \"\"\"\n",
        "        バイアスの初期化\n",
        "        Parameters\n",
        "        ----------\n",
        "        n_nodes2 : int\n",
        "          後の層のノード数\n",
        "        Returns\n",
        "        ----------\n",
        "        B : ndarray shape with (n_nodes2, 1)\n",
        "          bias of hidden layer\n",
        "        \"\"\"\n",
        "        B = np.zeros(n_nodes2)\n",
        "        return B"
      ],
      "metadata": {
        "id": "VnFY2-DV_0kV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Problem 3: Classification of optimization methods**"
      ],
      "metadata": {
        "id": "OF3KwSTyDGNV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class SGD:\n",
        "    \"\"\"\n",
        "    Stochastic gradient descent\n",
        "    Parameters\n",
        "    ----------\n",
        "    lr : learning rate\n",
        "    \"\"\"\n",
        "    def __init__(self, lr):\n",
        "        self.lr = lr\n",
        "\n",
        "    def update(self, layer):\n",
        "        \"\"\"\n",
        "        Update weights and biases for a layer\n",
        "        Parameters\n",
        "        ----------\n",
        "        layer : Instance of the layer before update\n",
        "        \"\"\"\n",
        "\n",
        "        layer.W -= self.lr*layer.dW\n",
        "        layer.B -= self.lr*layer.dB\n",
        "\n",
        "        return layer"
      ],
      "metadata": {
        "id": "D5_uPTTqDJwu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Problem 4: Classification of activation function**"
      ],
      "metadata": {
        "id": "yXdYBDgTE9Me"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class softmax():\n",
        "  \"\"\"\n",
        "  Activation function - softmax\n",
        "  \"\"\"\n",
        "  def __init__(self):\n",
        "      pass\n",
        "  \n",
        "  def forward(self, A):\n",
        "    #print(\"A:\", A)\n",
        "    #print(\"temp:\", np.exp(A - np.max(A)))\n",
        "    #print(\"forward temp:\", np.sum(np.exp(A-np.max(A))))\n",
        "    temp = np.exp(A - np.max(A))/np.sum(np.exp(A-np.max(A)), axis = 1, keepdims= True)\n",
        "    return temp\n",
        "\n",
        "  def backward(self, dZ):\n",
        "    return dZ"
      ],
      "metadata": {
        "id": "V9_Wzfn1E-1B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Problem 5: Creating a ReLU Class**"
      ],
      "metadata": {
        "id": "-qVFJr-_Fe4W"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class ReLU():\n",
        "  \"\"\"\n",
        "  Activation function - relu\n",
        "  \"\"\"\n",
        "  def __init__(self):\n",
        "      pass\n",
        "  \n",
        "  def forward(self, A):\n",
        "    self.A = A\n",
        "    temp = np.maximum(self.A, 0)\n",
        "    return temp\n",
        "\n",
        "  def backward(self, dZ):\n",
        "    ret = np.where(self.A>0, dZ, 0)\n",
        "    return ret"
      ],
      "metadata": {
        "id": "k-55F_BCFody"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Problem 6: Initial value of weight**"
      ],
      "metadata": {
        "id": "lcf3ib47Fe1Y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class XavierInitializer:\n",
        "    \"\"\"\n",
        "    Xavier initialization\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self):\n",
        "        pass\n",
        "\n",
        "    def W(self, n_nodes1, n_nodes2):\n",
        "        \"\"\"\n",
        "        Weight initialization\n",
        "        Parameters\n",
        "        ----------\n",
        "        n_nodes1 : int\n",
        "          Number of nodes in the previous layer\n",
        "        n_nodes2 : int\n",
        "          Number of nodes in the later layer\n",
        "        Returns\n",
        "        ----------\n",
        "        W : ndarray shape with (n_nodes1, n_nodes2)\n",
        "          weights of hidden layer\n",
        "        \"\"\"\n",
        "        W = np.random.randn(n_nodes1, n_nodes2)/np.sqrt(n_nodes1)\n",
        "        return W\n",
        "\n",
        "    def B(self, n_nodes2):\n",
        "        \"\"\"\n",
        "        bias initializer\n",
        "        Parameters\n",
        "        ----------\n",
        "        n_nodes2 : int\n",
        "          Number of nodes in the later layer\n",
        "        Returns\n",
        "        ----------\n",
        "        B : ndarray shape with (n_nodes2, 1)\n",
        "          bias of hidden layer\n",
        "        \"\"\"\n",
        "        B = np.zeros(n_nodes2)\n",
        "        return B\n",
        "\n",
        "class HeInitializer:\n",
        "    \"\"\"\n",
        "    He initialization\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self):\n",
        "        pass\n",
        "\n",
        "    def W(self, n_nodes1, n_nodes2):\n",
        "        \"\"\"\n",
        "        Weight initialization\n",
        "        Parameters\n",
        "        ----------\n",
        "        n_nodes1 : int\n",
        "          Number of nodes in the previous layer\n",
        "        n_nodes2 : int\n",
        "          Number of nodes in the later layer\n",
        "        Returns\n",
        "        ----------\n",
        "        W : ndarray shape with (n_nodes1, n_nodes2)\n",
        "          weights of hidden layer\n",
        "        \"\"\"\n",
        "        W = np.random.randn(n_nodes1, n_nodes2)/np.sqrt(2/n_nodes1)\n",
        "        return W\n",
        "\n",
        "    def B(self, n_nodes2):\n",
        "        \"\"\"\n",
        "        bias initializer\n",
        "        Parameters\n",
        "        ----------\n",
        "        n_nodes2 : int\n",
        "          Number of nodes in the later layer\n",
        "        Returns\n",
        "        ----------\n",
        "        B : ndarray shape with (n_nodes2, 1)\n",
        "          bias of hidden layer\n",
        "        \"\"\"\n",
        "        B = np.zeros(n_nodes2)\n",
        "        return B"
      ],
      "metadata": {
        "id": "d9ZWom2DGFzb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Problem 7: Optimization method**"
      ],
      "metadata": {
        "id": "2_T8pZV4HTSi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Adagrad:\n",
        "  \"\"\"\n",
        "  stochastic gradient descent method\n",
        "  Parameters\n",
        "  -----------\n",
        "  lr : learning rate\n",
        "  \"\"\"\n",
        "  def __init__(self, lr):\n",
        "      self.lr = lr\n",
        "      self.hW = 0\n",
        "      self.hB = 0\n",
        "  \n",
        "  def update(self,layer):\n",
        "    \"\"\"\n",
        "    Updating the weights and biases of a layer\n",
        "    Parameters\n",
        "    -----------\n",
        "    layer : Instance of the layer before the update\n",
        "    \"\"\"\n",
        "    self.hW += layer.dW * layer.dW\n",
        "    self.hB = layer.dB * layer.dB\n",
        "\n",
        "    layer.W -= self.lr * layer.dW/(np.sqrt(self.hW) + 1e-7)\n",
        "    layer.B -=self.lr * layer.dB/(np.sqrt(self.hB) + 1e-7)\n",
        "\n",
        "    return layer"
      ],
      "metadata": {
        "id": "OVdt5EyQHS2R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Problem 8: Completion of the class**"
      ],
      "metadata": {
        "id": "WiS6ld9IIEtd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class ScratchDeepNeuralNetrowkClassifier():\n",
        "    \"\"\"\n",
        "    A Simple three-layer neural network classifier\n",
        "    Parameters\n",
        "    ----------\n",
        "    Attributes\n",
        "    ----------\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, n_epoch = 50, n_features = 784, n_nodes1 = 400, n_nodes2 = 200, n_output = 10,\n",
        "                sigma = 0.01, n_batch = 20, learning_rate = 0.01, verbose = False):\n",
        "        # Record hyperparameters as attributes\n",
        "        self.verbose = verbose\n",
        "        self.n_epoch = n_epoch\n",
        "        self.n_features = n_features\n",
        "        self.n_nodes1 = n_nodes1\n",
        "        self.n_nodes2 = n_nodes2\n",
        "        self.n_output = n_output\n",
        "        self.sigma = sigma\n",
        "        self.n_batch = n_batch\n",
        "        self.lr = learning_rate\n",
        "        self.log_loss = np.zeros(self.n_epoch)\n",
        "    \n",
        "    def loss_function(self, y, yt):\n",
        "        delta = 1e-7\n",
        "        loss = -np.mean(yt*np.log(y + delta))\n",
        "        return loss\n",
        "\n",
        "    def fit(self, X, y):\n",
        "        \"\"\"\n",
        "        Train a neural network classifier.\n",
        "        Parameters\n",
        "        ----------\n",
        "        X : ndarray, shape (n_samples, n_features)\n",
        "            Features of training data\n",
        "        y : ndarray, shape (n_samples, )\n",
        "            Label of training data\n",
        "        X_val : ndarray, shape (n_samples, n_features)\n",
        "            Features of validation data\n",
        "        y_val : ndarray, shape (n_samples, )\n",
        "            Label of validation data\n",
        "        \"\"\"\n",
        "        #self.log_loss = []\n",
        "        # initialize weights\n",
        "        optimizer1 = Adagrad(self.lr)\n",
        "        optimizer2 = Adagrad(self.lr)\n",
        "        optimizer3 = Adagrad(self.lr)\n",
        "\n",
        "        initializer1 = XavierInitializer()\n",
        "        initializer2 = HeInitializer()\n",
        "        initializer3 = SimpleInitializer(self.sigma)\n",
        "\n",
        "        # the loss function for each epoch \n",
        "        \n",
        "\n",
        "        self.FC1 = FC(self.n_features, self.n_nodes1, initializer1, optimizer1,tanh())\n",
        "        self.FC2 = FC(self.n_nodes1, self.n_nodes2, initializer2, optimizer2,tanh())\n",
        "        self.FC3 = FC(self.n_nodes2, self.n_output, initializer3, optimizer3,softmax())\n",
        "\n",
        "        for epoch in range(self.n_epoch):\n",
        "            get_mini_batch = GetMiniBatch(X, y , batch_size = self.n_batch)\n",
        "            self.loss = 0\n",
        "\n",
        "            for mini_X_train , mini_y_train in get_mini_batch:\n",
        "                #forward propagation\n",
        "                #1st layer\n",
        "                self.z1 = self.FC1.forward(mini_X_train)\n",
        "                #2nd layer\n",
        "                self.z2 = self.FC2.forward(self.z1)\n",
        "                #3rd layer\n",
        "                self.z3 = self.FC3.forward(self.z2)\n",
        "\n",
        "                # back propagation\n",
        "                #3rd layer\n",
        "                self.dA3 = (self.z3 - mini_y_train)/self.n_batch\n",
        "                #2nd layer\n",
        "                self.dz2 = self.FC3.backward(self.dA3)\n",
        "                #1st layer\n",
        "                self.dz1 = self.FC2.backward(self.dz2)\n",
        "                self.dz0 = self.FC1.backward(self.dz1)\n",
        "\n",
        "                #backpropagation\n",
        "                self.loss += self.loss_function(self.z3, mini_y_train)\n",
        "            \n",
        "            # record loss function for each epoch\n",
        "            self.log_loss[epoch] = self.loss/len(get_mini_batch)\n",
        "\n",
        "            if self.verbose:\n",
        "                print('epoch:{} loss:{}'.format(epoch, self.loss/self.n_batch))        \n",
        "\n",
        "    def predict(self, X):\n",
        "        \"\"\"\n",
        "        Estimate using a neural network classifier.\n",
        "        Parameters\n",
        "        ----------\n",
        "        X : ndarray, shape (n_samples, n_features)\n",
        "            training sample\n",
        "        Returns\n",
        "        -------\n",
        "        pred:    ndarray, shape (n_samples, 1)\n",
        "            predicted result\n",
        "        \"\"\"\n",
        "\n",
        "        pred_z1 = self.FC1.forward(X)\n",
        "        pred_z2 = self.FC2.forward(pred_z1)\n",
        "        pred = np.argmax(self.FC3.forward(pred_z2), axis=1)\n",
        "        return pred\n"
      ],
      "metadata": {
        "id": "jRPBnh8sH96Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Problem 9:Learning and estimation**"
      ],
      "metadata": {
        "id": "EsixMMYAPBux"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "(X_train, y_train), (X_test, y_test) = mnist.load_data()\n",
        "\n",
        "print(\"X_train data shape: \", X_train.shape) # (60000, 28, 28)\n",
        "print(\"X_test data shape: \", X_test.shape) # (10000, 28, 28)\n",
        "\n",
        "X_train = X_train.reshape(-1, 784)\n",
        "X_test = X_test.reshape(-1, 784)\n",
        "\n",
        "print(\"X_train flatten data shape: \", X_train.shape) # (60000, 28, 28)\n",
        "print(\"X_test flatten data shape: \", X_test.shape) # (10000, 28, 28)\n",
        "\n",
        "# Preprocessing\n",
        "\n",
        "X_train = X_train.astype(np.float)\n",
        "X_test = X_test.astype(np.float)\n",
        "X_train /= 255\n",
        "X_test /= 255\n",
        "print('X_train max value:', X_train.max()) # 1.0\n",
        "print('X_train min value:', X_train.min()) # 0.0\n",
        "\n",
        "# the correct label is an integer from 0 to 9, but it is converted to a one-hot representation\n",
        "\n",
        "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.2)\n",
        "enc = OneHotEncoder(handle_unknown='ignore', sparse=False)\n",
        "y_train_one_hot = enc.fit_transform(y_train.reshape(-1,1))\n",
        "y_val_one_hot = enc.fit_transform(y_val.reshape(-1,1))\n",
        "print(\"Train dataset:\", X_train.shape) # (48000, 784)\n",
        "print(\"Validation dataset:\", X_val.shape) # (12000, 784)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VWDR4Q4EOzbV",
        "outputId": "ec11b4d1-7dda-457a-866d-2a56ae2392eb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "X_train data shape:  (60000, 28, 28)\n",
            "X_test data shape:  (10000, 28, 28)\n",
            "X_train flatten data shape:  (60000, 784)\n",
            "X_test flatten data shape:  (10000, 784)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-101-42cec8aa94c4>:14: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
            "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
            "  X_train = X_train.astype(np.float)\n",
            "<ipython-input-101-42cec8aa94c4>:15: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
            "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
            "  X_test = X_test.astype(np.float)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "X_train max value: 1.0\n",
            "X_train min value: 0.0\n",
            "Train dataset: (48000, 784)\n",
            "Validation dataset: (12000, 784)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/sklearn/preprocessing/_encoders.py:868: FutureWarning: `sparse` was renamed to `sparse_output` in version 1.2 and will be removed in 1.4. `sparse_output` is ignored unless you leave `sparse` to its default value.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/preprocessing/_encoders.py:868: FutureWarning: `sparse` was renamed to `sparse_output` in version 1.2 and will be removed in 1.4. `sparse_output` is ignored unless you leave `sparse` to its default value.\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "clf = ScratchDeepNeuralNetrowkClassifier(n_epoch = 20, n_features = 784, n_nodes1 = 400, n_nodes2 = 200, n_output = 10,\n",
        "                sigma = 0.01, n_batch = 20, learning_rate = 0.01, verbose = True)\n",
        "\n",
        "clf.fit(X_train, y_train_one_hot)\n",
        "pred = clf.predict(X_val)\n",
        "print(pred)\n",
        "\n",
        "acc = accuracy_score(y_val, pred)\n",
        "print(\"Accuracy:\", acc)\n",
        "\n",
        "fig = plt.subplots()\n",
        "plt.title(\"Loss learning graph\")\n",
        "plt.xlabel(\"epoch\")\n",
        "plt.ylabel('Loss')\n",
        "plt.plot(clf.log_loss)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "DBGr8bI8PMVt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "num = 36\n",
        "true_false = pred==y_val\n",
        "false_list = np.where(true_false==False)[0].astype(np.int)\n",
        "\n",
        "if false_list.shape[0] < num:\n",
        "    num = false_list.shape[0]\n",
        "fig = plt.figure(figsize=(6, 6))\n",
        "fig.subplots_adjust(left=0, right=0.8,  bottom=0, top=0.8, hspace=1, wspace=0.5)\n",
        "for i in range(num):\n",
        "    ax = fig.add_subplot(6, 6, i + 1, xticks=[], yticks=[])\n",
        "    ax.set_title(\"{} / {}\".format(pred[false_list[i]],y_val[false_list[i]]))\n",
        "    ax.imshow(X_val.reshape(-1,28,28)[false_list[i]], cmap='gray')"
      ],
      "metadata": {
        "id": "_8LfTJ__bOhz"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}