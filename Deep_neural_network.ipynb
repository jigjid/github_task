{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jigjid/github_task/blob/main/Deep_neural_network.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "0xLwcrxHZVys"
      },
      "outputs": [],
      "source": [
        "import numpy as np \n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from keras.datasets import mnist\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.metrics import accuracy_score, f1_score"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zC_CsyUESTvP"
      },
      "source": [
        "[Problem 1] Classification of fully connected layers\n",
        "Please classify the fully connected layer."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "W3yw-bAdarYN"
      },
      "outputs": [],
      "source": [
        "\n",
        "class FC:\n",
        "    \"\"\"\n",
        "    ノード数n_nodes1からn_nodes2への全結合層\n",
        "    Parameters\n",
        "    ----------\n",
        "    n_nodes1 : int\n",
        "      前の層のノード数\n",
        "    n_nodes2 : int\n",
        "      後の層のノード数\n",
        "    initializer : 初期化方法のインスタンス\n",
        "    optimizer : 最適化手法のインスタンス\n",
        "    \"\"\"\n",
        "    def __init__(self, n_nodes1, n_nodes2, initializer, optimizer,activation):\n",
        "        self.optimizer = optimizer\n",
        "        # 初期化\n",
        "        # initializerのメソッドを使い、self.Wとself.Bを初期化する\n",
        "        self.optimizer = optimizer\n",
        "        self.initializer = initializer\n",
        "        self.n_nodes1 = n_nodes1\n",
        "        self.n_nodes2 = n_nodes2\n",
        "        self.activation = activation\n",
        "        # Initialization\n",
        "        # Initialize self.W and self.B using the #initialr method\n",
        "\n",
        "        self.W = self.initializer.W(self.n_nodes1, self.n_nodes2)\n",
        "        self.B = self.initializer.B(self.n_nodes2)\n",
        "\n",
        "    def forward(self, X):\n",
        "        \"\"\"\n",
        "        フォワード\n",
        "        Parameters\n",
        "        ----------\n",
        "        X : 次の形のndarray, shape (batch_size, n_nodes1)\n",
        "            入力\n",
        "        Returns\n",
        "        ----------\n",
        "        A : 次の形のndarray, shape (batch_size, n_nodes2)\n",
        "            出力\n",
        "        \"\"\"     \n",
        "        self.X = X\n",
        "        self.A = np.dot(self.X, self.W) + self.B\n",
        "        A = self.activation.forward(self.A)\n",
        "        return A\n",
        "\n",
        "    def backward(self, dA):\n",
        "        \"\"\"\n",
        "        バックワード\n",
        "        Parameters\n",
        "        ----------\n",
        "        dA : 次の形のndarray, shape (batch_size, n_nodes2)\n",
        "            後ろから流れてきた勾配\n",
        "        Returns\n",
        "        ----------\n",
        "        dZ : 次の形のndarray, shape (batch_size, n_nodes1)\n",
        "            前に流す勾配\n",
        "        \"\"\"\n",
        "        dA = self.activation.backward(dA)\n",
        "        self.dB = np.mean(dA, axis = 0)\n",
        "        self.dW = np.dot(self.X.T, dA)/len(self.X)\n",
        "        dZ = np.dot(dA, self.W.T)\n",
        "         \n",
        "        self = self.optimizer.update(self)\n",
        "        return dZ\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_IMJHx7dU1As"
      },
      "source": [
        "[Problem 2] Classification of initialization method\n",
        "Classify the code that does the initialization."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "FdFZ2VrfawSt"
      },
      "outputs": [],
      "source": [
        "class SimpleInitializer:\n",
        "    \"\"\"\n",
        "    ガウス分布によるシンプルな初期化\n",
        "    Parameters\n",
        "    ----------\n",
        "    sigma : float\n",
        "      ガウス分布の標準偏差\n",
        "    \"\"\"\n",
        "    def __init__(self, sigma):\n",
        "        self.sigma = sigma\n",
        "    def W(self, n_nodes1, n_nodes2):\n",
        "        \"\"\"\n",
        "        重みの初期化\n",
        "        Parameters\n",
        "        ----------\n",
        "        n_nodes1 : int\n",
        "          前の層のノード数\n",
        "        n_nodes2 : int\n",
        "          後の層のノード数\n",
        "\n",
        "        Returns\n",
        "        ----------\n",
        "        W :\n",
        "        \"\"\"\n",
        "        W = self.sigma * np.random.randn(n_nodes1, n_nodes2)\n",
        "        return W\n",
        "        \n",
        "    def B(self, n_nodes2):\n",
        "        \"\"\"\n",
        "        バイアスの初期化\n",
        "        Parameters\n",
        "        ----------\n",
        "        n_nodes2 : int\n",
        "          後の層のノード数\n",
        "\n",
        "        Returns\n",
        "        ----------\n",
        "        B :\n",
        "        \"\"\"\n",
        "        B = np.zeros(n_nodes2)\n",
        "        return B"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yWX4-VpEU6DQ"
      },
      "source": [
        "[Problem 3] Classification of optimization methods Do a class of optimization methods."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "rQPFArR-U8x7"
      },
      "outputs": [],
      "source": [
        "class SGD:\n",
        "    \"\"\"\n",
        "    確率的勾配降下法\n",
        "    Parameters\n",
        "    ----------\n",
        "    lr : 学習率\n",
        "    \"\"\"\n",
        "    def __init__(self, lr):\n",
        "        self.lr = lr\n",
        "    def update(self, layer):\n",
        "        \"\"\"\n",
        "        ある層の重みやバイアスの更新\n",
        "        Parameters\n",
        "        ----------\n",
        "        layer : 更新前の層のインスタンス\n",
        "        \"\"\"\n",
        "        layer.W -= self.lr*layer.dW\n",
        "        layer.B -= self.lr*layer.dB\n",
        "\n",
        "        return layer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BvQq2NTHU9HS"
      },
      "source": [
        "[Problem 4] Classification of activation function Please classify the activation function."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "V3xcwbOrVA5f"
      },
      "outputs": [],
      "source": [
        "class softmax():\n",
        "  \"\"\"\n",
        "  Activation function - softmax\n",
        "  \"\"\"\n",
        "  def __init__(self):\n",
        "      pass\n",
        "  \n",
        "  def forward(self, A):\n",
        "    #print(\"A:\", A)\n",
        "    #print(\"temp:\", np.exp(A - np.max(A)))\n",
        "    #print(\"forward temp:\", np.sum(np.exp(A-np.max(A))))\n",
        "    temp = np.exp(A - np.max(A))/np.sum(np.exp(A-np.max(A)), axis = 1, keepdims= True)\n",
        "    return temp\n",
        "\n",
        "  def backward(self, dZ):\n",
        "    return dZ"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ww4836mOVBRj"
      },
      "source": [
        "[Problem 5] Create ReLU class Implement ReLU (Rectified Linear Unit), a currently commonly used activation function, as a ReLU class."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "NiFkU5omVFC-"
      },
      "outputs": [],
      "source": [
        "class ReLU():\n",
        "  \"\"\"\n",
        "  Activation function - relu\n",
        "  \"\"\"\n",
        "  def __init__(self):\n",
        "      pass\n",
        "  \n",
        "  def forward(self, A):\n",
        "    self.A = A\n",
        "    temp = np.maximum(self.A, 0)\n",
        "    return temp\n",
        "\n",
        "  def backward(self, dZ):\n",
        "    ret = np.where(self.A>0, dZ, 0)\n",
        "    return ret"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "0g0ptMsybJoN"
      },
      "outputs": [],
      "source": [
        "class tanh():\n",
        "  \"\"\"\n",
        "  Activation function - tangent\n",
        "  \"\"\"\n",
        "  def __init__(self):\n",
        "      pass\n",
        "  \n",
        "  def forward(self, A):\n",
        "    self.A = A\n",
        "    self.Z = np.tanh(self.A)\n",
        "\n",
        "    return self.Z\n",
        "\n",
        "  def backward(self, dZ):\n",
        "    ret = dZ * (1-self.Z**2)\n",
        "    return ret"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xuwSmUexVFbN"
      },
      "source": [
        "[Problem 6] Initial value of weight "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "8tzRlePMVJfs"
      },
      "outputs": [],
      "source": [
        "class XavierInitializer:\n",
        "    \"\"\"\n",
        "    Xavier initialization\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self):\n",
        "        pass\n",
        "\n",
        "    def W(self, n_nodes1, n_nodes2):\n",
        "        \"\"\"\n",
        "        Weight initialization\n",
        "        Parameters\n",
        "        ----------\n",
        "        n_nodes1 : int\n",
        "          Number of nodes in the previous layer\n",
        "        n_nodes2 : int\n",
        "          Number of nodes in the later layer\n",
        "\n",
        "        Returns\n",
        "        ----------\n",
        "        W : ndarray shape with (n_nodes1, n_nodes2)\n",
        "          weights of hidden layer\n",
        "        \"\"\"\n",
        "        W = np.random.randn(n_nodes1, n_nodes2)/np.sqrt(n_nodes1)\n",
        "        return W\n",
        "\n",
        "    def B(self, n_nodes2):\n",
        "        \"\"\"\n",
        "        bias initializer\n",
        "        Parameters\n",
        "        ----------\n",
        "        n_nodes2 : int\n",
        "          Number of nodes in the later layer\n",
        "\n",
        "        Returns\n",
        "        ----------\n",
        "        B : ndarray shape with (n_nodes2, 1)\n",
        "          bias of hidden layer\n",
        "        \"\"\"\n",
        "        B = np.zeros(n_nodes2)\n",
        "        return B\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "O29y5dSuXcAS"
      },
      "outputs": [],
      "source": [
        "class HeInitializer:\n",
        "    \"\"\"\n",
        "    He initialization\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self):\n",
        "        pass\n",
        "\n",
        "    def W(self, n_nodes1, n_nodes2):\n",
        "        \"\"\"\n",
        "        Weight initialization\n",
        "        Parameters\n",
        "        ----------\n",
        "        n_nodes1 : int\n",
        "          Number of nodes in the previous layer\n",
        "        n_nodes2 : int\n",
        "          Number of nodes in the later layer\n",
        "\n",
        "        Returns\n",
        "        ----------\n",
        "        W : ndarray shape with (n_nodes1, n_nodes2)\n",
        "          weights of hidden layer\n",
        "        \"\"\"\n",
        "        W = np.random.randn(n_nodes1, n_nodes2)/np.sqrt(2/n_nodes1)\n",
        "        return W\n",
        "\n",
        "    def B(self, n_nodes2):\n",
        "        \"\"\"\n",
        "        bias initializer\n",
        "        Parameters\n",
        "        ----------\n",
        "        n_nodes2 : int\n",
        "          Number of nodes in the later layer\n",
        "\n",
        "        Returns\n",
        "        ----------\n",
        "        B : ndarray shape with (n_nodes2, 1)\n",
        "          bias of hidden layer\n",
        "        \"\"\"\n",
        "        B = np.zeros(n_nodes2)\n",
        "        return B\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PEwgrRNNVJy8"
      },
      "source": [
        "[Problem 7] Optimization method "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "ISHGPxODVPKg"
      },
      "outputs": [],
      "source": [
        "class Adagrad:\n",
        "  \"\"\"\n",
        "  stochastic gradient descent method\n",
        "\n",
        "  Parameters\n",
        "  -----------\n",
        "  lr : learning rate\n",
        "  \"\"\"\n",
        "  def __init__(self, lr):\n",
        "      self.lr = lr\n",
        "      self.hW = 0\n",
        "      self.hB = 0\n",
        "  \n",
        "  def update(self,layer):\n",
        "    \"\"\"\n",
        "    Updating the weights and biases of a layer\n",
        "    Parameters\n",
        "    -----------\n",
        "    layer : Instance of the layer before the update\n",
        "    \"\"\"\n",
        "    self.hW += layer.dW * layer.dW\n",
        "    self.hB = layer.dB * layer.dB\n",
        "\n",
        "    layer.W -= self.lr * layer.dW/(np.sqrt(self.hW) + 1e-7)\n",
        "    layer.B -=self.lr * layer.dB/(np.sqrt(self.hB) + 1e-7)\n",
        "\n",
        "    return layer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "2jNL5QDobd_o"
      },
      "outputs": [],
      "source": [
        "class GetMiniBatch:\n",
        "    \"\"\"\n",
        "    Iterator to get a mini-batch\n",
        "    Parameters\n",
        "    ----------\n",
        "    X : ndarray, shape (n_samples, n_features)\n",
        "      training data\n",
        "    y : ndarray, shape (n_samples, 1)\n",
        "      Label of training data\n",
        "    batch_size : int\n",
        "      batch size\n",
        "    seed : int\n",
        "      NumPy random seed\n",
        "    \"\"\"\n",
        "    def __init__(self, X, y, batch_size = 20, seed=0):\n",
        "        self.batch_size = batch_size\n",
        "        np.random.seed(seed)\n",
        "        shuffle_index = np.random.permutation(np.arange(X.shape[0]))\n",
        "        self._X = X[shuffle_index]\n",
        "        self._y = y[shuffle_index]\n",
        "        self._stop = np.ceil(X.shape[0]/self.batch_size).astype(np.int64)\n",
        "\n",
        "    def __len__(self):\n",
        "        return self._stop\n",
        "\n",
        "    def __getitem__(self,item):\n",
        "        p0 = item*self.batch_size\n",
        "        p1 = item*self.batch_size + self.batch_size\n",
        "        return self._X[p0:p1], self._y[p0:p1]        \n",
        "\n",
        "    def __iter__(self):\n",
        "        self._counter = 0\n",
        "        return self\n",
        "\n",
        "    def __next__(self):\n",
        "        if self._counter >= self._stop:\n",
        "            raise StopIteration()\n",
        "        p0 = self._counter*self.batch_size\n",
        "        p1 = self._counter*self.batch_size + self.batch_size\n",
        "        self._counter += 1\n",
        "        return self._X[p0:p1], self._y[p0:p1]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E9K9BHm5VPYk"
      },
      "source": [
        "[Problem 8] Class Completion Complete the ScratchDeepNeuralNetrowkClassifier class for training and estimation with any configuration."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "GCk71TuRVSr_"
      },
      "outputs": [],
      "source": [
        "class ScratchDeepNeuralNetrowkClassifier():\n",
        "    \"\"\"\n",
        "    A Simple three-layer neural network classifier\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "\n",
        "    Attributes\n",
        "    ----------\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, n_epoch = 50, n_features = 784, n_nodes1 = 400, n_nodes2 = 200, n_output = 10,\n",
        "                sigma = 0.01, n_batch = 20, learning_rate = 0.01, verbose = False):\n",
        "        # Record hyperparameters as attributes\n",
        "        self.verbose = verbose\n",
        "        self.n_epoch = n_epoch\n",
        "        self.n_features = n_features\n",
        "        self.n_nodes1 = n_nodes1\n",
        "        self.n_nodes2 = n_nodes2\n",
        "        self.n_output = n_output\n",
        "        self.sigma = sigma\n",
        "        self.n_batch = n_batch\n",
        "        self.lr = learning_rate\n",
        "        self.log_loss = np.zeros(self.n_epoch)\n",
        "    \n",
        "    def loss_function(self, y, yt):\n",
        "        delta = 1e-7\n",
        "        return -np.mean(yt*np.log(y + delta))\n",
        "\n",
        "    def fit(self, X, y, X_val=None, y_val=None):\n",
        "        \"\"\"\n",
        "        Train a neural network classifier.\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        X : ndarray, shape (n_samples, n_features)\n",
        "            Features of training data\n",
        "        y : ndarray, shape (n_samples, )\n",
        "            Label of training data\n",
        "        X_val : ndarray, shape (n_samples, n_features)\n",
        "            Features of validation data\n",
        "        y_val : ndarray, shape (n_samples, )\n",
        "            Label of validation data\n",
        "        \"\"\"\n",
        "        #self.log_loss = []\n",
        "        # initialize weights\n",
        "        optimizer1 = Adagrad(self.lr)\n",
        "        optimizer2 = Adagrad(self.lr)\n",
        "        optimizer3 = Adagrad(self.lr)\n",
        "\n",
        "        initializer1 = XavierInitializer()\n",
        "        initializer2 = HeInitializer()\n",
        "        initializer3 = SimpleInitializer(self.sigma)\n",
        "\n",
        "        # the loss function for each epoch \n",
        "        \n",
        "\n",
        "        self.FC1 = FC(self.n_features, self.n_nodes1, initializer1, optimizer1,tanh())\n",
        "        self.FC2 = FC(self.n_nodes1, self.n_nodes2, initializer2, optimizer2,tanh())\n",
        "        self.FC3 = FC(self.n_nodes2, self.n_output, initializer3, optimizer3,softmax())\n",
        "\n",
        "        for epoch in range(self.n_epoch):\n",
        "            get_mini_batch = GetMiniBatch(X, y , batch_size = self.n_batch)\n",
        "            self.loss = 0\n",
        "\n",
        "            for mini_X_train , mini_y_train in get_mini_batch:\n",
        "                #forward propagation\n",
        "                #1st layer\n",
        "                self.z1 = self.FC1.forward(mini_X_train)\n",
        "                #2nd layer\n",
        "                self.z2 = self.FC2.forward(self.z1)\n",
        "                #3rd layer\n",
        "                self.z3 = self.FC3.forward(self.z2)\n",
        "\n",
        "                # back propagation\n",
        "                #3rd layer\n",
        "                self.dA3 = (self.z3 - mini_y_train)/self.n_batch\n",
        "                #2nd layer\n",
        "                self.dz2 = self.FC3.backward(self.dA3)\n",
        "                #1st layer\n",
        "                self.dz1 = self.FC2.backward(self.dz2)\n",
        "                self.dz0 = self.FC1.backward(self.dz1)\n",
        "\n",
        "                #backpropagation\n",
        "                self.loss += self.loss_function(self.z3, mini_y_train)\n",
        "            \n",
        "            # record loss function for each epoch\n",
        "            self.log_loss[epoch] = self.loss/len(get_mini_batch)\n",
        "\n",
        "            if self.verbose:\n",
        "                print('epoch:{} loss:{}'.format(epoch, self.loss/self.n_batch))        \n",
        "\n",
        "    def predict(self, X):\n",
        "        \"\"\"\n",
        "        Estimate using a neural network classifier.\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        X : ndarray, shape (n_samples, n_features)\n",
        "            training sample\n",
        "\n",
        "        Returns\n",
        "        -------\n",
        "        pred:    ndarray, shape (n_samples, 1)\n",
        "            predicted result\n",
        "        \"\"\"\n",
        "\n",
        "        pred_z1 = self.FC1.forward(X)\n",
        "        pred_z2 = self.FC2.forward(pred_z1)\n",
        "        pred = np.argmax(self.FC3.forward(pred_z2), axis=1)\n",
        "        return pred"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uRp2pPU4VTBd"
      },
      "source": [
        "[Problem 9] Learning and Estimation Create several networks with varying numbers of layers and activation functions. Then learn and estimate the MNIST data and calculate Accuracy."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "maqGoIMwVXQn",
        "outputId": "68b1517a-e754-4c79-8e40-b230b96b50ce"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\n",
            "11490434/11490434 [==============================] - 0s 0us/step\n",
            "X_train data shape:  (60000, 28, 28)\n",
            "X_test data shape:  (10000, 28, 28)\n",
            "X_train flatten data shape:  (60000, 784)\n",
            "X_test flatten data shape:  (10000, 784)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-13-9595c8a4a04c>:14: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
            "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
            "  X_train = X_train.astype(np.float)\n",
            "<ipython-input-13-9595c8a4a04c>:15: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
            "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
            "  X_test = X_test.astype(np.float)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "X_train max value: 1.0\n",
            "X_train min value: 0.0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/preprocessing/_encoders.py:868: FutureWarning: `sparse` was renamed to `sparse_output` in version 1.2 and will be removed in 1.4. `sparse_output` is ignored unless you leave `sparse` to its default value.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/preprocessing/_encoders.py:868: FutureWarning: `sparse` was renamed to `sparse_output` in version 1.2 and will be removed in 1.4. `sparse_output` is ignored unless you leave `sparse` to its default value.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train dataset: (48000, 784)\n",
            "Validation dataset: (12000, 784)\n",
            "epoch:0 loss:6.596722909827096\n",
            "epoch:1 loss:3.9678087105594892\n",
            "epoch:2 loss:3.458182657910398\n"
          ]
        }
      ],
      "source": [
        "(X_train, y_train), (X_test, y_test) = mnist.load_data()\n",
        "\n",
        "print(\"X_train data shape: \", X_train.shape) # (60000, 28, 28)\n",
        "print(\"X_test data shape: \", X_test.shape) # (10000, 28, 28)\n",
        "\n",
        "X_train = X_train.reshape(-1, 784)\n",
        "X_test = X_test.reshape(-1, 784)\n",
        "\n",
        "print(\"X_train flatten data shape: \", X_train.shape) # (60000, 28, 28)\n",
        "print(\"X_test flatten data shape: \", X_test.shape) # (10000, 28, 28)\n",
        "\n",
        "# Preprocessing\n",
        "\n",
        "X_train = X_train.astype(np.float)\n",
        "X_test = X_test.astype(np.float)\n",
        "X_train /= 255\n",
        "X_test /= 255\n",
        "print('X_train max value:', X_train.max()) # 1.0\n",
        "print('X_train min value:', X_train.min()) # 0.0\n",
        "\n",
        "# the correct label is an integer from 0 to 9, but it is converted to a one-hot representation\n",
        "\n",
        "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.2)\n",
        "enc = OneHotEncoder(handle_unknown='ignore', sparse=False)\n",
        "y_train_one_hot = enc.fit_transform(y_train.reshape(-1,1))\n",
        "y_val_one_hot = enc.fit_transform(y_val.reshape(-1,1))\n",
        "print(\"Train dataset:\", X_train.shape) # (48000, 784)\n",
        "print(\"Validation dataset:\", X_val.shape) # (12000, 784)\n",
        "\n",
        "clf = ScratchDeepNeuralNetrowkClassifier(n_epoch = 25, n_features = 784, n_nodes1 = 400, n_nodes2 = 200, n_output = 10,\n",
        "                sigma = 0.01, n_batch = 20, learning_rate = 0.01, verbose = True)\n",
        "\n",
        "clf.fit(X_train, y_train_one_hot)\n",
        "pred = clf.predict(X_val)\n",
        "print(pred)\n",
        "acc = accuracy_score(y_val, pred)\n",
        "print(\"Accuracy:\", acc)\n",
        "\n",
        "fig = plt.subplots()\n",
        "plt.title(\"Loss learning graph\")\n",
        "plt.xlabel(\"epoch\")\n",
        "plt.ylabel('Loss')\n",
        "plt.plot(clf.log_loss)\n",
        "plt.show()"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}