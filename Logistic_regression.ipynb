{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jigjid/github_task/blob/main/Logistic_regression.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "import matplotlib.colors\n",
        "import pickle as pkl"
      ],
      "metadata": {
        "id": "EHSi323Ys2yc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**Logistic regression scratch**"
      ],
      "metadata": {
        "id": "hZVDkT7QohA8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class ScratchLogisticRegression():\n",
        "    def __init__(self, num_iter, lr, bias, verbose,lam):\n",
        "        self.num_iter = num_iter\n",
        "        self.lr = lr\n",
        "        self.bias = bias\n",
        "        self.verbose = verbose\n",
        "        self.lam = lam\n",
        "        self.theta = np.array([])\n",
        "        self.loss = np.array([])\n",
        "        self.val_loss = np.array([])\n",
        "\n",
        "\n",
        "\n",
        "    \"\"\"-----------------------------\n",
        "      Problem 1: Assumption function\n",
        "    ------------------------------\"\"\"\n",
        "\n",
        "    def _sigmoid(self,y):\n",
        "        return 1 / (1 + np.exp(-y))\n",
        "\n",
        "    def _logistic_hypothesis(self, X):\n",
        "        pred = X @ self.theta\n",
        "        pred = self._sigmoid(pred)\n",
        "        return pred\n",
        "\n",
        "\n",
        "    \"\"\"-----------------------------\n",
        "      Problem 2: Steepest descent method\n",
        "    ------------------------------\"\"\"\n",
        "\n",
        "    def _gradient_descent(self, X, y):\n",
        "        m = X.shape[0]\n",
        "        n = X.shape[1]\n",
        "        pred = self._logistic_hypothesis(X)\n",
        "        for j in range(n):\n",
        "            gradient = 0\n",
        "            for i in range(m):\n",
        "                gradient += (pred[i] - y[i]) * X[i, j]\n",
        "            self.theta[j] = self.theta[j] - self.lr * ((gradient+self.lam*self.theta[j]) / m)\n",
        "\n",
        "\n",
        "\n",
        "    \"\"\"-----------------------------\n",
        "      Problem 3: Estimation\n",
        "    ------------------------------\"\"\"\n",
        "\n",
        "    def predict_proba(self, X):\n",
        "        if self.bias == True:\n",
        "          a = np.ones(X.shape[0]).reshape(X.shape[0], 1)\n",
        "          X = np.hstack([a, X])\n",
        "        pred = self._logistic_hypothesis(X)\n",
        "        return pred\n",
        "\n",
        "\n",
        "    def predict(self, X):\n",
        "        if self.bias == True:\n",
        "            a = np.ones(X.shape[0]).reshape(X.shape[0], 1)\n",
        "            X = np.hstack([a, X])\n",
        "        return  np.where(self._logistic_hypothesis(X) >= 0.5,1,0)\n",
        "\n",
        "    \n",
        "    \"\"\"-----------------------------\n",
        "      Problem 4: Objective function\n",
        "    ------------------------------\"\"\"\n",
        "\n",
        "    def _loss_func(self, pred, y):\n",
        "        error = 0\n",
        "        for i in range(y.shape[0]):\n",
        "            error += -np.sum(y[i] *  np.log(pred[i])+(1-y[i]) *  np.log(1-pred[i]))\n",
        "        loss = error / (y.shape[0])\n",
        "        loss = loss + np.sum(self.theta**2)*self.lam/(2 * y.shape[0])\n",
        "        return loss\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    def fit(self, X, y, X_val=None, y_val=None): \n",
        "        if self.bias == True:\n",
        "\t\t        bias = np.ones((X.shape[0], 1))\n",
        "\t\t        X = np.hstack((bias, X))\n",
        "\t\t        bias = np.ones((X_val.shape[0], 1))\n",
        "\t\t        X_val = np.hstack((bias, X_val))\n",
        "        self.theta = np.zeros(X.shape[1])\n",
        "        self.theta = self.theta.reshape(X.shape[1], 1)\n",
        "        for i in range(self.num_iter):\n",
        "            pred = self._logistic_hypothesis(X)\n",
        "            pred_val = self._logistic_hypothesis(X_val)\n",
        "            self._gradient_descent(X, y)\n",
        "            loss = self._loss_func(pred, y)\n",
        "            self.loss = np.append(self.loss, loss)\n",
        "            loss_val = self._loss_func(pred_val, y_val)\n",
        "            self.val_loss = np.append(self.val_loss, loss_val)\n",
        "            \n",
        "            if self.verbose == True:\n",
        "                print('{}the loss of learning the first time is {}'.format(i,loss))\n",
        "         \n"
      ],
      "metadata": {
        "id": "8k78_IHpdVWd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def decision_region(X,y,slr):\n",
        "    mesh_f0, mesh_f1  = np.meshgrid(\n",
        "        np.arange(np.min(X[:,0]), np.max(X[:,0]), 0.01), \n",
        "        np.arange(np.min(X[:,1]), np.max(X[:,1]), 0.01)\n",
        "    )\n",
        "    mesh = np.c_[np.ravel(mesh_f0),np.ravel(mesh_f1)]\n",
        "    print(\"mesh shape:{}\".format(mesh.shape))\n",
        "    y_pred = slr.predict(mesh).reshape(mesh_f0.shape)\n",
        "    plt.title('decision region')\n",
        "    plt.xlabel('feature0')\n",
        "    plt.ylabel('feature1')\n",
        "    plt.contourf(mesh_f0, mesh_f1, y_pred,cmap=matplotlib.colors.ListedColormap(['pink', 'skyblue']))\n",
        "    plt.contour(mesh_f0, mesh_f1, y_pred,colors='red')\n",
        "    plt.scatter(X[y==0][:, 0], X[y==0][:, 1],label='0')\n",
        "    plt.scatter(X[y==1][:, 0], X[y==1][:, 1],label='1')\n",
        "    plt.legend()\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "B3crrXtTwQzs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**Verification**\n",
        "\n",
        "###**Problem 5: Learning and estimation**"
      ],
      "metadata": {
        "id": "1FCpIExxseS2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "iris = load_iris()\n",
        "X = iris.data[:100,:]\n",
        "y = iris.target[:100]\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n",
        "\n",
        "scaler = StandardScaler()\n",
        "X_train = scaler.fit_transform(X_train)\n",
        "X_test = scaler.fit_transform(X_test)\n",
        "\n",
        "\n",
        "slr = ScratchLogisticRegression(num_iter=100, lr=0.1, bias=True, verbose=True, lam=0.1)\n",
        "slr.fit(X_train, y_train, X_test, y_test)\n",
        "pred = slr.predict(X_test)\n",
        "print(pred.shape)\n",
        "acc = accuracy_score(y_test, pred)\n",
        "precision = precision_score(y_test, pred)\n",
        "recall = recall_score(y_test, pred)\n",
        "\n",
        "print(\"Scratch logistic regression: Accuracy - {}, Precision - {}, Recall - {}\".format(acc, precision, recall))\n",
        "\n",
        "log_reg = LogisticRegression()\n",
        "log_reg.fit(X_train, y_train)\n",
        "\n",
        "pred = log_reg.predict(X_test)\n",
        "acc = accuracy_score(y_test, pred)\n",
        "precision = precision_score(y_test, pred)\n",
        "recall = recall_score(y_test, pred)\n",
        "print(\"logistic regression: Accuracy - {}, Precision - {}, Recall - {}\".format(acc, precision, recall))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jpnHA1LxsgKN",
        "outputId": "f9b11dcb-c94e-4115-8e5b-5a3a3532864f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0the loss of learning the first time is 0.6931516443666398\n",
            "1the loss of learning the first time is 0.6244475339679191\n",
            "2the loss of learning the first time is 0.5657338793416352\n",
            "3the loss of learning the first time is 0.5154324471091436\n",
            "4the loss of learning the first time is 0.47217083101294816\n",
            "5the loss of learning the first time is 0.4347858572851285\n",
            "6the loss of learning the first time is 0.40230775970313\n",
            "7the loss of learning the first time is 0.3739358640996475\n",
            "8the loss of learning the first time is 0.34901236555233944\n",
            "9the loss of learning the first time is 0.32699766577902983\n",
            "10the loss of learning the first time is 0.3074487582100008\n",
            "11the loss of learning the first time is 0.2900010355114283\n",
            "12the loss of learning the first time is 0.2743533420393862\n",
            "13the loss of learning the first time is 0.2602558659050301\n",
            "14the loss of learning the first time is 0.247500408712819\n",
            "15the loss of learning the first time is 0.23591259552125227\n",
            "16the loss of learning the first time is 0.22534564416689615\n",
            "17the loss of learning the first time is 0.21567537706800777\n",
            "18the loss of learning the first time is 0.20679621869915632\n",
            "19the loss of learning the first time is 0.198617973828315\n",
            "20the loss of learning the first time is 0.1910632244809633\n",
            "21the loss of learning the first time is 0.18406521810281223\n",
            "22the loss of learning the first time is 0.17756614673987597\n",
            "23the loss of learning the first time is 0.17151573853217877\n",
            "24the loss of learning the first time is 0.16587009960270657\n",
            "25the loss of learning the first time is 0.1605907575136117\n",
            "26the loss of learning the first time is 0.15564386766835442\n",
            "27the loss of learning the first time is 0.15099955200561782\n",
            "28the loss of learning the first time is 0.1466313455627027\n",
            "29the loss of learning the first time is 0.1425157313739409\n",
            "30the loss of learning the first time is 0.13863174801557715\n",
            "31the loss of learning the first time is 0.13496065714528938\n",
            "32the loss of learning the first time is 0.13148566079123802\n",
            "33the loss of learning the first time is 0.12819166006031202\n",
            "34the loss of learning the first time is 0.12506504846462527\n",
            "35the loss of learning the first time is 0.12209353429171864\n",
            "36the loss of learning the first time is 0.11926598743125123\n",
            "37the loss of learning the first time is 0.11657230686896257\n",
            "38the loss of learning the first time is 0.1140033057061456\n",
            "39the loss of learning the first time is 0.11155061109021538\n",
            "40the loss of learning the first time is 0.10920657687306494\n",
            "41the loss of learning the first time is 0.10696420716764228\n",
            "42the loss of learning the first time is 0.10481708926447202\n",
            "43the loss of learning the first time is 0.10275933461055511\n",
            "44the loss of learning the first time is 0.10078552675267531\n",
            "45the loss of learning the first time is 0.09889067531318375\n",
            "46the loss of learning the first time is 0.09707017520492348\n",
            "47the loss of learning the first time is 0.09531977040798854\n",
            "48the loss of learning the first time is 0.09363552172846011\n",
            "49the loss of learning the first time is 0.0920137780413451\n",
            "50the loss of learning the first time is 0.09045115058928356\n",
            "51the loss of learning the first time is 0.08894448996733313\n",
            "52the loss of learning the first time is 0.08749086547403626\n",
            "53the loss of learning the first time is 0.08608754655147857\n",
            "54the loss of learning the first time is 0.08473198607333507\n",
            "55the loss of learning the first time is 0.08342180527097187\n",
            "56the loss of learning the first time is 0.08215478011433117\n",
            "57the loss of learning the first time is 0.08092882898726228\n",
            "58the loss of learning the first time is 0.07974200151673383\n",
            "59the loss of learning the first time is 0.07859246843245125\n",
            "60the loss of learning the first time is 0.07747851234819964\n",
            "61the loss of learning the first time is 0.0763985193690739\n",
            "62the loss of learning the first time is 0.07535097143992836\n",
            "63the loss of learning the first time is 0.07433443936010695\n",
            "64the loss of learning the first time is 0.07334757639801609\n",
            "65the loss of learning the first time is 0.07238911244653154\n",
            "66the loss of learning the first time is 0.07145784866674858\n",
            "67the loss of learning the first time is 0.07055265257330222\n",
            "68the loss of learning the first time is 0.0696724535195169\n",
            "69the loss of learning the first time is 0.06881623854507625\n",
            "70the loss of learning the first time is 0.06798304855281652\n",
            "71the loss of learning the first time is 0.0671719747847044\n",
            "72the loss of learning the first time is 0.06638215557012399\n",
            "73the loss of learning the first time is 0.06561277332230986\n",
            "74the loss of learning the first time is 0.06486305176117728\n",
            "75the loss of learning the first time is 0.06413225334294503\n",
            "76the loss of learning the first time is 0.063419676878856\n",
            "77the loss of learning the first time is 0.06272465532700613\n",
            "78the loss of learning the first time is 0.062046553742814446\n",
            "79the loss of learning the first time is 0.06138476737502806\n",
            "80the loss of learning the first time is 0.06073871989537663\n",
            "81the loss of learning the first time is 0.0601078617510836\n",
            "82the loss of learning the first time is 0.059491668630424384\n",
            "83the loss of learning the first time is 0.05888964003240365\n",
            "84the loss of learning the first time is 0.05830129793242024\n",
            "85the loss of learning the first time is 0.057726185536502835\n",
            "86the loss of learning the first time is 0.05716386611734647\n",
            "87the loss of learning the first time is 0.05661392192596386\n",
            "88the loss of learning the first time is 0.056075953173293086\n",
            "89the loss of learning the first time is 0.0555495770765809\n",
            "90the loss of learning the first time is 0.055034426965794185\n",
            "91the loss of learning the first time is 0.05453015144570437\n",
            "92the loss of learning the first time is 0.054036413609646866\n",
            "93the loss of learning the first time is 0.05355289030128183\n",
            "94the loss of learning the first time is 0.05307927142097664\n",
            "95the loss of learning the first time is 0.05261525927369998\n",
            "96the loss of learning the first time is 0.052160567955562516\n",
            "97the loss of learning the first time is 0.051714922776361735\n",
            "98the loss of learning the first time is 0.051278059715692985\n",
            "99the loss of learning the first time is 0.050849724910375926\n",
            "(20, 1)\n",
            "Scratch logistic regression: Accuracy - 1.0, Precision - 1.0, Recall - 1.0\n",
            "logistic regression: Accuracy - 1.0, Precision - 1.0, Recall - 1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###**Problem 6**\n",
        "**Plot of learning curve**"
      ],
      "metadata": {
        "id": "TLPdBa3SvtZj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "plt.plot(slr.loss, label = 'Training loss')\n",
        "plt.plot(slr.val_loss, label = 'Validation loss')\n",
        "plt.legend()\n",
        "plt.xlabel('iteration')\n",
        "plt.ylabel('Loss')\n",
        "plt.title('Learning curve')\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 295
        },
        "id": "biz18S0hv74V",
        "outputId": "0b1ed00c-da52-4100-920a-86fc6382ab2c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###**Problem 7**\n",
        "**Visualization of decision area**"
      ],
      "metadata": {
        "id": "zNXGQRuYwbeu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X = iris.data[:100,:2]\n",
        "y = iris.target[:100]\n",
        "(X_train, X_test, y_train, y_test) = train_test_split(X, y, test_size=0.2)\n",
        "slr = ScratchLogisticRegression(num_iter=1000, lr=0.01, bias=True,verbose=True,lam = 0.1)\n",
        "slr.fit(X_train, y_train,X_test,y_test)\n",
        "\n",
        "decision_region(X_test, y_test, slr)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "9n_v1g-PwocK",
        "outputId": "65eda51e-3e52-4670-92dc-b8fb252f74b7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0the loss of learning the first time is 0.6931471870719085\n",
            "1the loss of learning the first time is 0.6921267909022221\n",
            "2the loss of learning the first time is 0.6911840270467304\n",
            "3the loss of learning the first time is 0.6903041601485964\n",
            "4the loss of learning the first time is 0.6894752840345735\n",
            "5the loss of learning the first time is 0.6886877800123623\n",
            "6the loss of learning the first time is 0.6879338780924643\n",
            "7the loss of learning the first time is 0.6872073019266126\n",
            "8the loss of learning the first time is 0.6865029816973621\n",
            "9the loss of learning the first time is 0.685816822075512\n",
            "10the loss of learning the first time is 0.6851455147521293\n",
            "11the loss of learning the first time is 0.6844863870200797\n",
            "12the loss of learning the first time is 0.6838372794920312\n",
            "13the loss of learning the first time is 0.6831964473570575\n",
            "14the loss of learning the first time is 0.6825624806476631\n",
            "15the loss of learning the first time is 0.6819342398571308\n",
            "16the loss of learning the first time is 0.6813108039503603\n",
            "17the loss of learning the first time is 0.6806914283804066\n",
            "18the loss of learning the first time is 0.6800755111829043\n",
            "19the loss of learning the first time is 0.6794625655921978\n",
            "20the loss of learning the first time is 0.6788521979230426\n",
            "21the loss of learning the first time is 0.6782440897039823\n",
            "22the loss of learning the first time is 0.6776379832439989\n",
            "23the loss of learning the first time is 0.677033669971799\n",
            "24the loss of learning the first time is 0.6764309810144218\n",
            "25the loss of learning the first time is 0.6758297795845879\n",
            "26the loss of learning the first time is 0.6752299548291335\n",
            "27the loss of learning the first time is 0.6746314168577922\n",
            "28the loss of learning the first time is 0.6740340927256072\n",
            "29the loss of learning the first time is 0.6734379231858633\n",
            "30the loss of learning the first time is 0.6728428600656253\n",
            "31the loss of learning the first time is 0.672248864144412\n",
            "32the loss of learning the first time is 0.6716559034394675\n",
            "33the loss of learning the first time is 0.671063951819652\n",
            "34the loss of learning the first time is 0.670472987884926\n",
            "35the loss of learning the first time is 0.6698829940605137\n",
            "36the loss of learning the first time is 0.6692939558645937\n",
            "37the loss of learning the first time is 0.6687058613162556\n",
            "38the loss of learning the first time is 0.6681187004568535\n",
            "39the loss of learning the first time is 0.6675324649630169\n",
            "40the loss of learning the first time is 0.666947147833776\n",
            "41the loss of learning the first time is 0.6663627431375913\n",
            "42the loss of learning the first time is 0.6657792458078216\n",
            "43the loss of learning the first time is 0.6651966514773544\n",
            "44the loss of learning the first time is 0.6646149563448943\n",
            "45the loss of learning the first time is 0.6640341570668542\n",
            "46the loss of learning the first time is 0.663454250669942\n",
            "47the loss of learning the first time is 0.6628752344804824\n",
            "48the loss of learning the first time is 0.662297106067272\n",
            "49the loss of learning the first time is 0.6617198631953749\n",
            "50the loss of learning the first time is 0.6611435037887633\n",
            "51the loss of learning the first time is 0.6605680259001128\n",
            "52the loss of learning the first time is 0.6599934276863813\n",
            "53the loss of learning the first time is 0.6594197073890649\n",
            "54the loss of learning the first time is 0.6588468633182352\n",
            "55the loss of learning the first time is 0.6582748938396342\n",
            "56the loss of learning the first time is 0.6577037973642417\n",
            "57the loss of learning the first time is 0.6571335723398417\n",
            "58the loss of learning the first time is 0.6565642172442059\n",
            "59the loss of learning the first time is 0.655995730579584\n",
            "60the loss of learning the first time is 0.6554281108682477\n",
            "61the loss of learning the first time is 0.6548613566488984\n",
            "62the loss of learning the first time is 0.6542954664737509\n",
            "63the loss of learning the first time is 0.6537304389061909\n",
            "64the loss of learning the first time is 0.6531662725188729\n",
            "65the loss of learning the first time is 0.6526029658921906\n",
            "66the loss of learning the first time is 0.6520405176130367\n",
            "67the loss of learning the first time is 0.6514789262738054\n",
            "68the loss of learning the first time is 0.6509181904715866\n",
            "69the loss of learning the first time is 0.6503583088075134\n",
            "70the loss of learning the first time is 0.6497992798862409\n",
            "71the loss of learning the first time is 0.6492411023155218\n",
            "72the loss of learning the first time is 0.648683774705866\n",
            "73the loss of learning the first time is 0.6481272956702688\n",
            "74the loss of learning the first time is 0.6475716638239893\n",
            "75the loss of learning the first time is 0.6470168777843744\n",
            "76the loss of learning the first time is 0.6464629361707186\n",
            "77the loss of learning the first time is 0.6459098376041479\n",
            "78the loss of learning the first time is 0.6453575807075324\n",
            "79the loss of learning the first time is 0.6448061641054138\n",
            "80the loss of learning the first time is 0.6442555864239471\n",
            "81the loss of learning the first time is 0.6437058462908571\n",
            "82the loss of learning the first time is 0.6431569423354025\n",
            "83the loss of learning the first time is 0.6426088731883494\n",
            "84the loss of learning the first time is 0.6420616374819474\n",
            "85the loss of learning the first time is 0.6415152338499182\n",
            "86the loss of learning the first time is 0.6409696609274387\n",
            "87the loss of learning the first time is 0.6404249173511358\n",
            "88the loss of learning the first time is 0.6398810017590777\n",
            "89the loss of learning the first time is 0.6393379127907727\n",
            "90the loss of learning the first time is 0.6387956490871638\n",
            "91the loss of learning the first time is 0.6382542092906313\n",
            "92the loss of learning the first time is 0.6377135920449882\n",
            "93the loss of learning the first time is 0.6371737959954872\n",
            "94the loss of learning the first time is 0.6366348197888168\n",
            "95the loss of learning the first time is 0.6360966620731058\n",
            "96the loss of learning the first time is 0.6355593214979272\n",
            "97the loss of learning the first time is 0.6350227967142993\n",
            "98the loss of learning the first time is 0.6344870863746902\n",
            "99the loss of learning the first time is 0.6339521891330181\n",
            "100the loss of learning the first time is 0.6334181036446607\n",
            "101the loss of learning the first time is 0.6328848285664531\n",
            "102the loss of learning the first time is 0.6323523625566947\n",
            "103the loss of learning the first time is 0.6318207042751522\n",
            "104the loss of learning the first time is 0.6312898523830633\n",
            "105the loss of learning the first time is 0.6307598055431408\n",
            "106the loss of learning the first time is 0.6302305624195761\n",
            "107the loss of learning the first time is 0.6297021216780434\n",
            "108the loss of learning the first time is 0.6291744819857039\n",
            "109the loss of learning the first time is 0.6286476420112083\n",
            "110the loss of learning the first time is 0.6281216004247024\n",
            "111the loss of learning the first time is 0.6275963558978287\n",
            "112the loss of learning the first time is 0.6270719071037337\n",
            "113the loss of learning the first time is 0.6265482527170664\n",
            "114the loss of learning the first time is 0.6260253914139867\n",
            "115the loss of learning the first time is 0.6255033218721672\n",
            "116the loss of learning the first time is 0.6249820427707962\n",
            "117the loss of learning the first time is 0.6244615527905821\n",
            "118the loss of learning the first time is 0.6239418506137574\n",
            "119the loss of learning the first time is 0.6234229349240813\n",
            "120the loss of learning the first time is 0.6229048044068423\n",
            "121the loss of learning the first time is 0.6223874577488638\n",
            "122the loss of learning the first time is 0.6218708936385071\n",
            "123the loss of learning the first time is 0.6213551107656732\n",
            "124the loss of learning the first time is 0.6208401078218077\n",
            "125the loss of learning the first time is 0.6203258834999014\n",
            "126the loss of learning the first time is 0.6198124364944982\n",
            "127the loss of learning the first time is 0.6192997655016952\n",
            "128the loss of learning the first time is 0.6187878692191446\n",
            "129the loss of learning the first time is 0.6182767463460604\n",
            "130the loss of learning the first time is 0.6177663955832187\n",
            "131the loss of learning the first time is 0.6172568156329623\n",
            "132the loss of learning the first time is 0.6167480051992023\n",
            "133the loss of learning the first time is 0.6162399629874239\n",
            "134the loss of learning the first time is 0.6157326877046845\n",
            "135the loss of learning the first time is 0.6152261780596218\n",
            "136the loss of learning the first time is 0.6147204327624546\n",
            "137the loss of learning the first time is 0.6142154505249827\n",
            "138the loss of learning the first time is 0.6137112300605964\n",
            "139the loss of learning the first time is 0.6132077700842723\n",
            "140the loss of learning the first time is 0.6127050693125803\n",
            "141the loss of learning the first time is 0.6122031264636855\n",
            "142the loss of learning the first time is 0.6117019402573498\n",
            "143the loss of learning the first time is 0.6112015094149359\n",
            "144the loss of learning the first time is 0.6107018326594084\n",
            "145the loss of learning the first time is 0.610202908715338\n",
            "146the loss of learning the first time is 0.6097047363089032\n",
            "147the loss of learning the first time is 0.609207314167893\n",
            "148the loss of learning the first time is 0.6087106410217097\n",
            "149the loss of learning the first time is 0.6082147156013696\n",
            "150the loss of learning the first time is 0.607719536639508\n",
            "151the loss of learning the first time is 0.6072251028703793\n",
            "152the loss of learning the first time is 0.6067314130298624\n",
            "153the loss of learning the first time is 0.6062384658554594\n",
            "154the loss of learning the first time is 0.6057462600862997\n",
            "155the loss of learning the first time is 0.6052547944631418\n",
            "156the loss of learning the first time is 0.6047640677283767\n",
            "157the loss of learning the first time is 0.604274078626029\n",
            "158the loss of learning the first time is 0.6037848259017587\n",
            "159the loss of learning the first time is 0.603296308302865\n",
            "160the loss of learning the first time is 0.6028085245782859\n",
            "161the loss of learning the first time is 0.602321473478602\n",
            "162the loss of learning the first time is 0.6018351537560397\n",
            "163the loss of learning the first time is 0.6013495641644702\n",
            "164the loss of learning the first time is 0.6008647034594125\n",
            "165the loss of learning the first time is 0.6003805703980375\n",
            "166the loss of learning the first time is 0.5998971637391675\n",
            "167the loss of learning the first time is 0.5994144822432786\n",
            "168the loss of learning the first time is 0.598932524672503\n",
            "169the loss of learning the first time is 0.5984512897906314\n",
            "170the loss of learning the first time is 0.597970776363112\n",
            "171the loss of learning the first time is 0.5974909831570567\n",
            "172the loss of learning the first time is 0.5970119089412396\n",
            "173the loss of learning the first time is 0.5965335524860983\n",
            "174the loss of learning the first time is 0.5960559125637381\n",
            "175the loss of learning the first time is 0.5955789879479331\n",
            "176the loss of learning the first time is 0.5951027774141249\n",
            "177the loss of learning the first time is 0.5946272797394287\n",
            "178the loss of learning the first time is 0.5941524937026312\n",
            "179the loss of learning the first time is 0.5936784180841936\n",
            "180the loss of learning the first time is 0.593205051666254\n",
            "181the loss of learning the first time is 0.5927323932326263\n",
            "182the loss of learning the first time is 0.5922604415688053\n",
            "183the loss of learning the first time is 0.5917891954619642\n",
            "184the loss of learning the first time is 0.5913186537009587\n",
            "185the loss of learning the first time is 0.5908488150763282\n",
            "186the loss of learning the first time is 0.590379678380295\n",
            "187the loss of learning the first time is 0.5899112424067685\n",
            "188the loss of learning the first time is 0.5894435059513446\n",
            "189the loss of learning the first time is 0.5889764678113071\n",
            "190the loss of learning the first time is 0.5885101267856302\n",
            "191the loss of learning the first time is 0.5880444816749781\n",
            "192the loss of learning the first time is 0.5875795312817069\n",
            "193the loss of learning the first time is 0.5871152744098664\n",
            "194the loss of learning the first time is 0.5866517098652001\n",
            "195the loss of learning the first time is 0.5861888364551462\n",
            "196the loss of learning the first time is 0.5857266529888413\n",
            "197the loss of learning the first time is 0.5852651582771172\n",
            "198the loss of learning the first time is 0.5848043511325051\n",
            "199the loss of learning the first time is 0.5843442303692359\n",
            "200the loss of learning the first time is 0.5838847948032402\n",
            "201the loss of learning the first time is 0.5834260432521512\n",
            "202the loss of learning the first time is 0.5829679745353029\n",
            "203the loss of learning the first time is 0.5825105874737339\n",
            "204the loss of learning the first time is 0.5820538808901857\n",
            "205the loss of learning the first time is 0.581597853609106\n",
            "206the loss of learning the first time is 0.5811425044566471\n",
            "207the loss of learning the first time is 0.5806878322606684\n",
            "208the loss of learning the first time is 0.5802338358507365\n",
            "209the loss of learning the first time is 0.5797805140581263\n",
            "210the loss of learning the first time is 0.5793278657158213\n",
            "211the loss of learning the first time is 0.5788758896585146\n",
            "212the loss of learning the first time is 0.5784245847226098\n",
            "213the loss of learning the first time is 0.5779739497462211\n",
            "214the loss of learning the first time is 0.577523983569174\n",
            "215the loss of learning the first time is 0.5770746850330061\n",
            "216the loss of learning the first time is 0.5766260529809685\n",
            "217the loss of learning the first time is 0.5761780862580244\n",
            "218the loss of learning the first time is 0.575730783710852\n",
            "219the loss of learning the first time is 0.5752841441878422\n",
            "220the loss of learning the first time is 0.5748381665391027\n",
            "221the loss of learning the first time is 0.574392849616455\n",
            "222the loss of learning the first time is 0.5739481922734371\n",
            "223the loss of learning the first time is 0.5735041933653025\n",
            "224the loss of learning the first time is 0.5730608517490218\n",
            "225the loss of learning the first time is 0.5726181662832827\n",
            "226the loss of learning the first time is 0.5721761358284895\n",
            "227the loss of learning the first time is 0.5717347592467648\n",
            "228the loss of learning the first time is 0.5712940354019483\n",
            "229the loss of learning the first time is 0.5708539631595997\n",
            "230the loss of learning the first time is 0.570414541386995\n",
            "231the loss of learning the first time is 0.5699757689531307\n",
            "232the loss of learning the first time is 0.5695376447287211\n",
            "233the loss of learning the first time is 0.5691001675862015\n",
            "234the loss of learning the first time is 0.5686633363997236\n",
            "235the loss of learning the first time is 0.5682271500451622\n",
            "236the loss of learning the first time is 0.5677916074001095\n",
            "237the loss of learning the first time is 0.5673567073438777\n",
            "238the loss of learning the first time is 0.5669224487575\n",
            "239the loss of learning the first time is 0.5664888305237291\n",
            "240the loss of learning the first time is 0.566055851527037\n",
            "241the loss of learning the first time is 0.5656235106536175\n",
            "242the loss of learning the first time is 0.5651918067913833\n",
            "243the loss of learning the first time is 0.5647607388299671\n",
            "244the loss of learning the first time is 0.564330305660723\n",
            "245the loss of learning the first time is 0.5639005061767248\n",
            "246the loss of learning the first time is 0.5634713392727657\n",
            "247the loss of learning the first time is 0.5630428038453591\n",
            "248the loss of learning the first time is 0.5626148987927386\n",
            "249the loss of learning the first time is 0.5621876230148581\n",
            "250the loss of learning the first time is 0.5617609754133899\n",
            "251the loss of learning the first time is 0.5613349548917266\n",
            "252the loss of learning the first time is 0.5609095603549802\n",
            "253the loss of learning the first time is 0.5604847907099818\n",
            "254the loss of learning the first time is 0.5600606448652812\n",
            "255the loss of learning the first time is 0.5596371217311462\n",
            "256the loss of learning the first time is 0.559214220219565\n",
            "257the loss of learning the first time is 0.558791939244242\n",
            "258the loss of learning the first time is 0.5583702777206013\n",
            "259the loss of learning the first time is 0.5579492345657834\n",
            "260the loss of learning the first time is 0.5575288086986465\n",
            "261the loss of learning the first time is 0.5571089990397657\n",
            "262the loss of learning the first time is 0.5566898045114327\n",
            "263the loss of learning the first time is 0.5562712240376555\n",
            "264the loss of learning the first time is 0.5558532565441583\n",
            "265the loss of learning the first time is 0.55543590095838\n",
            "266the loss of learning the first time is 0.5550191562094758\n",
            "267the loss of learning the first time is 0.554603021228314\n",
            "268the loss of learning the first time is 0.5541874949474784\n",
            "269the loss of learning the first time is 0.5537725763012655\n",
            "270the loss of learning the first time is 0.5533582642256857\n",
            "271the loss of learning the first time is 0.552944557658461\n",
            "272the loss of learning the first time is 0.5525314555390272\n",
            "273the loss of learning the first time is 0.5521189568085303\n",
            "274the loss of learning the first time is 0.5517070604098281\n",
            "275the loss of learning the first time is 0.5512957652874887\n",
            "276the loss of learning the first time is 0.5508850703877903\n",
            "277the loss of learning the first time is 0.5504749746587195\n",
            "278the loss of learning the first time is 0.5500654770499731\n",
            "279the loss of learning the first time is 0.5496565765129545\n",
            "280the loss of learning the first time is 0.5492482720007756\n",
            "281the loss of learning the first time is 0.5488405624682545\n",
            "282the loss of learning the first time is 0.5484334468719156\n",
            "283the loss of learning the first time is 0.5480269241699879\n",
            "284the loss of learning the first time is 0.5476209933224059\n",
            "285the loss of learning the first time is 0.5472156532908079\n",
            "286the loss of learning the first time is 0.5468109030385352\n",
            "287the loss of learning the first time is 0.5464067415306314\n",
            "288the loss of learning the first time is 0.5460031677338417\n",
            "289the loss of learning the first time is 0.5456001806166124\n",
            "290the loss of learning the first time is 0.5451977791490901\n",
            "291the loss of learning the first time is 0.5447959623031198\n",
            "292the loss of learning the first time is 0.5443947290522454\n",
            "293the loss of learning the first time is 0.5439940783717085\n",
            "294the loss of learning the first time is 0.5435940092384475\n",
            "295the loss of learning the first time is 0.5431945206310962\n",
            "296the loss of learning the first time is 0.542795611529983\n",
            "297the loss of learning the first time is 0.5423972809171316\n",
            "298the loss of learning the first time is 0.541999527776257\n",
            "299the loss of learning the first time is 0.5416023510927692\n",
            "300the loss of learning the first time is 0.5412057498537655\n",
            "301the loss of learning the first time is 0.5408097230480374\n",
            "302the loss of learning the first time is 0.5404142696660632\n",
            "303the loss of learning the first time is 0.5400193887000106\n",
            "304the loss of learning the first time is 0.5396250791437344\n",
            "305the loss of learning the first time is 0.5392313399927755\n",
            "306the loss of learning the first time is 0.5388381702443606\n",
            "307the loss of learning the first time is 0.5384455688974007\n",
            "308the loss of learning the first time is 0.5380535349524891\n",
            "309the loss of learning the first time is 0.537662067411902\n",
            "310the loss of learning the first time is 0.5372711652795973\n",
            "311the loss of learning the first time is 0.5368808275612118\n",
            "312the loss of learning the first time is 0.536491053264061\n",
            "313the loss of learning the first time is 0.53610184139714\n",
            "314the loss of learning the first time is 0.5357131909711194\n",
            "315the loss of learning the first time is 0.5353251009983444\n",
            "316the loss of learning the first time is 0.5349375704928363\n",
            "317the loss of learning the first time is 0.534550598470289\n",
            "318the loss of learning the first time is 0.5341641839480681\n",
            "319the loss of learning the first time is 0.5337783259452104\n",
            "320the loss of learning the first time is 0.5333930234824232\n",
            "321the loss of learning the first time is 0.53300827558208\n",
            "322the loss of learning the first time is 0.532624081268225\n",
            "323the loss of learning the first time is 0.5322404395665653\n",
            "324the loss of learning the first time is 0.5318573495044738\n",
            "325the loss of learning the first time is 0.5314748101109881\n",
            "326the loss of learning the first time is 0.5310928204168067\n",
            "327the loss of learning the first time is 0.5307113794542897\n",
            "328the loss of learning the first time is 0.5303304862574569\n",
            "329the loss of learning the first time is 0.5299501398619869\n",
            "330the loss of learning the first time is 0.5295703393052149\n",
            "331the loss of learning the first time is 0.5291910836261315\n",
            "332the loss of learning the first time is 0.5288123718653834\n",
            "333the loss of learning the first time is 0.5284342030652683\n",
            "334the loss of learning the first time is 0.5280565762697381\n",
            "335the loss of learning the first time is 0.5276794905243932\n",
            "336the loss of learning the first time is 0.5273029448764845\n",
            "337the loss of learning the first time is 0.5269269383749091\n",
            "338the loss of learning the first time is 0.5265514700702122\n",
            "339the loss of learning the first time is 0.5261765390145824\n",
            "340the loss of learning the first time is 0.5258021442618528\n",
            "341the loss of learning the first time is 0.5254282848674979\n",
            "342the loss of learning the first time is 0.5250549598886337\n",
            "343the loss of learning the first time is 0.5246821683840148\n",
            "344the loss of learning the first time is 0.524309909414034\n",
            "345the loss of learning the first time is 0.5239381820407197\n",
            "346the loss of learning the first time is 0.5235669853277364\n",
            "347the loss of learning the first time is 0.5231963183403805\n",
            "348the loss of learning the first time is 0.5228261801455819\n",
            "349the loss of learning the first time is 0.5224565698119\n",
            "350the loss of learning the first time is 0.522087486409523\n",
            "351the loss of learning the first time is 0.5217189290102671\n",
            "352the loss of learning the first time is 0.5213508966875737\n",
            "353the loss of learning the first time is 0.5209833885165096\n",
            "354the loss of learning the first time is 0.520616403573763\n",
            "355the loss of learning the first time is 0.5202499409376451\n",
            "356the loss of learning the first time is 0.5198839996880855\n",
            "357the loss of learning the first time is 0.5195185789066321\n",
            "358the loss of learning the first time is 0.5191536776764503\n",
            "359the loss of learning the first time is 0.5187892950823194\n",
            "360the loss of learning the first time is 0.518425430210633\n",
            "361the loss of learning the first time is 0.5180620821493962\n",
            "362the loss of learning the first time is 0.5176992499882241\n",
            "363the loss of learning the first time is 0.5173369328183409\n",
            "364the loss of learning the first time is 0.5169751297325773\n",
            "365the loss of learning the first time is 0.5166138398253693\n",
            "366the loss of learning the first time is 0.5162530621927579\n",
            "367the loss of learning the first time is 0.5158927959323847\n",
            "368the loss of learning the first time is 0.5155330401434922\n",
            "369the loss of learning the first time is 0.5151737939269222\n",
            "370the loss of learning the first time is 0.514815056385113\n",
            "371the loss of learning the first time is 0.5144568266220984\n",
            "372the loss of learning the first time is 0.5140991037435065\n",
            "373the loss of learning the first time is 0.513741886856557\n",
            "374the loss of learning the first time is 0.5133851750700599\n",
            "375the loss of learning the first time is 0.5130289674944147\n",
            "376the loss of learning the first time is 0.5126732632416064\n",
            "377the loss of learning the first time is 0.5123180614252071\n",
            "378the loss of learning the first time is 0.5119633611603708\n",
            "379the loss of learning the first time is 0.5116091615638341\n",
            "380the loss of learning the first time is 0.5112554617539141\n",
            "381the loss of learning the first time is 0.5109022608505048\n",
            "382the loss of learning the first time is 0.5105495579750788\n",
            "383the loss of learning the first time is 0.5101973522506812\n",
            "384the loss of learning the first time is 0.5098456428019326\n",
            "385the loss of learning the first time is 0.5094944287550224\n",
            "386the loss of learning the first time is 0.5091437092377119\n",
            "387the loss of learning the first time is 0.5087934833793284\n",
            "388the loss of learning the first time is 0.5084437503107659\n",
            "389the loss of learning the first time is 0.5080945091644826\n",
            "390the loss of learning the first time is 0.5077457590744987\n",
            "391the loss of learning the first time is 0.507397499176395\n",
            "392the loss of learning the first time is 0.5070497286073116\n",
            "393the loss of learning the first time is 0.5067024465059445\n",
            "394the loss of learning the first time is 0.5063556520125458\n",
            "395the loss of learning the first time is 0.5060093442689202\n",
            "396the loss of learning the first time is 0.5056635224184246\n",
            "397the loss of learning the first time is 0.5053181856059644\n",
            "398the loss of learning the first time is 0.5049733329779933\n",
            "399the loss of learning the first time is 0.5046289636825114\n",
            "400the loss of learning the first time is 0.5042850768690619\n",
            "401the loss of learning the first time is 0.5039416716887303\n",
            "402the loss of learning the first time is 0.5035987472941434\n",
            "403the loss of learning the first time is 0.5032563028394659\n",
            "404the loss of learning the first time is 0.502914337480398\n",
            "405the loss of learning the first time is 0.5025728503741762\n",
            "406the loss of learning the first time is 0.5022318406795687\n",
            "407the loss of learning the first time is 0.5018913075568748\n",
            "408the loss of learning the first time is 0.5015512501679236\n",
            "409the loss of learning the first time is 0.5012116676760697\n",
            "410the loss of learning the first time is 0.5008725592461942\n",
            "411the loss of learning the first time is 0.5005339240447012\n",
            "412the loss of learning the first time is 0.5001957612395165\n",
            "413the loss of learning the first time is 0.4998580700000837\n",
            "414the loss of learning the first time is 0.4995208494973654\n",
            "415the loss of learning the first time is 0.4991840989038398\n",
            "416the loss of learning the first time is 0.4988478173934976\n",
            "417the loss of learning the first time is 0.49851200414184316\n",
            "418the loss of learning the first time is 0.49817665832588803\n",
            "419the loss of learning the first time is 0.49784177912415345\n",
            "420the loss of learning the first time is 0.49750736571666615\n",
            "421the loss of learning the first time is 0.497173417284956\n",
            "422the loss of learning the first time is 0.4968399330120557\n",
            "423the loss of learning the first time is 0.4965069120824972\n",
            "424the loss of learning the first time is 0.49617435368231166\n",
            "425the loss of learning the first time is 0.49584225699902423\n",
            "426the loss of learning the first time is 0.495510621221656\n",
            "427the loss of learning the first time is 0.4951794455407192\n",
            "428the loss of learning the first time is 0.4948487291482161\n",
            "429the loss of learning the first time is 0.4945184712376372\n",
            "430the loss of learning the first time is 0.49418867100396024\n",
            "431the loss of learning the first time is 0.4938593276436446\n",
            "432the loss of learning the first time is 0.49353044035463406\n",
            "433the loss of learning the first time is 0.4932020083363514\n",
            "434the loss of learning the first time is 0.49287403078969727\n",
            "435the loss of learning the first time is 0.4925465069170493\n",
            "436the loss of learning the first time is 0.49221943592225875\n",
            "437the loss of learning the first time is 0.49189281701064863\n",
            "438the loss of learning the first time is 0.49156664938901196\n",
            "439the loss of learning the first time is 0.4912409322656106\n",
            "440the loss of learning the first time is 0.4909156648501721\n",
            "441the loss of learning the first time is 0.4905908463538871\n",
            "442the loss of learning the first time is 0.4902664759894091\n",
            "443the loss of learning the first time is 0.489942552970851\n",
            "444the loss of learning the first time is 0.48961907651378395\n",
            "445the loss of learning the first time is 0.48929604583523434\n",
            "446the loss of learning the first time is 0.48897346015368354\n",
            "447the loss of learning the first time is 0.4886513186890628\n",
            "448the loss of learning the first time is 0.4883296206627541\n",
            "449the loss of learning the first time is 0.48800836529758707\n",
            "450the loss of learning the first time is 0.48768755181783763\n",
            "451the loss of learning the first time is 0.4873671794492232\n",
            "452the loss of learning the first time is 0.48704724741890426\n",
            "453the loss of learning the first time is 0.4867277549554804\n",
            "454the loss of learning the first time is 0.4864087012889877\n",
            "455the loss of learning the first time is 0.4860900856508988\n",
            "456the loss of learning the first time is 0.48577190727411845\n",
            "457the loss of learning the first time is 0.4854541653929833\n",
            "458the loss of learning the first time is 0.48513685924325817\n",
            "459the loss of learning the first time is 0.4848199880621358\n",
            "460the loss of learning the first time is 0.48450355108823223\n",
            "461the loss of learning the first time is 0.4841875475615884\n",
            "462the loss of learning the first time is 0.483871976723665\n",
            "463the loss of learning the first time is 0.4835568378173407\n",
            "464the loss of learning the first time is 0.48324213008691147\n",
            "465the loss of learning the first time is 0.48292785277808736\n",
            "466the loss of learning the first time is 0.48261400513799124\n",
            "467the loss of learning the first time is 0.48230058641515605\n",
            "468the loss of learning the first time is 0.4819875958595221\n",
            "469the loss of learning the first time is 0.48167503272243734\n",
            "470the loss of learning the first time is 0.48136289625665224\n",
            "471the loss of learning the first time is 0.4810511857163199\n",
            "472the loss of learning the first time is 0.48073990035699316\n",
            "473the loss of learning the first time is 0.48042903943562176\n",
            "474the loss of learning the first time is 0.4801186022105524\n",
            "475the loss of learning the first time is 0.4798085879415241\n",
            "476the loss of learning the first time is 0.4794989958896671\n",
            "477the loss of learning the first time is 0.479189825317502\n",
            "478the loss of learning the first time is 0.47888107548893555\n",
            "479the loss of learning the first time is 0.4785727456692599\n",
            "480the loss of learning the first time is 0.47826483512515\n",
            "481the loss of learning the first time is 0.47795734312466126\n",
            "482the loss of learning the first time is 0.4776502689372286\n",
            "483the loss of learning the first time is 0.4773436118336627\n",
            "484the loss of learning the first time is 0.4770373710861487\n",
            "485the loss of learning the first time is 0.4767315459682444\n",
            "486the loss of learning the first time is 0.4764261357548775\n",
            "487the loss of learning the first time is 0.4761211397223443\n",
            "488the loss of learning the first time is 0.47581655714830595\n",
            "489the loss of learning the first time is 0.4755123873117884\n",
            "490the loss of learning the first time is 0.47520862949317905\n",
            "491the loss of learning the first time is 0.4749052829742246\n",
            "492the loss of learning the first time is 0.4746023470380292\n",
            "493the loss of learning the first time is 0.4742998209690525\n",
            "494the loss of learning the first time is 0.4739977040531075\n",
            "495the loss of learning the first time is 0.4736959955773568\n",
            "496the loss of learning the first time is 0.4733946948303148\n",
            "497the loss of learning the first time is 0.4730938011018395\n",
            "498the loss of learning the first time is 0.4727933136831356\n",
            "499the loss of learning the first time is 0.47249323186674924\n",
            "500the loss of learning the first time is 0.47219355494656784\n",
            "501the loss of learning the first time is 0.47189428221781615\n",
            "502the loss of learning the first time is 0.47159541297705515\n",
            "503the loss of learning the first time is 0.47129694652218096\n",
            "504the loss of learning the first time is 0.4709988821524195\n",
            "505the loss of learning the first time is 0.4707012191683283\n",
            "506the loss of learning the first time is 0.4704039568717912\n",
            "507the loss of learning the first time is 0.4701070945660181\n",
            "508the loss of learning the first time is 0.4698106315555413\n",
            "509the loss of learning the first time is 0.469514567146216\n",
            "510the loss of learning the first time is 0.469218900645214\n",
            "511the loss of learning the first time is 0.4689236313610259\n",
            "512the loss of learning the first time is 0.46862875860345665\n",
            "513the loss of learning the first time is 0.4683342816836225\n",
            "514the loss of learning the first time is 0.46804019991395124\n",
            "515the loss of learning the first time is 0.46774651260817984\n",
            "516the loss of learning the first time is 0.4674532190813492\n",
            "517the loss of learning the first time is 0.46716031864980595\n",
            "518the loss of learning the first time is 0.4668678106311979\n",
            "519the loss of learning the first time is 0.46657569434447255\n",
            "520the loss of learning the first time is 0.46628396910987546\n",
            "521the loss of learning the first time is 0.4659926342489471\n",
            "522the loss of learning the first time is 0.4657016890845214\n",
            "523the loss of learning the first time is 0.4654111329407237\n",
            "524the loss of learning the first time is 0.46512096514296836\n",
            "525the loss of learning the first time is 0.46483118501795617\n",
            "526the loss of learning the first time is 0.4645417918936731\n",
            "527the loss of learning the first time is 0.46425278509938744\n",
            "528the loss of learning the first time is 0.46396416396564866\n",
            "529the loss of learning the first time is 0.4636759278242828\n",
            "530the loss of learning the first time is 0.46338807600839443\n",
            "531the loss of learning the first time is 0.4631006078523606\n",
            "532the loss of learning the first time is 0.4628135226918301\n",
            "533the loss of learning the first time is 0.46252681986372257\n",
            "534the loss of learning the first time is 0.4622404987062252\n",
            "535the loss of learning the first time is 0.4619545585587894\n",
            "536the loss of learning the first time is 0.4616689987621312\n",
            "537the loss of learning the first time is 0.4613838186582273\n",
            "538the loss of learning the first time is 0.46109901759031346\n",
            "539the loss of learning the first time is 0.4608145949028827\n",
            "540the loss of learning the first time is 0.46053054994168247\n",
            "541the loss of learning the first time is 0.46024688205371345\n",
            "542the loss of learning the first time is 0.45996359058722586\n",
            "543the loss of learning the first time is 0.45968067489171976\n",
            "544the loss of learning the first time is 0.4593981343179398\n",
            "545the loss of learning the first time is 0.45911596821787615\n",
            "546the loss of learning the first time is 0.45883417594476056\n",
            "547the loss of learning the first time is 0.4585527568530644\n",
            "548the loss of learning the first time is 0.45827171029849695\n",
            "549the loss of learning the first time is 0.45799103563800286\n",
            "550the loss of learning the first time is 0.4577107322297611\n",
            "551the loss of learning the first time is 0.45743079943318116\n",
            "552the loss of learning the first time is 0.457151236608902\n",
            "553the loss of learning the first time is 0.4568720431187893\n",
            "554the loss of learning the first time is 0.4565932183259349\n",
            "555the loss of learning the first time is 0.45631476159465234\n",
            "556the loss of learning the first time is 0.4560366722904761\n",
            "557the loss of learning the first time is 0.4557589497801595\n",
            "558the loss of learning the first time is 0.4554815934316727\n",
            "559the loss of learning the first time is 0.4552046026141993\n",
            "560the loss of learning the first time is 0.454927976698136\n",
            "561the loss of learning the first time is 0.45465171505508917\n",
            "562the loss of learning the first time is 0.45437581705787294\n",
            "563the loss of learning the first time is 0.4541002820805083\n",
            "564the loss of learning the first time is 0.45382510949821925\n",
            "565the loss of learning the first time is 0.4535502986874309\n",
            "566the loss of learning the first time is 0.45327584902576973\n",
            "567the loss of learning the first time is 0.4530017598920578\n",
            "568the loss of learning the first time is 0.45272803066631295\n",
            "569the loss of learning the first time is 0.45245466072974744\n",
            "570the loss of learning the first time is 0.4521816494647629\n",
            "571the loss of learning the first time is 0.4519089962549514\n",
            "572the loss of learning the first time is 0.45163670048509047\n",
            "573the loss of learning the first time is 0.4513647615411437\n",
            "574the loss of learning the first time is 0.45109317881025696\n",
            "575the loss of learning the first time is 0.45082195168075606\n",
            "576the loss of learning the first time is 0.45055107954214596\n",
            "577the loss of learning the first time is 0.45028056178510845\n",
            "578the loss of learning the first time is 0.45001039780149843\n",
            "579the loss of learning the first time is 0.4497405869843437\n",
            "580the loss of learning the first time is 0.44947112872784206\n",
            "581the loss of learning the first time is 0.44920202242735896\n",
            "582the loss of learning the first time is 0.4489332674794263\n",
            "583the loss of learning the first time is 0.4486648632817393\n",
            "584the loss of learning the first time is 0.44839680923315495\n",
            "585the loss of learning the first time is 0.4481291047336907\n",
            "586the loss of learning the first time is 0.44786174918451976\n",
            "587the loss of learning the first time is 0.4475947419879727\n",
            "588the loss of learning the first time is 0.44732808254753204\n",
            "589the loss of learning the first time is 0.4470617702678325\n",
            "590the loss of learning the first time is 0.4467958045546579\n",
            "591the loss of learning the first time is 0.4465301848149386\n",
            "592the loss of learning the first time is 0.44626491045675004\n",
            "593the loss of learning the first time is 0.4459999808893112\n",
            "594the loss of learning the first time is 0.445735395522982\n",
            "595the loss of learning the first time is 0.4454711537692604\n",
            "596the loss of learning the first time is 0.4452072550407821\n",
            "597the loss of learning the first time is 0.4449436987513161\n",
            "598the loss of learning the first time is 0.4446804843157657\n",
            "599the loss of learning the first time is 0.4444176111501634\n",
            "600the loss of learning the first time is 0.44415507867167103\n",
            "601the loss of learning the first time is 0.44389288629857593\n",
            "602the loss of learning the first time is 0.4436310334502909\n",
            "603the loss of learning the first time is 0.44336951954735027\n",
            "604the loss of learning the first time is 0.44310834401140897\n",
            "605the loss of learning the first time is 0.44284750626523955\n",
            "606the loss of learning the first time is 0.442587005732731\n",
            "607the loss of learning the first time is 0.4423268418388873\n",
            "608the loss of learning the first time is 0.4420670140098229\n",
            "609the loss of learning the first time is 0.4418075216727633\n",
            "610the loss of learning the first time is 0.4415483642560416\n",
            "611the loss of learning the first time is 0.44128954118909636\n",
            "612the loss of learning the first time is 0.44103105190247077\n",
            "613the loss of learning the first time is 0.44077289582780904\n",
            "614the loss of learning the first time is 0.4405150723978559\n",
            "615the loss of learning the first time is 0.4402575810464534\n",
            "616the loss of learning the first time is 0.44000042120853916\n",
            "617the loss of learning the first time is 0.4397435923201448\n",
            "618the loss of learning the first time is 0.43948709381839324\n",
            "619the loss of learning the first time is 0.43923092514149714\n",
            "620the loss of learning the first time is 0.43897508572875643\n",
            "621the loss of learning the first time is 0.4387195750205572\n",
            "622the loss of learning the first time is 0.4384643924583686\n",
            "623the loss of learning the first time is 0.43820953748474156\n",
            "624the loss of learning the first time is 0.4379550095433062\n",
            "625the loss of learning the first time is 0.43770080807876977\n",
            "626the loss of learning the first time is 0.4374469325369162\n",
            "627the loss of learning the first time is 0.4371933823646016\n",
            "628the loss of learning the first time is 0.4369401570097534\n",
            "629the loss of learning the first time is 0.4366872559213702\n",
            "630the loss of learning the first time is 0.43643467854951595\n",
            "631the loss of learning the first time is 0.43618242434532095\n",
            "632the loss of learning the first time is 0.435930492760979\n",
            "633the loss of learning the first time is 0.43567888324974496\n",
            "634the loss of learning the first time is 0.4354275952659332\n",
            "635the loss of learning the first time is 0.43517662826491543\n",
            "636the loss of learning the first time is 0.4349259817031189\n",
            "637the loss of learning the first time is 0.43467565503802463\n",
            "638the loss of learning the first time is 0.43442564772816383\n",
            "639the loss of learning the first time is 0.4341759592331184\n",
            "640the loss of learning the first time is 0.4339265890135172\n",
            "641the loss of learning the first time is 0.4336775365310343\n",
            "642the loss of learning the first time is 0.4334288012483885\n",
            "643the loss of learning the first time is 0.433180382629338\n",
            "644the loss of learning the first time is 0.4329322801386827\n",
            "645the loss of learning the first time is 0.43268449324225844\n",
            "646the loss of learning the first time is 0.43243702140693746\n",
            "647the loss of learning the first time is 0.4321898641006255\n",
            "648the loss of learning the first time is 0.4319430207922596\n",
            "649the loss of learning the first time is 0.43169649095180745\n",
            "650the loss of learning the first time is 0.43145027405026337\n",
            "651the loss of learning the first time is 0.43120436955964775\n",
            "652the loss of learning the first time is 0.4309587769530051\n",
            "653the loss of learning the first time is 0.43071349570440143\n",
            "654the loss of learning the first time is 0.430468525288923\n",
            "655the loss of learning the first time is 0.4302238651826737\n",
            "656the loss of learning the first time is 0.42997951486277336\n",
            "657the loss of learning the first time is 0.42973547380735644\n",
            "658the loss of learning the first time is 0.4294917414955691\n",
            "659the loss of learning the first time is 0.42924831740756714\n",
            "660the loss of learning the first time is 0.42900520102451595\n",
            "661the loss of learning the first time is 0.4287623918285857\n",
            "662the loss of learning the first time is 0.42851988930295165\n",
            "663the loss of learning the first time is 0.42827769293179196\n",
            "664the loss of learning the first time is 0.4280358022002841\n",
            "665the loss of learning the first time is 0.4277942165946054\n",
            "666the loss of learning the first time is 0.42755293560192903\n",
            "667the loss of learning the first time is 0.42731195871042277\n",
            "668the loss of learning the first time is 0.4270712854092479\n",
            "669the loss of learning the first time is 0.426830915188556\n",
            "670the loss of learning the first time is 0.42659084753948767\n",
            "671the loss of learning the first time is 0.4263510819541712\n",
            "672the loss of learning the first time is 0.42611161792571917\n",
            "673the loss of learning the first time is 0.4258724549482278\n",
            "674the loss of learning the first time is 0.42563359251677496\n",
            "675the loss of learning the first time is 0.4253950301274175\n",
            "676the loss of learning the first time is 0.4251567672771894\n",
            "677the loss of learning the first time is 0.42491880346410177\n",
            "678the loss of learning the first time is 0.42468113818713776\n",
            "679the loss of learning the first time is 0.42444377094625374\n",
            "680the loss of learning the first time is 0.42420670124237486\n",
            "681the loss of learning the first time is 0.423969928577395\n",
            "682the loss of learning the first time is 0.4237334524541751\n",
            "683the loss of learning the first time is 0.42349727237653817\n",
            "684the loss of learning the first time is 0.42326138784927186\n",
            "685the loss of learning the first time is 0.42302579837812326\n",
            "686the loss of learning the first time is 0.4227905034697984\n",
            "687the loss of learning the first time is 0.42255550263195985\n",
            "688the loss of learning the first time is 0.42232079537322587\n",
            "689the loss of learning the first time is 0.4220863812031665\n",
            "690the loss of learning the first time is 0.4218522596323045\n",
            "691the loss of learning the first time is 0.42161843017211115\n",
            "692the loss of learning the first time is 0.421384892335005\n",
            "693the loss of learning the first time is 0.42115164563435076\n",
            "694the loss of learning the first time is 0.42091868958445683\n",
            "695the loss of learning the first time is 0.4206860237005734\n",
            "696the loss of learning the first time is 0.4204536474988908\n",
            "697the loss of learning the first time is 0.42022156049653736\n",
            "698the loss of learning the first time is 0.41998976221157885\n",
            "699the loss of learning the first time is 0.41975825216301405\n",
            "700the loss of learning the first time is 0.4195270298707752\n",
            "701the loss of learning the first time is 0.4192960948557256\n",
            "702the loss of learning the first time is 0.4190654466396578\n",
            "703the loss of learning the first time is 0.4188350847452906\n",
            "704the loss of learning the first time is 0.418605008696269\n",
            "705the loss of learning the first time is 0.41837521801716165\n",
            "706the loss of learning the first time is 0.41814571223345837\n",
            "707the loss of learning the first time is 0.4179164908715693\n",
            "708the loss of learning the first time is 0.4176875534588224\n",
            "709the loss of learning the first time is 0.4174588995234625\n",
            "710the loss of learning the first time is 0.41723052859464843\n",
            "711the loss of learning the first time is 0.4170024402024519\n",
            "712the loss of learning the first time is 0.4167746338778554\n",
            "713the loss of learning the first time is 0.4165471091527506\n",
            "714the loss of learning the first time is 0.416319865559936\n",
            "715the loss of learning the first time is 0.41609290263311627\n",
            "716the loss of learning the first time is 0.4158662199068995\n",
            "717the loss of learning the first time is 0.41563981691679536\n",
            "718the loss of learning the first time is 0.41541369319921434\n",
            "719the loss of learning the first time is 0.4151878482914641\n",
            "720the loss of learning the first time is 0.41496228173174987\n",
            "721the loss of learning the first time is 0.41473699305917133\n",
            "722the loss of learning the first time is 0.4145119818137213\n",
            "723the loss of learning the first time is 0.41428724753628315\n",
            "724the loss of learning the first time is 0.41406278976863\n",
            "725the loss of learning the first time is 0.41383860805342315\n",
            "726the loss of learning the first time is 0.4136147019342092\n",
            "727the loss of learning the first time is 0.4133910709554187\n",
            "728the loss of learning the first time is 0.4131677146623653\n",
            "729the loss of learning the first time is 0.4129446326012426\n",
            "730the loss of learning the first time is 0.4127218243191235\n",
            "731the loss of learning the first time is 0.4124992893639575\n",
            "732the loss of learning the first time is 0.41227702728456983\n",
            "733the loss of learning the first time is 0.4120550376306594\n",
            "734the loss of learning the first time is 0.4118333199527965\n",
            "735the loss of learning the first time is 0.4116118738024214\n",
            "736the loss of learning the first time is 0.41139069873184375\n",
            "737the loss of learning the first time is 0.4111697942942391\n",
            "738the loss of learning the first time is 0.41094916004364795\n",
            "739the loss of learning the first time is 0.4107287955349733\n",
            "740the loss of learning the first time is 0.41050870032398123\n",
            "741the loss of learning the first time is 0.41028887396729596\n",
            "742the loss of learning the first time is 0.4100693160224004\n",
            "743the loss of learning the first time is 0.40985002604763365\n",
            "744the loss of learning the first time is 0.4096310036021896\n",
            "745the loss of learning the first time is 0.4094122482461142\n",
            "746the loss of learning the first time is 0.4091937595403053\n",
            "747the loss of learning the first time is 0.4089755370465097\n",
            "748the loss of learning the first time is 0.4087575803273222\n",
            "749the loss of learning the first time is 0.40853988894618315\n",
            "750the loss of learning the first time is 0.40832246246737774\n",
            "751the loss of learning the first time is 0.4081053004560335\n",
            "752the loss of learning the first time is 0.407888402478119\n",
            "753the loss of learning the first time is 0.4076717681004416\n",
            "754the loss of learning the first time is 0.40745539689064675\n",
            "755the loss of learning the first time is 0.40723928841721485\n",
            "756the loss of learning the first time is 0.40702344224946185\n",
            "757the loss of learning the first time is 0.4068078579575347\n",
            "758the loss of learning the first time is 0.4065925351124124\n",
            "759the loss of learning the first time is 0.406377473285902\n",
            "760the loss of learning the first time is 0.4061626720506389\n",
            "761the loss of learning the first time is 0.40594813098008303\n",
            "762the loss of learning the first time is 0.40573384964852005\n",
            "763the loss of learning the first time is 0.4055198276310567\n",
            "764the loss of learning the first time is 0.4053060645036211\n",
            "765the loss of learning the first time is 0.40509255984296033\n",
            "766the loss of learning the first time is 0.40487931322663917\n",
            "767the loss of learning the first time is 0.404666324233038\n",
            "768the loss of learning the first time is 0.40445359244135104\n",
            "769the loss of learning the first time is 0.40424111743158564\n",
            "770the loss of learning the first time is 0.4040288987845595\n",
            "771the loss of learning the first time is 0.40381693608190017\n",
            "772the loss of learning the first time is 0.403605228906042\n",
            "773the loss of learning the first time is 0.4033937768402257\n",
            "774the loss of learning the first time is 0.4031825794684963\n",
            "775the loss of learning the first time is 0.40297163637570094\n",
            "776the loss of learning the first time is 0.4027609471474892\n",
            "777the loss of learning the first time is 0.4025505113703086\n",
            "778the loss of learning the first time is 0.40234032863140495\n",
            "779the loss of learning the first time is 0.4021303985188204\n",
            "780the loss of learning the first time is 0.40192072062139206\n",
            "781the loss of learning the first time is 0.4017112945287487\n",
            "782the loss of learning the first time is 0.40150211983131223\n",
            "783the loss of learning the first time is 0.4012931961202923\n",
            "784the loss of learning the first time is 0.40108452298768843\n",
            "785the loss of learning the first time is 0.4008761000262853\n",
            "786the loss of learning the first time is 0.40066792682965363\n",
            "787the loss of learning the first time is 0.40046000299214646\n",
            "788the loss of learning the first time is 0.4002523281088997\n",
            "789the loss of learning the first time is 0.40004490177582797\n",
            "790the loss of learning the first time is 0.3998377235896257\n",
            "791the loss of learning the first time is 0.3996307931477636\n",
            "792the loss of learning the first time is 0.3994241100484879\n",
            "793the loss of learning the first time is 0.3992176738908188\n",
            "794the loss of learning the first time is 0.3990114842745486\n",
            "795the loss of learning the first time is 0.39880554080024\n",
            "796the loss of learning the first time is 0.39859984306922486\n",
            "797the loss of learning the first time is 0.39839439068360305\n",
            "798the loss of learning the first time is 0.39818918324623953\n",
            "799the loss of learning the first time is 0.39798422036076464\n",
            "800the loss of learning the first time is 0.3977795016315703\n",
            "801the loss of learning the first time is 0.39757502666381084\n",
            "802the loss of learning the first time is 0.3973707950633996\n",
            "803the loss of learning the first time is 0.3971668064370082\n",
            "804the loss of learning the first time is 0.39696306039206514\n",
            "805the loss of learning the first time is 0.3967595565367531\n",
            "806the loss of learning the first time is 0.39655629448000973\n",
            "807the loss of learning the first time is 0.3963532738315228\n",
            "808the loss of learning the first time is 0.3961504942017323\n",
            "809the loss of learning the first time is 0.3959479552018259\n",
            "810the loss of learning the first time is 0.39574565644373905\n",
            "811the loss of learning the first time is 0.39554359754015295\n",
            "812the loss of learning the first time is 0.39534177810449295\n",
            "813the loss of learning the first time is 0.39514019775092746\n",
            "814the loss of learning the first time is 0.3949388560943659\n",
            "815the loss of learning the first time is 0.3947377527504577\n",
            "816the loss of learning the first time is 0.39453688733559017\n",
            "817the loss of learning the first time is 0.39433625946688705\n",
            "818the loss of learning the first time is 0.39413586876220835\n",
            "819the loss of learning the first time is 0.39393571484014644\n",
            "820the loss of learning the first time is 0.3937357973200267\n",
            "821the loss of learning the first time is 0.3935361158219052\n",
            "822the loss of learning the first time is 0.3933366699665664\n",
            "823the loss of learning the first time is 0.3931374593755229\n",
            "824the loss of learning the first time is 0.3929384836710142\n",
            "825the loss of learning the first time is 0.3927397424760031\n",
            "826the loss of learning the first time is 0.3925412354141767\n",
            "827the loss of learning the first time is 0.3923429621099432\n",
            "828the loss of learning the first time is 0.3921449221884311\n",
            "829the loss of learning the first time is 0.3919471152754883\n",
            "830the loss of learning the first time is 0.3917495409976783\n",
            "831the loss of learning the first time is 0.39155219898228255\n",
            "832the loss of learning the first time is 0.39135508885729553\n",
            "833the loss of learning the first time is 0.3911582102514242\n",
            "834the loss of learning the first time is 0.3909615627940882\n",
            "835the loss of learning the first time is 0.39076514611541624\n",
            "836the loss of learning the first time is 0.3905689598462452\n",
            "837the loss of learning the first time is 0.3903730036181198\n",
            "838the loss of learning the first time is 0.39017727706328986\n",
            "839the loss of learning the first time is 0.38998177981470883\n",
            "840the loss of learning the first time is 0.38978651150603427\n",
            "841the loss of learning the first time is 0.3895914717716229\n",
            "842the loss of learning the first time is 0.3893966602465327\n",
            "843the loss of learning the first time is 0.3892020765665189\n",
            "844the loss of learning the first time is 0.38900772036803505\n",
            "845the loss of learning the first time is 0.3888135912882284\n",
            "846the loss of learning the first time is 0.3886196889649414\n",
            "847the loss of learning the first time is 0.3884260130367085\n",
            "848the loss of learning the first time is 0.38823256314275556\n",
            "849the loss of learning the first time is 0.38803933892299797\n",
            "850the loss of learning the first time is 0.38784634001803936\n",
            "851the loss of learning the first time is 0.3876535660691703\n",
            "852the loss of learning the first time is 0.38746101671836713\n",
            "853the loss of learning the first time is 0.3872686916082893\n",
            "854the loss of learning the first time is 0.3870765903822797\n",
            "855the loss of learning the first time is 0.38688471268436225\n",
            "856the loss of learning the first time is 0.3866930581592407\n",
            "857the loss of learning the first time is 0.3865016264522968\n",
            "858the loss of learning the first time is 0.38631041720958975\n",
            "859the loss of learning the first time is 0.3861194300778542\n",
            "860the loss of learning the first time is 0.38592866470449877\n",
            "861the loss of learning the first time is 0.38573812073760505\n",
            "862the loss of learning the first time is 0.38554779782592646\n",
            "863the loss of learning the first time is 0.38535769561888567\n",
            "864the loss of learning the first time is 0.3851678137665743\n",
            "865the loss of learning the first time is 0.38497815191975115\n",
            "866the loss of learning the first time is 0.38478870972984164\n",
            "867the loss of learning the first time is 0.38459948684893464\n",
            "868the loss of learning the first time is 0.3844104829297822\n",
            "869the loss of learning the first time is 0.3842216976257993\n",
            "870the loss of learning the first time is 0.38403313059105965\n",
            "871the loss of learning the first time is 0.38384478148029705\n",
            "872the loss of learning the first time is 0.38365664994890303\n",
            "873the loss of learning the first time is 0.38346873565292444\n",
            "874the loss of learning the first time is 0.38328103824906423\n",
            "875the loss of learning the first time is 0.3830935573946778\n",
            "876the loss of learning the first time is 0.382906292747774\n",
            "877the loss of learning the first time is 0.38271924396701157\n",
            "878the loss of learning the first time is 0.38253241071169863\n",
            "879the loss of learning the first time is 0.3823457926417929\n",
            "880the loss of learning the first time is 0.3821593894178966\n",
            "881the loss of learning the first time is 0.38197320070126006\n",
            "882the loss of learning the first time is 0.3817872261537755\n",
            "883the loss of learning the first time is 0.3816014654379797\n",
            "884the loss of learning the first time is 0.3814159182170497\n",
            "885the loss of learning the first time is 0.3812305841548027\n",
            "886the loss of learning the first time is 0.381045462915696\n",
            "887the loss of learning the first time is 0.38086055416482295\n",
            "888the loss of learning the first time is 0.380675857567914\n",
            "889the loss of learning the first time is 0.3804913727913339\n",
            "890the loss of learning the first time is 0.38030709950208147\n",
            "891the loss of learning the first time is 0.3801230373677874\n",
            "892the loss of learning the first time is 0.37993918605671273\n",
            "893the loss of learning the first time is 0.37975554523774974\n",
            "894the loss of learning the first time is 0.37957211458041695\n",
            "895the loss of learning the first time is 0.37938889375486196\n",
            "896the loss of learning the first time is 0.37920588243185627\n",
            "897the loss of learning the first time is 0.3790230802827966\n",
            "898the loss of learning the first time is 0.37884048697970285\n",
            "899the loss of learning the first time is 0.3786581021952167\n",
            "900the loss of learning the first time is 0.37847592560259996\n",
            "901the loss of learning the first time is 0.3782939568757341\n",
            "902the loss of learning the first time is 0.378112195689118\n",
            "903the loss of learning the first time is 0.3779306417178679\n",
            "904the loss of learning the first time is 0.37774929463771534\n",
            "905the loss of learning the first time is 0.37756815412500455\n",
            "906the loss of learning the first time is 0.37738721985669477\n",
            "907the loss of learning the first time is 0.3772064915103552\n",
            "908the loss of learning the first time is 0.37702596876416605\n",
            "909the loss of learning the first time is 0.3768456512969159\n",
            "910the loss of learning the first time is 0.37666553878800213\n",
            "911the loss of learning the first time is 0.3764856309174274\n",
            "912the loss of learning the first time is 0.37630592736580043\n",
            "913the loss of learning the first time is 0.3761264278143336\n",
            "914the loss of learning the first time is 0.375947131944842\n",
            "915the loss of learning the first time is 0.37576803943974196\n",
            "916the loss of learning the first time is 0.3755891499820506\n",
            "917the loss of learning the first time is 0.37541046325538346\n",
            "918the loss of learning the first time is 0.375231978943954\n",
            "919the loss of learning the first time is 0.3750536967325719\n",
            "920the loss of learning the first time is 0.37487561630664296\n",
            "921the loss of learning the first time is 0.3746977373521655\n",
            "922the loss of learning the first time is 0.3745200595557317\n",
            "923the loss of learning the first time is 0.374342582604525\n",
            "924the loss of learning the first time is 0.37416530618631916\n",
            "925the loss of learning the first time is 0.3739882299894767\n",
            "926the loss of learning the first time is 0.37381135370294816\n",
            "927the loss of learning the first time is 0.37363467701627134\n",
            "928the loss of learning the first time is 0.37345819961956805\n",
            "929the loss of learning the first time is 0.3732819212035454\n",
            "930the loss of learning the first time is 0.3731058414594932\n",
            "931the loss of learning the first time is 0.37292996007928275\n",
            "932the loss of learning the first time is 0.37275427675536643\n",
            "933the loss of learning the first time is 0.3725787911807752\n",
            "934the loss of learning the first time is 0.3724035030491187\n",
            "935the loss of learning the first time is 0.37222841205458335\n",
            "936the loss of learning the first time is 0.37205351789193114\n",
            "937the loss of learning the first time is 0.3718788202564989\n",
            "938the loss of learning the first time is 0.37170431884419664\n",
            "939the loss of learning the first time is 0.3715300133515064\n",
            "940the loss of learning the first time is 0.37135590347548114\n",
            "941the loss of learning the first time is 0.3711819889137435\n",
            "942the loss of learning the first time is 0.37100826936448544\n",
            "943the loss of learning the first time is 0.3708347445264654\n",
            "944the loss of learning the first time is 0.37066141409900843\n",
            "945the loss of learning the first time is 0.37048827778200405\n",
            "946the loss of learning the first time is 0.3703153352759073\n",
            "947the loss of learning the first time is 0.37014258628173347\n",
            "948the loss of learning the first time is 0.3699700305010613\n",
            "949the loss of learning the first time is 0.36979766763602884\n",
            "950the loss of learning the first time is 0.3696254973893339\n",
            "951the loss of learning the first time is 0.36945351946423177\n",
            "952the loss of learning the first time is 0.3692817335645353\n",
            "953the loss of learning the first time is 0.3691101393946123\n",
            "954the loss of learning the first time is 0.36893873665938526\n",
            "955the loss of learning the first time is 0.3687675250643303\n",
            "956the loss of learning the first time is 0.36859650431547564\n",
            "957the loss of learning the first time is 0.3684256741194007\n",
            "958the loss of learning the first time is 0.36825503418323435\n",
            "959the loss of learning the first time is 0.36808458421465484\n",
            "960the loss of learning the first time is 0.367914323921888\n",
            "961the loss of learning the first time is 0.36774425301370534\n",
            "962the loss of learning the first time is 0.3675743711994254\n",
            "963the loss of learning the first time is 0.3674046781889087\n",
            "964the loss of learning the first time is 0.36723517369256065\n",
            "965the loss of learning the first time is 0.36706585742132813\n",
            "966the loss of learning the first time is 0.3668967290866983\n",
            "967the loss of learning the first time is 0.3667277884006989\n",
            "968the loss of learning the first time is 0.3665590350758956\n",
            "969the loss of learning the first time is 0.36639046882539156\n",
            "970the loss of learning the first time is 0.3662220893628269\n",
            "971the loss of learning the first time is 0.36605389640237596\n",
            "972the loss of learning the first time is 0.36588588965874835\n",
            "973the loss of learning the first time is 0.36571806884718566\n",
            "974the loss of learning the first time is 0.365550433683462\n",
            "975the loss of learning the first time is 0.3653829838838821\n",
            "976the loss of learning the first time is 0.36521571916528\n",
            "977the loss of learning the first time is 0.36504863924501874\n",
            "978the loss of learning the first time is 0.364881743840989\n",
            "979the loss of learning the first time is 0.3647150326716073\n",
            "980the loss of learning the first time is 0.36454850545581563\n",
            "981the loss of learning the first time is 0.36438216191308037\n",
            "982the loss of learning the first time is 0.3642160017633907\n",
            "983the loss of learning the first time is 0.3640500247272581\n",
            "984the loss of learning the first time is 0.3638842305257147\n",
            "985the loss of learning the first time is 0.3637186188803126\n",
            "986the loss of learning the first time is 0.3635531895131226\n",
            "987the loss of learning the first time is 0.36338794214673303\n",
            "988the loss of learning the first time is 0.3632228765042492\n",
            "989the loss of learning the first time is 0.3630579923092916\n",
            "990the loss of learning the first time is 0.36289328928599546\n",
            "991the loss of learning the first time is 0.3627287671590087\n",
            "992the loss of learning the first time is 0.36256442565349245\n",
            "993the loss of learning the first time is 0.3624002644951184\n",
            "994the loss of learning the first time is 0.36223628341006886\n",
            "995the loss of learning the first time is 0.362072482125035\n",
            "996the loss of learning the first time is 0.36190886036721603\n",
            "997the loss of learning the first time is 0.3617454178643182\n",
            "998the loss of learning the first time is 0.36158215434455376\n",
            "999the loss of learning the first time is 0.3614190695366396\n",
            "mesh shape:(53020, 2)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    }
  ]
}