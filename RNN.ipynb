{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jigjid/github_task/blob/main/RNN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###**[Question 1]** SimpleRNN forward propagation implementation\n",
        "\n",
        "Create a class SimpleRNN for SimpleRNN. The basic structure will be the same as that of the FC class.\n",
        "\n",
        "The forward propagation formula looks like this: It also shows what happens to the shape of ndarray.\n",
        "\n",
        "The batch size, the number of features in the input, and the number of nodes in the RNN are expressed as . The activation function proceeds as tanh, but it is replaced by ReLU etc. like conventional neural networks.```batch_size n_features n_nodes```"
      ],
      "metadata": {
        "id": "SUmhUmDGqBN4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Library\n",
        "import numpy as np"
      ],
      "metadata": {
        "id": "FJ6wWapTyGM3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gFadopZ9p9Tb"
      },
      "outputs": [],
      "source": [
        "class ScratchSimpleRNNClassifier:\n",
        "    \"\"\"\n",
        "    \n",
        "    \"\"\"\n",
        "    def __init__(self, x, w_x, w_h):\n",
        "        self.w_x = w_x\n",
        "        self.w_h = w_h\n",
        "        self.batch_size = x.shape[0] # 1\n",
        "        self.n_sequences = x.shape[1] # 3\n",
        "        self.n_features = x.shape[2] # 2\n",
        "        self.n_nodes = w_x.shape[1] # 4\n",
        "        self.h = np.zeros((self.batch_size, self.n_nodes)) # (batch_size, n_nodes)\n",
        "        self.b = np.array([1, 1, 1, 1]) # (n_nodes,)\n",
        "      \n",
        "        \n",
        "    def forward(self,x):\n",
        "      '''\n",
        "\n",
        "      '''\n",
        "      self.x = x\n",
        "      for n in range(self.n_sequences):\n",
        "          self.h = np.tanh(x[:, n, :] @ self.w_x + self.h @ self.w_h + self.b)\n",
        "      return self.h\n",
        "\n",
        "\n",
        "    def backward(self, dA):\n",
        "      \"\"\"\n",
        "      \"\"\"\n",
        "      pass\n",
        "       \n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###**Problem 2:** Experiment of forward propagation on small arrays\n",
        "\n",
        "Consider forward propagation with a small array.\n",
        "\n",
        "Input x, initial state h, weight w_x and w_h, bias b is:\n",
        "\n",
        "Here the axes of array x are in the order of batch size, number of series, and number of features."
      ],
      "metadata": {
        "id": "eQcCi4HVyRhx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "x = np.array([[[1, 2], [2, 3], [3, 4]]])/100 # (batch_size, n_sequences, n_features)\n",
        "w_x = np.array([[1, 3, 5, 7], [3, 5, 7, 8]])/100 # (n_features, n_nodes)\n",
        "w_h = np.array([[1, 3, 5, 7], [2, 4, 6, 8], [3, 5, 7, 8], [4, 6, 8, 10]])/100 # (n_nodes, n_nodes)\n",
        "\n",
        "rnn = ScratchSimpleRNNClassifier(x=x, w_h=w_h, w_x=w_x)\n",
        "\n",
        "rnn.forward(x)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tGx8UWWJzhIU",
        "outputId": "3aab9cce-6809-4e2e-9a00-edb7c8dc8600"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0.79494228, 0.81839002, 0.83939649, 0.85584174]])"
            ]
          },
          "metadata": {},
          "execution_count": 59
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###**[Question 3] (Advanced Task)** Implementing Backpropagation"
      ],
      "metadata": {
        "id": "zgU3fm755Loo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from numpy.random import randn\n",
        "\n",
        "class ScratchSimpleRNNClassifier:\n",
        "  \n",
        "\n",
        "  # def __init__(self, input_size, output_size, hidden_size=64):\n",
        "  #   # Weights\n",
        "  #   self.Whh = randn(hidden_size, hidden_size) / 1000\n",
        "  #   self.Wxh = randn(hidden_size, input_size) / 1000\n",
        "  #   self.Why = randn(output_size, hidden_size) / 1000\n",
        "  #   # Biases\n",
        "  #   self.bh = np.zeros((hidden_size, 1))\n",
        "  #   self.by = np.zeros((output_size, 1))\n",
        "\n",
        "  def __init__(self, x, w_x, w_h):\n",
        "        self.w_x = w_x\n",
        "        self.w_h = w_h\n",
        "        self.batch_size = x.shape[0] # 1\n",
        "        self.n_sequences = x.shape[1] # 3\n",
        "        self.n_features = x.shape[2] # 2\n",
        "        self.n_nodes = w_x.shape[1] # 4\n",
        "        self.bh = np.zeros((self.batch_size, self.n_nodes)) # (batch_size, n_nodes)\n",
        "        self.by = np.array([1, 1, 1, 1]) # (n_nodes,)\n",
        "\n",
        "\n",
        "\n",
        "  def forward(self, inputs):\n",
        "    '''\n",
        "    Perform a forward pass of the RNN using the given inputs.\n",
        "    Returns the final output and hidden state.\n",
        "    - inputs is an array of one hot vectors with shape (input_size, 1).\n",
        "    '''\n",
        "\n",
        "    self.last_inputs = inputs\n",
        "    self.last_hs = { 0: self.bh }\n",
        "\n",
        "    # Perform each step of the RNN\n",
        "    for i in enumerate(inputs):\n",
        "      self.bh = np.tanh(inputs[:, i, :] @ self.w_x + self.bh @ self.w_h + self.by)\n",
        "      self.last_hs[i + 1] = self.bh\n",
        "\n",
        "\n",
        "    return self.bh\n",
        "\n",
        "  def backprop(self, d_y, learn_rate=2e-2):\n",
        "    '''\n",
        "    Perform a backward pass of the RNN.\n",
        "    - d_y (dL/dy) has shape (output_size, 1).\n",
        "    - learn_rate is a float.\n",
        "    '''\n",
        "    n = len(self.last_inputs)\n",
        "\n",
        "    # Calculate dL/dWhy and dL/dby.\n",
        "    d_W_x = d_y @ self.last_hs[n].T\n",
        "    d_by = d_y\n",
        "\n",
        "    # Initialize dL/dWhh, dL/dWxh, and dL/dbh to zero.\n",
        "    d_W_h = np.zeros(self.w_h.shape)\n",
        "    d_W_x = np.zeros(self.w_x.shape)\n",
        "    d_bh = np.zeros(self.bh.shape)\n",
        "\n",
        "    # Calculate dL/dh for the last h.\n",
        "    # dL/dh = dL/dy * dy/dh\n",
        "    d_h =  d_y\n",
        "\n",
        "    # Backpropagate through time.\n",
        "    for t in reversed(range(n)):\n",
        "      # An intermediate value: dL/dh * (1 - h^2)\n",
        "      temp = ((1 - self.last_hs[t + 1] ** 2) * d_h)\n",
        "\n",
        "      # dL/db = dL/dh * (1 - h^2)\n",
        "      d_bh += temp\n",
        "\n",
        "      # dL/dWhh = dL/dh * (1 - h^2) * h_{t-1}\n",
        "      d_Whh += temp @ self.last_hs[t].T\n",
        "\n",
        "      # dL/dWxh = dL/dh * (1 - h^2) * x\n",
        "      d_Wxh += temp @ self.last_inputs[t].T\n",
        "\n",
        "      # Next dL/dh = dL/dh * (1 - h^2) * Whh\n",
        "      d_h = self.Whh @ temp\n",
        "\n",
        "    # Clip to prevent exploding gradients.\n",
        "    for d in [d_Wxh, d_Whh, d_bh, d_by]:\n",
        "      np.clip(d, -1, 1, out=d)\n",
        "\n",
        "    # Update weights and biases using gradient descent.\n",
        "    self.w_h -= learn_rate * d_Whh\n",
        "    self.w_x -= learn_rate * d_Wxh\n",
        "    self.bh -= learn_rate * d_bh\n",
        "    self.by -= learn_rate * d_by"
      ],
      "metadata": {
        "id": "TGh8968myWEl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "rnn = ScratchSimpleRNNClassifier(x=x, w_x=w_x, w_h=w_h)\n",
        "\n",
        "rnn.forward(x)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aPs9Bf-r_MsP",
        "outputId": "6c9cf405-03de-4adc-9a42-7aac084740ca"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0.79494228, 0.81839002, 0.83939649, 0.85584174]])"
            ]
          },
          "metadata": {},
          "execution_count": 60
        }
      ]
    }
  ]
}