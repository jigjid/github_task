{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jigjid/github_task/blob/main/TensorFlow.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##processing"
      ],
      "metadata": {
        "id": "tLVD94QBnaxM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "import tensorflow.compat.v1 as tf\n",
        "tf.compat.v1.disable_eager_execution()\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from keras.datasets import mnist\n",
        "from matplotlib import pyplot as plt"
      ],
      "metadata": {
        "id": "eGfRKVWznfNQ"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Problem 1:** Looking back at scratch\n",
        "\n",
        "Preparing the dataset"
      ],
      "metadata": {
        "id": "OQYCkZt2mIjD"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "_IqzMZ_Kwxud"
      },
      "outputs": [],
      "source": [
        "df = pd.read_csv(\"Iris.csv\")\n",
        "df = df[(df[\"Species\"] == \"Iris-versicolor\") | (df[\"Species\"] == \"Iris-virginica\")]\n",
        "y = df[\"Species\"]\n",
        "X = df.loc[:, [\"SepalLengthCm\", \"SepalWidthCm\", \"PetalLengthCm\", \"PetalWidthCm\"]]\n",
        "X = np.array(X)\n",
        "y = np.array(y)\n",
        "y[y == \"Iris-versicolor\"] = 0\n",
        "y[y == \"Iris-virginica\"] = 1\n",
        "y = y.astype(np.int64)[:, np.newaxis]\n",
        "X_train, X_test, y_train,y_test = train_test_split(X,y, test_size = 0.2, random_state = 0)\n",
        "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.2, random_state=0)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Problem 2:** Thinking about how to deal with scratch and TensorFlow"
      ],
      "metadata": {
        "id": "Y7neRva7seH1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class GetMiniBatch:\n",
        "    \"\"\"\n",
        "    ミニバッチを取得するイテレータ\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    X : 次の形のndarray, shape (n_samples, n_features)\n",
        "      訓練データ\n",
        "    y : 次の形のndarray, shape (n_samples, 1)\n",
        "      正解値\n",
        "    batch_size : int\n",
        "      バッチサイズ\n",
        "    seed : int\n",
        "      NumPyの乱数のシード\n",
        "    \"\"\"\n",
        "    def __init__(self, X, y, batch_size = 10, seed=0):\n",
        "        self.batch_size = batch_size\n",
        "        np.random.seed(seed)\n",
        "        shuffle_index = np.random.permutation(np.arange(X.shape[0]))\n",
        "        self.X = X[shuffle_index]\n",
        "        self.y = y[shuffle_index]\n",
        "        self._stop = np.ceil(X.shape[0]/self.batch_size).astype(np.int)\n",
        "    def __len__(self):\n",
        "        return self._stop\n",
        "    def __getitem__(self,item):\n",
        "        p0 = item*self.batch_size\n",
        "        p1 = item*self.batch_size + self.batch_size\n",
        "        return self.X[p0:p1], self.y[p0:p1]        \n",
        "    def __iter__(self):\n",
        "        self._counter = 0\n",
        "        return self\n",
        "    def __next__(self):\n",
        "        if self._counter >= self._stop:\n",
        "            raise StopIteration()\n",
        "        p0 = self._counter*self.batch_size\n",
        "        p1 = self._counter*self.batch_size + self.batch_size\n",
        "        self._counter += 1\n",
        "        return self.X[p0:p1], self.y[p0:p1]\n",
        "\n",
        "# Configuring Hyperparameters\n",
        "learning_rate = 0.001\n",
        "batch_size = 10\n",
        "num_epochs = 100\n",
        "\n",
        "n_hidden1 = 50\n",
        "n_hidden2 = 100\n",
        "n_input = X_train.shape[1]\n",
        "n_samples = X_train.shape[0]\n",
        "n_classes = 1\n",
        "\n",
        "# Determine the shape of the arguments to be passed to the calculation graph\n",
        "X = tf.placeholder(\"float\", [None, n_input])\n",
        "Y = tf.placeholder(\"float\", [None, n_classes])\n",
        "# trainのミニバッチイテレータ\n",
        "get_mini_batch_train = GetMiniBatch(X_train, y_train, batch_size=batch_size)"
      ],
      "metadata": {
        "id": "IIgHzQ-w4TFg",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cadd9012-73ce-4201-ab2f-4c3848914dbd"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-7-30d56e557bd2>:22: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.\n",
            "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
            "  self._stop = np.ceil(X.shape[0]/self.batch_size).astype(np.int)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def example_net(x):\n",
        "    \"\"\"\n",
        "    単純な3層ニューラルネットワーク\n",
        "    \"\"\"\n",
        "    tf.random.set_random_seed(0)\n",
        "    # 重みとバイアスの宣言\n",
        "    weights = {\n",
        "        'w1': tf.Variable(tf.random_normal([n_input, n_hidden1])),\n",
        "        'w2': tf.Variable(tf.random_normal([n_hidden1, n_hidden2])),\n",
        "        'w3': tf.Variable(tf.random_normal([n_hidden2, n_classes]))\n",
        "    }\n",
        "    biases = {\n",
        "        'b1': tf.Variable(tf.random_normal([n_hidden1])),\n",
        "        'b2': tf.Variable(tf.random_normal([n_hidden2])),\n",
        "        'b3': tf.Variable(tf.random_normal([n_classes]))\n",
        "    }\n",
        "\n",
        "    layer_1 = tf.add(tf.matmul(x, weights['w1']), biases['b1'])\n",
        "    layer_1 = tf.nn.relu(layer_1)\n",
        "    layer_2 = tf.add(tf.matmul(layer_1, weights['w2']), biases['b2'])\n",
        "    layer_2 = tf.nn.relu(layer_2)\n",
        "    layer_output = tf.matmul(layer_2, weights['w3']) + biases['b3'] # tf.addと+は等価である\n",
        "    return layer_output\n",
        "\n",
        "# Reading the network structure                               \n",
        "logits = example_net(X)\n",
        "\n",
        "# Objective function\n",
        "loss_op = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(labels=Y, logits=logits))\n",
        "# Optimization techniques\n",
        "optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate)\n",
        "train_op = optimizer.minimize(loss_op)\n",
        "\n",
        "# Estimated results\n",
        "correct_pred = tf.equal(tf.sign(Y - 0.5), tf.sign(tf.sigmoid(logits) - 0.5))\n",
        "# Index value calculation\n",
        "accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32))\n",
        "\n",
        "# Initializing a variable\n",
        "init = tf.global_variables_initializer()\n",
        "\n",
        "\n",
        "# Running Calculation Graphs\n",
        "with tf.Session() as sess:\n",
        "    sess.run(init)\n",
        "    for epoch in range(num_epochs):\n",
        "        # Loop every epoch\n",
        "        total_batch = np.ceil(X_train.shape[0]/batch_size).astype(np.int64)\n",
        "        total_loss = 0\n",
        "        total_acc = 0\n",
        "        for i, (mini_batch_x, mini_batch_y) in enumerate(get_mini_batch_train):\n",
        "            # Loop per mini-batch\n",
        "            sess.run(train_op, feed_dict={X: mini_batch_x, Y: mini_batch_y})\n",
        "            loss, acc = sess.run([loss_op, accuracy], feed_dict={X: mini_batch_x, Y: mini_batch_y})\n",
        "            total_loss += loss\n",
        "        total_loss /= n_samples\n",
        "        val_loss, acc = sess.run([loss_op, accuracy], feed_dict={X: X_val, Y: y_val})\n",
        "        print(\"Epoch {}, loss : {:.4f}, val_loss : {:.4f}, acc : {:.3f}\".format(epoch, total_loss, val_loss, acc))\n",
        "    test_acc = sess.run(accuracy, feed_dict={X: X_test, Y: y_test})\n",
        "    print(\"test_acc : {:.3f}\".format(test_acc))\n",
        "\n",
        "\n",
        "print(\"It is much faster than implemented CNN from scratch. It's also very easy to use \\\n",
        "\\nFirst, weights and biases are initialized and then layers are defined.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Dg3TmPlJsnPm",
        "outputId": "6d4ad60e-efba-4a19-f30d-d1e23cd801ec"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 0, loss : 7.0241, val_loss : 67.6860, acc : 0.375\n",
            "Epoch 1, loss : 3.4241, val_loss : 23.4026, acc : 0.312\n",
            "Epoch 2, loss : 1.9387, val_loss : 11.6681, acc : 0.375\n",
            "Epoch 3, loss : 2.0917, val_loss : 13.1400, acc : 0.312\n",
            "Epoch 4, loss : 1.7685, val_loss : 17.7284, acc : 0.312\n",
            "Epoch 5, loss : 1.6097, val_loss : 12.9607, acc : 0.312\n",
            "Epoch 6, loss : 1.4402, val_loss : 10.0593, acc : 0.312\n",
            "Epoch 7, loss : 1.3704, val_loss : 9.4797, acc : 0.312\n",
            "Epoch 8, loss : 1.2536, val_loss : 9.8518, acc : 0.312\n",
            "Epoch 9, loss : 1.1476, val_loss : 8.5670, acc : 0.375\n",
            "Epoch 10, loss : 1.0930, val_loss : 8.0430, acc : 0.375\n",
            "Epoch 11, loss : 1.0412, val_loss : 7.8791, acc : 0.375\n",
            "Epoch 12, loss : 0.9804, val_loss : 7.1233, acc : 0.375\n",
            "Epoch 13, loss : 0.9326, val_loss : 6.7908, acc : 0.375\n",
            "Epoch 14, loss : 0.8792, val_loss : 6.2492, acc : 0.375\n",
            "Epoch 15, loss : 0.8304, val_loss : 5.7680, acc : 0.375\n",
            "Epoch 16, loss : 0.7835, val_loss : 5.2886, acc : 0.438\n",
            "Epoch 17, loss : 0.7384, val_loss : 4.8037, acc : 0.438\n",
            "Epoch 18, loss : 0.6961, val_loss : 4.3575, acc : 0.500\n",
            "Epoch 19, loss : 0.6543, val_loss : 3.9175, acc : 0.500\n",
            "Epoch 20, loss : 0.6136, val_loss : 3.5188, acc : 0.500\n",
            "Epoch 21, loss : 0.5738, val_loss : 3.1371, acc : 0.500\n",
            "Epoch 22, loss : 0.5349, val_loss : 2.7697, acc : 0.500\n",
            "Epoch 23, loss : 0.4973, val_loss : 2.4528, acc : 0.562\n",
            "Epoch 24, loss : 0.4606, val_loss : 2.1728, acc : 0.562\n",
            "Epoch 25, loss : 0.4255, val_loss : 1.9324, acc : 0.625\n",
            "Epoch 26, loss : 0.3919, val_loss : 1.7049, acc : 0.625\n",
            "Epoch 27, loss : 0.3609, val_loss : 1.5248, acc : 0.688\n",
            "Epoch 28, loss : 0.3326, val_loss : 1.3724, acc : 0.750\n",
            "Epoch 29, loss : 0.3071, val_loss : 1.2386, acc : 0.750\n",
            "Epoch 30, loss : 0.2851, val_loss : 1.1454, acc : 0.750\n",
            "Epoch 31, loss : 0.2647, val_loss : 1.0364, acc : 0.750\n",
            "Epoch 32, loss : 0.2466, val_loss : 0.9368, acc : 0.750\n",
            "Epoch 33, loss : 0.2297, val_loss : 0.8304, acc : 0.750\n",
            "Epoch 34, loss : 0.2155, val_loss : 0.7243, acc : 0.750\n",
            "Epoch 35, loss : 0.2024, val_loss : 0.6215, acc : 0.750\n",
            "Epoch 36, loss : 0.1920, val_loss : 0.5405, acc : 0.812\n",
            "Epoch 37, loss : 0.1815, val_loss : 0.4515, acc : 0.812\n",
            "Epoch 38, loss : 0.1735, val_loss : 0.4016, acc : 0.812\n",
            "Epoch 39, loss : 0.1643, val_loss : 0.3323, acc : 0.812\n",
            "Epoch 40, loss : 0.1574, val_loss : 0.2935, acc : 0.875\n",
            "Epoch 41, loss : 0.1496, val_loss : 0.2519, acc : 0.875\n",
            "Epoch 42, loss : 0.1429, val_loss : 0.2206, acc : 0.875\n",
            "Epoch 43, loss : 0.1364, val_loss : 0.1932, acc : 0.875\n",
            "Epoch 44, loss : 0.1302, val_loss : 0.1684, acc : 0.875\n",
            "Epoch 45, loss : 0.1244, val_loss : 0.1464, acc : 0.875\n",
            "Epoch 46, loss : 0.1188, val_loss : 0.1264, acc : 0.875\n",
            "Epoch 47, loss : 0.1139, val_loss : 0.1097, acc : 0.875\n",
            "Epoch 48, loss : 0.1091, val_loss : 0.0947, acc : 0.938\n",
            "Epoch 49, loss : 0.1050, val_loss : 0.0833, acc : 0.938\n",
            "Epoch 50, loss : 0.1007, val_loss : 0.0712, acc : 1.000\n",
            "Epoch 51, loss : 0.0975, val_loss : 0.0653, acc : 1.000\n",
            "Epoch 52, loss : 0.0932, val_loss : 0.0531, acc : 1.000\n",
            "Epoch 53, loss : 0.0915, val_loss : 0.0557, acc : 1.000\n",
            "Epoch 54, loss : 0.0862, val_loss : 0.0390, acc : 1.000\n",
            "Epoch 55, loss : 0.0875, val_loss : 0.0598, acc : 0.938\n",
            "Epoch 56, loss : 0.0790, val_loss : 0.0376, acc : 1.000\n",
            "Epoch 57, loss : 0.0871, val_loss : 0.0935, acc : 0.938\n",
            "Epoch 58, loss : 0.0723, val_loss : 0.0861, acc : 0.938\n",
            "Epoch 59, loss : 0.0936, val_loss : 0.1730, acc : 0.938\n",
            "Epoch 60, loss : 0.0693, val_loss : 0.1648, acc : 0.938\n",
            "Epoch 61, loss : 0.1047, val_loss : 0.2713, acc : 0.938\n",
            "Epoch 62, loss : 0.0711, val_loss : 0.1687, acc : 0.938\n",
            "Epoch 63, loss : 0.1072, val_loss : 0.3121, acc : 0.938\n",
            "Epoch 64, loss : 0.0738, val_loss : 0.1538, acc : 0.938\n",
            "Epoch 65, loss : 0.1069, val_loss : 0.3194, acc : 0.938\n",
            "Epoch 66, loss : 0.0757, val_loss : 0.1525, acc : 0.938\n",
            "Epoch 67, loss : 0.1082, val_loss : 0.3200, acc : 0.938\n",
            "Epoch 68, loss : 0.0780, val_loss : 0.1584, acc : 0.938\n",
            "Epoch 69, loss : 0.1114, val_loss : 0.3153, acc : 0.938\n",
            "Epoch 70, loss : 0.0808, val_loss : 0.1600, acc : 0.938\n",
            "Epoch 71, loss : 0.1139, val_loss : 0.2888, acc : 0.938\n",
            "Epoch 72, loss : 0.0824, val_loss : 0.1424, acc : 0.938\n",
            "Epoch 73, loss : 0.1110, val_loss : 0.2301, acc : 0.938\n",
            "Epoch 74, loss : 0.0791, val_loss : 0.1062, acc : 0.938\n",
            "Epoch 75, loss : 0.1002, val_loss : 0.1769, acc : 0.938\n",
            "Epoch 76, loss : 0.0719, val_loss : 0.0579, acc : 0.938\n",
            "Epoch 77, loss : 0.0810, val_loss : 0.0948, acc : 0.938\n",
            "Epoch 78, loss : 0.0610, val_loss : 0.0222, acc : 1.000\n",
            "Epoch 79, loss : 0.0601, val_loss : 0.0222, acc : 1.000\n",
            "Epoch 80, loss : 0.0514, val_loss : 0.0117, acc : 1.000\n",
            "Epoch 81, loss : 0.0498, val_loss : 0.0060, acc : 1.000\n",
            "Epoch 82, loss : 0.0454, val_loss : 0.0173, acc : 1.000\n",
            "Epoch 83, loss : 0.0472, val_loss : 0.0077, acc : 1.000\n",
            "Epoch 84, loss : 0.0430, val_loss : 0.0289, acc : 1.000\n",
            "Epoch 85, loss : 0.0449, val_loss : 0.0094, acc : 1.000\n",
            "Epoch 86, loss : 0.0400, val_loss : 0.0493, acc : 0.938\n",
            "Epoch 87, loss : 0.0430, val_loss : 0.0089, acc : 1.000\n",
            "Epoch 88, loss : 0.0374, val_loss : 0.0669, acc : 0.938\n",
            "Epoch 89, loss : 0.0429, val_loss : 0.0068, acc : 1.000\n",
            "Epoch 90, loss : 0.0361, val_loss : 0.0709, acc : 0.938\n",
            "Epoch 91, loss : 0.0422, val_loss : 0.0058, acc : 1.000\n",
            "Epoch 92, loss : 0.0349, val_loss : 0.0692, acc : 0.938\n",
            "Epoch 93, loss : 0.0411, val_loss : 0.0055, acc : 1.000\n",
            "Epoch 94, loss : 0.0337, val_loss : 0.0678, acc : 0.938\n",
            "Epoch 95, loss : 0.0397, val_loss : 0.0059, acc : 1.000\n",
            "Epoch 96, loss : 0.0326, val_loss : 0.0613, acc : 0.938\n",
            "Epoch 97, loss : 0.0376, val_loss : 0.0081, acc : 1.000\n",
            "Epoch 98, loss : 0.0316, val_loss : 0.0499, acc : 0.938\n",
            "Epoch 99, loss : 0.0349, val_loss : 0.0138, acc : 1.000\n",
            "test_acc : 0.900\n",
            "It is much faster than implemented CNN from scratch. It's also very easy to use \n",
            "First, weights and biases are initialized and then layers are defined.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Problem 3:** Create an Iris model using all three types of objective variables"
      ],
      "metadata": {
        "id": "jqYq9SBR-LOQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_csv(\"Iris.csv\")\n",
        "df = df[(df[\"Species\"] == \"Iris-versicolor\") | (df[\"Species\"] == \"Iris-virginica\") | (df[\"Species\"]==\"Iris-setosa\")]\n",
        "y = df[\"Species\"]\n",
        "X = df.loc[:, [\"SepalLengthCm\", \"SepalWidthCm\", \"PetalLengthCm\", \"PetalWidthCm\"]]\n",
        "X = np.array(X)\n",
        "y = np.array(y)\n",
        "y[y == \"Iris-versicolor\"] = 0\n",
        "y[y == \"Iris-virginica\"] = 1\n",
        "y[y == \"Iris-setosa\"] = 2\n",
        "\n",
        "enc = OneHotEncoder(handle_unknown='ignore', sparse=False)\n",
        "y = enc.fit_transform(y[:,np.newaxis])\n",
        "\n",
        "X_train, X_test, y_train,y_test = train_test_split(X,y, test_size = 0.2, random_state = 0)\n",
        "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.2, random_state=0)\n",
        "\n",
        "mmsc = MinMaxScaler()\n",
        "X_train = mmsc.fit_transform(X_train)\n",
        "X_test = mmsc.transform(X_test)\n",
        "X_val = mmsc.transform(X_val)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ks-dfBOy7rc8",
        "outputId": "bf791a31-4130-48d1-deb8-3ba0dd785ba6"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/preprocessing/_encoders.py:868: FutureWarning: `sparse` was renamed to `sparse_output` in version 1.2 and will be removed in 1.4. `sparse_output` is ignored unless you leave `sparse` to its default value.\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "learning_rate = 0.001\n",
        "batch_size = 10\n",
        "num_epochs = 100\n",
        "n_hidden1 = 50\n",
        "n_hidden2 = 100\n",
        "n_input = X_train.shape[1]\n",
        "n_samples = X_train.shape[0]\n",
        "\n",
        "n_classes = 3\n",
        "X = tf.placeholder(\"float\", [None, n_input])\n",
        "Y = tf.placeholder(\"float\", [None, n_classes])\n",
        "\n",
        "get_mini_batch_train = GetMiniBatch(X_train, y_train, batch_size=batch_size)\n",
        "\n",
        "logits = example_net(X)\n",
        "loss_op = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(labels=Y, logits=logits)) \n",
        "optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate)  \n",
        "train_op = optimizer.minimize(loss_op)  \n",
        "correct_pred = tf.equal(tf.sign(Y - 0.5), tf.sign(tf.sigmoid(logits) - 0.5))   \n",
        "accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32))  \n",
        "init = tf.global_variables_initializer()\n",
        "\n",
        "with tf.Session() as sess:\n",
        "    sess.run(init)\n",
        "    for epoch in range(num_epochs):\n",
        "        # Loop every epoch\n",
        "        total_batch = np.ceil(X_train.shape[0]/batch_size).astype(np.int64)\n",
        "        total_loss = 0\n",
        "        total_acc = 0\n",
        "        for i, (mini_batch_x, mini_batch_y) in enumerate(get_mini_batch_train):\n",
        "            # Loop per mini-batch\n",
        "            sess.run(train_op, feed_dict={X: mini_batch_x, Y: mini_batch_y})\n",
        "            loss, acc = sess.run([loss_op, accuracy], feed_dict={X: mini_batch_x, Y: mini_batch_y})\n",
        "            total_loss += loss\n",
        "        total_loss /= n_samples\n",
        "        val_loss, acc = sess.run([loss_op, accuracy], feed_dict={X: X_val, Y: y_val})\n",
        "        print(\"Epoch {}, loss : {:.4f}, val_loss : {:.4f}, acc : {:.3f}\".format(epoch, total_loss, val_loss, acc))\n",
        "    test_acc = sess.run(accuracy, feed_dict={X: X_test, Y: y_test})\n",
        "    print(\"test_acc : {:.3f}\".format(test_acc))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-4hrZsjFBSVv",
        "outputId": "20099a4e-8189-415c-e405-4023b1938707"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-7-30d56e557bd2>:22: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.\n",
            "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
            "  self._stop = np.ceil(X.shape[0]/self.batch_size).astype(np.int)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 0, loss : 1.7987, val_loss : 14.7636, acc : 0.444\n",
            "Epoch 1, loss : 1.2412, val_loss : 9.2677, acc : 0.431\n",
            "Epoch 2, loss : 0.7166, val_loss : 5.2194, acc : 0.611\n",
            "Epoch 3, loss : 0.4205, val_loss : 3.4381, acc : 0.667\n",
            "Epoch 4, loss : 0.2684, val_loss : 2.1337, acc : 0.667\n",
            "Epoch 5, loss : 0.1845, val_loss : 1.4346, acc : 0.722\n",
            "Epoch 6, loss : 0.1488, val_loss : 1.0783, acc : 0.778\n",
            "Epoch 7, loss : 0.1272, val_loss : 0.9443, acc : 0.847\n",
            "Epoch 8, loss : 0.1129, val_loss : 0.8756, acc : 0.833\n",
            "Epoch 9, loss : 0.1016, val_loss : 0.8028, acc : 0.833\n",
            "Epoch 10, loss : 0.0919, val_loss : 0.7375, acc : 0.833\n",
            "Epoch 11, loss : 0.0831, val_loss : 0.6720, acc : 0.833\n",
            "Epoch 12, loss : 0.0745, val_loss : 0.6075, acc : 0.847\n",
            "Epoch 13, loss : 0.0663, val_loss : 0.5457, acc : 0.847\n",
            "Epoch 14, loss : 0.0586, val_loss : 0.4917, acc : 0.861\n",
            "Epoch 15, loss : 0.0514, val_loss : 0.4502, acc : 0.875\n",
            "Epoch 16, loss : 0.0446, val_loss : 0.4147, acc : 0.875\n",
            "Epoch 17, loss : 0.0382, val_loss : 0.3815, acc : 0.889\n",
            "Epoch 18, loss : 0.0325, val_loss : 0.3492, acc : 0.903\n",
            "Epoch 19, loss : 0.0273, val_loss : 0.3205, acc : 0.903\n",
            "Epoch 20, loss : 0.0228, val_loss : 0.2979, acc : 0.903\n",
            "Epoch 21, loss : 0.0190, val_loss : 0.2798, acc : 0.903\n",
            "Epoch 22, loss : 0.0157, val_loss : 0.2718, acc : 0.903\n",
            "Epoch 23, loss : 0.0131, val_loss : 0.2723, acc : 0.903\n",
            "Epoch 24, loss : 0.0111, val_loss : 0.2696, acc : 0.903\n",
            "Epoch 25, loss : 0.0098, val_loss : 0.2665, acc : 0.917\n",
            "Epoch 26, loss : 0.0088, val_loss : 0.2629, acc : 0.903\n",
            "Epoch 27, loss : 0.0081, val_loss : 0.2588, acc : 0.903\n",
            "Epoch 28, loss : 0.0076, val_loss : 0.2547, acc : 0.903\n",
            "Epoch 29, loss : 0.0072, val_loss : 0.2512, acc : 0.903\n",
            "Epoch 30, loss : 0.0069, val_loss : 0.2474, acc : 0.903\n",
            "Epoch 31, loss : 0.0066, val_loss : 0.2437, acc : 0.903\n",
            "Epoch 32, loss : 0.0064, val_loss : 0.2410, acc : 0.903\n",
            "Epoch 33, loss : 0.0062, val_loss : 0.2383, acc : 0.903\n",
            "Epoch 34, loss : 0.0060, val_loss : 0.2352, acc : 0.917\n",
            "Epoch 35, loss : 0.0058, val_loss : 0.2332, acc : 0.917\n",
            "Epoch 36, loss : 0.0057, val_loss : 0.2311, acc : 0.917\n",
            "Epoch 37, loss : 0.0056, val_loss : 0.2278, acc : 0.917\n",
            "Epoch 38, loss : 0.0054, val_loss : 0.2247, acc : 0.917\n",
            "Epoch 39, loss : 0.0053, val_loss : 0.2227, acc : 0.917\n",
            "Epoch 40, loss : 0.0052, val_loss : 0.2203, acc : 0.917\n",
            "Epoch 41, loss : 0.0051, val_loss : 0.2178, acc : 0.917\n",
            "Epoch 42, loss : 0.0050, val_loss : 0.2163, acc : 0.917\n",
            "Epoch 43, loss : 0.0049, val_loss : 0.2145, acc : 0.917\n",
            "Epoch 44, loss : 0.0049, val_loss : 0.2123, acc : 0.917\n",
            "Epoch 45, loss : 0.0048, val_loss : 0.2111, acc : 0.917\n",
            "Epoch 46, loss : 0.0047, val_loss : 0.2096, acc : 0.917\n",
            "Epoch 47, loss : 0.0046, val_loss : 0.2075, acc : 0.917\n",
            "Epoch 48, loss : 0.0046, val_loss : 0.2059, acc : 0.917\n",
            "Epoch 49, loss : 0.0045, val_loss : 0.2051, acc : 0.917\n",
            "Epoch 50, loss : 0.0044, val_loss : 0.2040, acc : 0.917\n",
            "Epoch 51, loss : 0.0044, val_loss : 0.2030, acc : 0.917\n",
            "Epoch 52, loss : 0.0043, val_loss : 0.2015, acc : 0.917\n",
            "Epoch 53, loss : 0.0042, val_loss : 0.2005, acc : 0.917\n",
            "Epoch 54, loss : 0.0042, val_loss : 0.2001, acc : 0.917\n",
            "Epoch 55, loss : 0.0041, val_loss : 0.1990, acc : 0.917\n",
            "Epoch 56, loss : 0.0041, val_loss : 0.1981, acc : 0.917\n",
            "Epoch 57, loss : 0.0040, val_loss : 0.1978, acc : 0.917\n",
            "Epoch 58, loss : 0.0040, val_loss : 0.1976, acc : 0.917\n",
            "Epoch 59, loss : 0.0039, val_loss : 0.1977, acc : 0.917\n",
            "Epoch 60, loss : 0.0039, val_loss : 0.1967, acc : 0.917\n",
            "Epoch 61, loss : 0.0038, val_loss : 0.1957, acc : 0.917\n",
            "Epoch 62, loss : 0.0038, val_loss : 0.1954, acc : 0.917\n",
            "Epoch 63, loss : 0.0037, val_loss : 0.1938, acc : 0.917\n",
            "Epoch 64, loss : 0.0037, val_loss : 0.1925, acc : 0.917\n",
            "Epoch 65, loss : 0.0036, val_loss : 0.1911, acc : 0.917\n",
            "Epoch 66, loss : 0.0035, val_loss : 0.1900, acc : 0.917\n",
            "Epoch 67, loss : 0.0034, val_loss : 0.1885, acc : 0.917\n",
            "Epoch 68, loss : 0.0034, val_loss : 0.1878, acc : 0.917\n",
            "Epoch 69, loss : 0.0033, val_loss : 0.1871, acc : 0.917\n",
            "Epoch 70, loss : 0.0033, val_loss : 0.1856, acc : 0.917\n",
            "Epoch 71, loss : 0.0033, val_loss : 0.1853, acc : 0.917\n",
            "Epoch 72, loss : 0.0032, val_loss : 0.1829, acc : 0.917\n",
            "Epoch 73, loss : 0.0032, val_loss : 0.1807, acc : 0.917\n",
            "Epoch 74, loss : 0.0032, val_loss : 0.1806, acc : 0.917\n",
            "Epoch 75, loss : 0.0031, val_loss : 0.1799, acc : 0.917\n",
            "Epoch 76, loss : 0.0031, val_loss : 0.1788, acc : 0.917\n",
            "Epoch 77, loss : 0.0031, val_loss : 0.1781, acc : 0.931\n",
            "Epoch 78, loss : 0.0031, val_loss : 0.1774, acc : 0.931\n",
            "Epoch 79, loss : 0.0030, val_loss : 0.1767, acc : 0.931\n",
            "Epoch 80, loss : 0.0030, val_loss : 0.1758, acc : 0.931\n",
            "Epoch 81, loss : 0.0030, val_loss : 0.1749, acc : 0.931\n",
            "Epoch 82, loss : 0.0030, val_loss : 0.1747, acc : 0.931\n",
            "Epoch 83, loss : 0.0029, val_loss : 0.1741, acc : 0.931\n",
            "Epoch 84, loss : 0.0029, val_loss : 0.1728, acc : 0.931\n",
            "Epoch 85, loss : 0.0029, val_loss : 0.1724, acc : 0.931\n",
            "Epoch 86, loss : 0.0029, val_loss : 0.1720, acc : 0.931\n",
            "Epoch 87, loss : 0.0029, val_loss : 0.1712, acc : 0.931\n",
            "Epoch 88, loss : 0.0028, val_loss : 0.1712, acc : 0.931\n",
            "Epoch 89, loss : 0.0028, val_loss : 0.1707, acc : 0.931\n",
            "Epoch 90, loss : 0.0028, val_loss : 0.1696, acc : 0.931\n",
            "Epoch 91, loss : 0.0028, val_loss : 0.1688, acc : 0.931\n",
            "Epoch 92, loss : 0.0028, val_loss : 0.1681, acc : 0.931\n",
            "Epoch 93, loss : 0.0027, val_loss : 0.1671, acc : 0.931\n",
            "Epoch 94, loss : 0.0027, val_loss : 0.1666, acc : 0.931\n",
            "Epoch 95, loss : 0.0027, val_loss : 0.1663, acc : 0.931\n",
            "Epoch 96, loss : 0.0027, val_loss : 0.1662, acc : 0.931\n",
            "Epoch 97, loss : 0.0026, val_loss : 0.1642, acc : 0.931\n",
            "Epoch 98, loss : 0.0026, val_loss : 0.1591, acc : 0.931\n",
            "Epoch 99, loss : 0.0025, val_loss : 0.1603, acc : 0.931\n",
            "test_acc : 1.000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Problem 4:** Create a model for House Prices"
      ],
      "metadata": {
        "id": "v15nxX-KFKr3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_csv(\"train.csv\")\n",
        "X = df[['GrLivArea', 'YearBuilt']].to_numpy()\n",
        "y = df[['SalePrice']].to_numpy()\n",
        "print(\"Xshape:\", X.shape)\n",
        "print(\"yshape:\", y.shape)\n",
        "X = np.log1p(X)\n",
        "y = np.log1p(y)\n",
        "\n",
        "print(\"Xshape:\", X.shape)\n",
        "print(\"yshape:\", y.shape)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)\n",
        "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.2, random_state=0)\n",
        "\n",
        "mmsc = MinMaxScaler()\n",
        "X_train = mmsc.fit_transform(X_train)\n",
        "X_test = mmsc.transform(X_test)\n",
        "X_val = mmsc.transform(X_val)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vjOtVn02FP8s",
        "outputId": "8475f3b7-c986-4b82-ca85-2a51056a1f43"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Xshape: (1460, 2)\n",
            "yshape: (1460, 1)\n",
            "Xshape: (1460, 2)\n",
            "yshape: (1460, 1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "learning_rate = 0.001\n",
        "batch_size = 10\n",
        "num_epochs = 50\n",
        "n_hidden1 = 50\n",
        "n_hidden2 = 100\n",
        "n_input = X_train.shape[1]\n",
        "n_samples = X_train.shape[0]\n",
        "n_classes = 1\n",
        "X = tf.placeholder(\"float\", [None, n_input])\n",
        "Y = tf.placeholder(\"float\", [None, n_classes])\n",
        "\n",
        "get_mini_batch_train = GetMiniBatch(X_train, y_train, batch_size=batch_size)\n",
        "\n",
        "logits = example_net(X)\n",
        "loss_op =  tf.losses.mean_squared_error(labels=Y, predictions=logits)\n",
        "optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate)  \n",
        "train_op = optimizer.minimize(loss_op)  \n",
        "correct_pred = tf.equal(tf.sign(Y - 0.5), tf.sign(tf.sigmoid(logits) - 0.5))   \n",
        "accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32))  \n",
        "init = tf.global_variables_initializer()\n",
        "\n",
        "with tf.Session() as sess:\n",
        "    sess.run(init)\n",
        "    for epoch in range(num_epochs):\n",
        "        # Loop every epoch\n",
        "        total_batch = np.ceil(X_train.shape[0]/batch_size).astype(np.int64)\n",
        "        total_loss = 0\n",
        "        total_acc = 0\n",
        "        for i, (mini_batch_x, mini_batch_y) in enumerate(get_mini_batch_train):\n",
        "            # Loop per mini-batch\n",
        "            sess.run(train_op, feed_dict={X: mini_batch_x, Y: mini_batch_y})\n",
        "            loss, acc = sess.run([loss_op, accuracy], feed_dict={X: mini_batch_x, Y: mini_batch_y})\n",
        "            total_loss += loss\n",
        "        total_loss /= n_samples\n",
        "        val_loss, acc = sess.run([loss_op, accuracy], feed_dict={X: X_val, Y: y_val})\n",
        "        print(\"Epoch {}, loss : {:.4f}, val_loss : {:.4f}, acc : {:.3f}\".format(epoch, total_loss, val_loss, acc))\n",
        "    test_acc = sess.run(accuracy, feed_dict={X: X_test, Y: y_test})\n",
        "    print(\"test_acc : {:.3f}\".format(test_acc))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AwyEtgYPF4qU",
        "outputId": "a2ad729c-ab6b-4e17-9a14-88c66e6cba1e"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-7-30d56e557bd2>:22: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.\n",
            "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
            "  self._stop = np.ceil(X.shape[0]/self.batch_size).astype(np.int)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 0, loss : 2.1372, val_loss : 2.4993, acc : 1.000\n",
            "Epoch 1, loss : 0.1843, val_loss : 0.8833, acc : 1.000\n",
            "Epoch 2, loss : 0.0764, val_loss : 0.4178, acc : 1.000\n",
            "Epoch 3, loss : 0.0468, val_loss : 0.2916, acc : 1.000\n",
            "Epoch 4, loss : 0.0351, val_loss : 0.2206, acc : 1.000\n",
            "Epoch 5, loss : 0.0277, val_loss : 0.1818, acc : 1.000\n",
            "Epoch 6, loss : 0.0227, val_loss : 0.1583, acc : 1.000\n",
            "Epoch 7, loss : 0.0189, val_loss : 0.1381, acc : 1.000\n",
            "Epoch 8, loss : 0.0161, val_loss : 0.1214, acc : 1.000\n",
            "Epoch 9, loss : 0.0143, val_loss : 0.1061, acc : 1.000\n",
            "Epoch 10, loss : 0.0130, val_loss : 0.0945, acc : 1.000\n",
            "Epoch 11, loss : 0.0120, val_loss : 0.0853, acc : 1.000\n",
            "Epoch 12, loss : 0.0112, val_loss : 0.0780, acc : 1.000\n",
            "Epoch 13, loss : 0.0106, val_loss : 0.0733, acc : 1.000\n",
            "Epoch 14, loss : 0.0100, val_loss : 0.0709, acc : 1.000\n",
            "Epoch 15, loss : 0.0097, val_loss : 0.0705, acc : 1.000\n",
            "Epoch 16, loss : 0.0094, val_loss : 0.0714, acc : 1.000\n",
            "Epoch 17, loss : 0.0091, val_loss : 0.0739, acc : 1.000\n",
            "Epoch 18, loss : 0.0088, val_loss : 0.0762, acc : 1.000\n",
            "Epoch 19, loss : 0.0086, val_loss : 0.0753, acc : 1.000\n",
            "Epoch 20, loss : 0.0085, val_loss : 0.0748, acc : 1.000\n",
            "Epoch 21, loss : 0.0084, val_loss : 0.0753, acc : 1.000\n",
            "Epoch 22, loss : 0.0084, val_loss : 0.0753, acc : 1.000\n",
            "Epoch 23, loss : 0.0085, val_loss : 0.0759, acc : 1.000\n",
            "Epoch 24, loss : 0.0086, val_loss : 0.0757, acc : 1.000\n",
            "Epoch 25, loss : 0.0087, val_loss : 0.0798, acc : 1.000\n",
            "Epoch 26, loss : 0.0090, val_loss : 0.0903, acc : 1.000\n",
            "Epoch 27, loss : 0.0093, val_loss : 0.1128, acc : 1.000\n",
            "Epoch 28, loss : 0.0097, val_loss : 0.1393, acc : 1.000\n",
            "Epoch 29, loss : 0.0102, val_loss : 0.1812, acc : 1.000\n",
            "Epoch 30, loss : 0.0108, val_loss : 0.2226, acc : 1.000\n",
            "Epoch 31, loss : 0.0111, val_loss : 0.2654, acc : 1.000\n",
            "Epoch 32, loss : 0.0113, val_loss : 0.3040, acc : 1.000\n",
            "Epoch 33, loss : 0.0112, val_loss : 0.3239, acc : 1.000\n",
            "Epoch 34, loss : 0.0107, val_loss : 0.3293, acc : 1.000\n",
            "Epoch 35, loss : 0.0100, val_loss : 0.3148, acc : 1.000\n",
            "Epoch 36, loss : 0.0093, val_loss : 0.2915, acc : 1.000\n",
            "Epoch 37, loss : 0.0087, val_loss : 0.2607, acc : 1.000\n",
            "Epoch 38, loss : 0.0080, val_loss : 0.2249, acc : 1.000\n",
            "Epoch 39, loss : 0.0076, val_loss : 0.1873, acc : 1.000\n",
            "Epoch 40, loss : 0.0072, val_loss : 0.1504, acc : 1.000\n",
            "Epoch 41, loss : 0.0070, val_loss : 0.1187, acc : 1.000\n",
            "Epoch 42, loss : 0.0068, val_loss : 0.0979, acc : 1.000\n",
            "Epoch 43, loss : 0.0067, val_loss : 0.0850, acc : 1.000\n",
            "Epoch 44, loss : 0.0066, val_loss : 0.0779, acc : 1.000\n",
            "Epoch 45, loss : 0.0065, val_loss : 0.0745, acc : 1.000\n",
            "Epoch 46, loss : 0.0064, val_loss : 0.0697, acc : 1.000\n",
            "Epoch 47, loss : 0.0063, val_loss : 0.0618, acc : 1.000\n",
            "Epoch 48, loss : 0.0063, val_loss : 0.0541, acc : 1.000\n",
            "Epoch 49, loss : 0.0063, val_loss : 0.0498, acc : 1.000\n",
            "test_acc : 1.000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Problem 5:** Create a model for MNIST"
      ],
      "metadata": {
        "id": "DDVZeQkXIV7v"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "(X_train, y_train), (X_test, y_test) = mnist.load_data()\n",
        "\n",
        "X_train = X_train.reshape(-1, 784)\n",
        "X_test = X_test.reshape(-1, 784)\n",
        "\n",
        "X_train = X_train.astype(np.float)\n",
        "X_test = X_test.astype(np.float)\n",
        "\n",
        "X_train /= 255\n",
        "X_test /= 255\n",
        "\n",
        "y_train = y_train.astype(np.int)[:, np.newaxis]\n",
        "y_test = y_test.astype(np.int)[:, np.newaxis]\n",
        "\n",
        "enc = OneHotEncoder(handle_unknown='ignore', sparse=False)\n",
        "y_train_one_hot = enc.fit_transform(y_train[:])\n",
        "y_test_one_hot = enc.fit_transform(y_test[:])\n",
        "\n",
        "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train_one_hot, test_size=0.2)"
      ],
      "metadata": {
        "id": "6puMdwlqIdfB",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2ac344f5-73c0-4930-d60a-abcd9e11a9a5"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\n",
            "11490434/11490434 [==============================] - 0s 0us/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-14-e6886271263a>:6: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
            "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
            "  X_train = X_train.astype(np.float)\n",
            "<ipython-input-14-e6886271263a>:7: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
            "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
            "  X_test = X_test.astype(np.float)\n",
            "<ipython-input-14-e6886271263a>:12: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.\n",
            "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
            "  y_train = y_train.astype(np.int)[:, np.newaxis]\n",
            "<ipython-input-14-e6886271263a>:13: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.\n",
            "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
            "  y_test = y_test.astype(np.int)[:, np.newaxis]\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/preprocessing/_encoders.py:868: FutureWarning: `sparse` was renamed to `sparse_output` in version 1.2 and will be removed in 1.4. `sparse_output` is ignored unless you leave `sparse` to its default value.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/preprocessing/_encoders.py:868: FutureWarning: `sparse` was renamed to `sparse_output` in version 1.2 and will be removed in 1.4. `sparse_output` is ignored unless you leave `sparse` to its default value.\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "learning_rate = 0.001\n",
        "batch_size = 10\n",
        "num_epochs = 40\n",
        "n_hidden1 = 50\n",
        "n_hidden2 = 100\n",
        "n_input = X_train.shape[1]\n",
        "n_samples = X_train.shape[0]\n",
        "n_classes = 10\n",
        "\n",
        "X = tf.placeholder(\"float\", [None, n_input])\n",
        "Y = tf.placeholder(\"float\", [None, n_classes])\n",
        "\n",
        "\n",
        "get_mini_batch_train = GetMiniBatch(X_train, y_train, batch_size=batch_size)\n",
        "\n",
        "logits = example_net(X)  \n",
        "loss_op = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels=Y, logits=logits))  \n",
        "optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate)  \n",
        "train_op = optimizer.minimize(loss_op)  \n",
        "correct_pred = tf.equal(tf.argmax(Y,1), tf.argmax(tf.nn.softmax(logits),1))   \n",
        "accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32))  \n",
        "init = tf.global_variables_initializer()\n",
        "\n",
        "with tf.Session() as sess:  \n",
        "    sess.run(init)  \n",
        "    for epoch in range(num_epochs):  \n",
        "        total_batch = np.ceil(X_train.shape[0]/batch_size).astype(np.int64)  \n",
        "        total_loss = 0  \n",
        "        total_acc = 0  \n",
        "        for i, (mini_batch_x, mini_batch_y) in enumerate(get_mini_batch_train):  \n",
        "            sess.run(train_op, feed_dict={X: mini_batch_x, Y: mini_batch_y})  \n",
        "            loss, acc = sess.run([loss_op, accuracy], feed_dict={X: mini_batch_x, Y: mini_batch_y})  \n",
        "            total_loss += loss  \n",
        "        total_loss /= n_samples  \n",
        "        val_loss, acc = sess.run([loss_op, accuracy], feed_dict={X: X_val, Y: y_val})  \n",
        "        print(\"Epoch {}, loss : {:.4f}, val_loss : {:.4f}, acc : {:.3f}\".format(epoch, total_loss, val_loss, acc))  \n",
        "    test_acc = sess.run(accuracy, feed_dict={X: X_test, Y: y_test_one_hot})  \n",
        "    print(\"test_acc : {:.3f}\".format(test_acc))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "exT8En5DJTfo",
        "outputId": "8f2fc633-6fb8-44f0-db0c-962974103e41"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-7-30d56e557bd2>:22: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.\n",
            "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
            "  self._stop = np.ceil(X.shape[0]/self.batch_size).astype(np.int)\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.10/dist-packages/tensorflow/python/util/dispatch.py:1176: softmax_cross_entropy_with_logits (from tensorflow.python.ops.nn_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "\n",
            "Future major versions of TensorFlow will allow gradients to flow\n",
            "into the labels input on backprop by default.\n",
            "\n",
            "See `tf.nn.softmax_cross_entropy_with_logits_v2`.\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 0, loss : 2.7857, val_loss : 8.4334, acc : 0.794\n",
            "Epoch 1, loss : 0.5005, val_loss : 3.6549, acc : 0.833\n",
            "Epoch 2, loss : 0.2364, val_loss : 2.2367, acc : 0.845\n",
            "Epoch 3, loss : 0.1332, val_loss : 1.5854, acc : 0.851\n",
            "Epoch 4, loss : 0.0870, val_loss : 1.1859, acc : 0.853\n",
            "Epoch 5, loss : 0.0617, val_loss : 0.9793, acc : 0.862\n",
            "Epoch 6, loss : 0.0479, val_loss : 0.8601, acc : 0.876\n",
            "Epoch 7, loss : 0.0392, val_loss : 0.8195, acc : 0.880\n",
            "Epoch 8, loss : 0.0334, val_loss : 0.7861, acc : 0.885\n",
            "Epoch 9, loss : 0.0290, val_loss : 0.7148, acc : 0.892\n",
            "Epoch 10, loss : 0.0263, val_loss : 0.7145, acc : 0.896\n",
            "Epoch 11, loss : 0.0240, val_loss : 0.7001, acc : 0.896\n",
            "Epoch 12, loss : 0.0220, val_loss : 0.6713, acc : 0.902\n",
            "Epoch 13, loss : 0.0199, val_loss : 0.6634, acc : 0.910\n",
            "Epoch 14, loss : 0.0184, val_loss : 0.6619, acc : 0.908\n",
            "Epoch 15, loss : 0.0174, val_loss : 0.6458, acc : 0.914\n",
            "Epoch 16, loss : 0.0165, val_loss : 0.6312, acc : 0.915\n",
            "Epoch 17, loss : 0.0156, val_loss : 0.6610, acc : 0.913\n",
            "Epoch 18, loss : 0.0149, val_loss : 0.6736, acc : 0.911\n",
            "Epoch 19, loss : 0.0140, val_loss : 0.6522, acc : 0.919\n",
            "Epoch 20, loss : 0.0132, val_loss : 0.6371, acc : 0.920\n",
            "Epoch 21, loss : 0.0129, val_loss : 0.6585, acc : 0.920\n",
            "Epoch 22, loss : 0.0120, val_loss : 0.6582, acc : 0.925\n",
            "Epoch 23, loss : 0.0117, val_loss : 0.6545, acc : 0.923\n",
            "Epoch 24, loss : 0.0113, val_loss : 0.6441, acc : 0.927\n",
            "Epoch 25, loss : 0.0109, val_loss : 0.6702, acc : 0.920\n",
            "Epoch 26, loss : 0.0107, val_loss : 0.6437, acc : 0.926\n",
            "Epoch 27, loss : 0.0100, val_loss : 0.6575, acc : 0.927\n",
            "Epoch 28, loss : 0.0097, val_loss : 0.6652, acc : 0.929\n",
            "Epoch 29, loss : 0.0093, val_loss : 0.6612, acc : 0.928\n",
            "Epoch 30, loss : 0.0089, val_loss : 0.6679, acc : 0.930\n",
            "Epoch 31, loss : 0.0086, val_loss : 0.6941, acc : 0.923\n",
            "Epoch 32, loss : 0.0085, val_loss : 0.6749, acc : 0.932\n",
            "Epoch 33, loss : 0.0083, val_loss : 0.6918, acc : 0.928\n",
            "Epoch 34, loss : 0.0077, val_loss : 0.6783, acc : 0.930\n",
            "Epoch 35, loss : 0.0077, val_loss : 0.7155, acc : 0.932\n",
            "Epoch 36, loss : 0.0072, val_loss : 0.7212, acc : 0.927\n",
            "Epoch 37, loss : 0.0073, val_loss : 0.7040, acc : 0.935\n",
            "Epoch 38, loss : 0.0069, val_loss : 0.7123, acc : 0.931\n",
            "Epoch 39, loss : 0.0068, val_loss : 0.6989, acc : 0.934\n",
            "test_acc : 0.937\n"
          ]
        }
      ]
    }
  ]
}