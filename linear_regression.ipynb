{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyN9w0MFnKEcRVJPpBUIGoe0",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jigjid/github_task/blob/main/linear_regression.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "[Problem 1] Hypothetical function"
      ],
      "metadata": {
        "id": "c966Km63trYP"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "id": "7Ot_qNzmtnIK"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "class ScratchLinearRegression():\n",
        "    \"\"\"\n",
        "    Scratch implementation of linear regression\n",
        "    \n",
        "    Parameters\n",
        "    ----------\n",
        "    num_iter : int\n",
        "      number of iterations\n",
        "    lr : float      \n",
        "      learning rate\n",
        "    no_bias : bool\n",
        "      If we do not include the bias term True\n",
        "    verbose : bool\n",
        "      When outputting the learning process True\n",
        "    \n",
        "    Attributes\n",
        "    ----------\n",
        "    self.coef_ : ndarray, shape (n_features,)\n",
        "      parameter\n",
        "    self.loss : ndarray, shape (self.iter,)\n",
        "      Record loss on training data\n",
        "    self.val_loss : ndarray, shape (self.iter,)\n",
        "      Record Loss Against Validation Data\n",
        "    \"\"\"\n",
        "    \n",
        "    def __init__(self, num_iter, lr, no_bias, verbose):\n",
        "        # Record hyperparameters as attributes\n",
        "        self.iter = num_iter\n",
        "        self.lr = lr\n",
        "        self.no_bias = no_bias\n",
        "        self.verbose = verbose\n",
        "        # Prepare an array to record losses\n",
        "        self.loss = np.zeros(self.iter)\n",
        "        self.val_loss = np.zeros(self.iter)\n",
        "\n",
        "    def _linear_hypothesis(self, X):\n",
        "        \"\"\"   \n",
        "        compute the linear hypothesis function\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        X : ndarray, shape (n_samples, n_features)\n",
        "          training data\n",
        "\n",
        "        Returns\n",
        "        -------\n",
        "        ndarray, shape (n_samples, 1)\n",
        "        Estimation result by linear hypothesis function\n",
        "\n",
        "        \"\"\"\n",
        "        pred = X @ self.theta\n",
        "        return pred\n",
        "        \n",
        "    def fit(self, X, y, X_val=None, y_val=None):\n",
        "        \"\"\"\n",
        "        Learn linear regression. If validation data is input, the loss and accuracy for it are also calculated for each iteration.\n",
        "        Parameters\n",
        "        ----------\n",
        "        X : ndarray, shape (n_samples, n_features)\n",
        "            Features of training data\n",
        "        y : ndarray, shape (n_samples, )\n",
        "            Correct value of training data\n",
        "        X_val : ndarray, shape (n_samples, n_features)\n",
        "            Features of validation data\n",
        "        y_val : ndarray, shape (n_samples, )\n",
        "            Correct value of validation data\n",
        "        \"\"\"\n",
        "        if self.verbose:\n",
        "            #Output learning process when verbose is set to True\n",
        "            print()\n",
        "        pass\n",
        "        \n",
        "    def predict(self, X):\n",
        "        \"\"\"\n",
        "        Estimate using linear regression。\n",
        "        Parameters\n",
        "        ----------\n",
        "        X : ndarray, shape (n_samples, n_features)\n",
        "            sample\n",
        "        Returns\n",
        "        -------\n",
        "            ndarray, shape (n_samples, 1)\n",
        "            Estimation result by linear regression\n",
        "        \"\"\"\n",
        "        pass\n",
        "        return\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "【Question 2】The steepest descent method"
      ],
      "metadata": {
        "id": "918watMDt-rn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class ScratchLinearRegression():\n",
        "    \"\"\"\n",
        "    Scratch implementation of linear regression\n",
        "    \n",
        "    Parameters\n",
        "    ----------\n",
        "    num_iter : int\n",
        "      number of iterations\n",
        "    lr : float      \n",
        "      learning rate\n",
        "    no_bias : bool\n",
        "      If we do not include the bias term True\n",
        "    verbose : bool\n",
        "      When outputting the learning process True\n",
        "    \n",
        "    Attributes\n",
        "    ----------\n",
        "    self.coef_ : ndarray, shape (n_features,)\n",
        "      parameter\n",
        "    self.loss : ndarray, shape (self.iter,)\n",
        "      Record loss on training data\n",
        "    self.val_loss : ndarray, shape (self.iter,)\n",
        "      Record Loss Against Validation Data\n",
        "    \"\"\"\n",
        "    \n",
        "    def __init__(self, num_iter, lr, no_bias, verbose):\n",
        "        # Record hyperparameters as attributes\n",
        "        self.iter = num_iter\n",
        "        self.lr = lr\n",
        "        self.no_bias = no_bias\n",
        "        self.verbose = verbose\n",
        "        # Prepare an array to record losses\n",
        "        self.loss = np.zeros(self.iter)\n",
        "        self.val_loss = np.zeros(self.iter)\n",
        "\n",
        "    def _linear_hypothesis(self, X):\n",
        "        \"\"\"   \n",
        "        compute the linear hypothesis function\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        X : ndarray, shape (n_samples, n_features)\n",
        "          training data\n",
        "\n",
        "        Returns\n",
        "        -------\n",
        "        ndarray, shape (n_samples, 1)\n",
        "        Estimation result by linear hypothesis function\n",
        "\n",
        "        \"\"\"\n",
        "        pred = X @ self.theta\n",
        "        return pred\n",
        "\n",
        "    def _gradient_descent(self, X, y):\n",
        "      \"\"\"\n",
        "      gradient descent algorithm, update formula\n",
        "      \"\"\"\n",
        "      m = X.shape[0]\n",
        "      n = X.shape[1]\n",
        "      pred = self._linear_hypothesis(X)\n",
        "      for j in range(n):\n",
        "          gradient = 0\n",
        "          for i in range(m):\n",
        "              gradient += (pred[i] - y[i]) * X[i, j]\n",
        "          self.theta[j] = self.theta[j] - self.lr * (gradient / m)    \n",
        "        \n",
        "    def fit(self, X, y, X_val=None, y_val=None):\n",
        "        \"\"\"\n",
        "        Learn linear regression. If validation data is input, the loss and accuracy for it are also calculated for each iteration.\n",
        "        Parameters\n",
        "        ----------\n",
        "        X : ndarray, shape (n_samples, n_features)\n",
        "            Features of training data\n",
        "        y : ndarray, shape (n_samples, )\n",
        "            Correct value of training data\n",
        "        X_val : ndarray, shape (n_samples, n_features)\n",
        "            Features of validation data\n",
        "        y_val : ndarray, shape (n_samples, )\n",
        "            Correct value of validation data\n",
        "        \"\"\"\n",
        "        if self.verbose:\n",
        "            #Output learning process when verbose is set to True\n",
        "            print()\n",
        "        pass\n",
        "        \n",
        "    def predict(self, X):\n",
        "        \"\"\"\n",
        "        Estimate using linear regression。\n",
        "        Parameters\n",
        "        ----------\n",
        "        X : ndarray, shape (n_samples, n_features)\n",
        "            sample\n",
        "        Returns\n",
        "        -------\n",
        "            ndarray, shape (n_samples, 1)\n",
        "            Estimation result by linear regression\n",
        "        \"\"\"\n",
        "        pass\n",
        "        return"
      ],
      "metadata": {
        "id": "UpIGBHYzt95g"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "[Question 3] Presumption"
      ],
      "metadata": {
        "id": "3lkc8QNpuinO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class ScratchLinearRegression():\n",
        "    \"\"\"\n",
        "    Scratch implementation of linear regression\n",
        "    \n",
        "    Parameters\n",
        "    ----------\n",
        "    num_iter : int\n",
        "      number of iterations\n",
        "    lr : float      \n",
        "      learning rate\n",
        "    no_bias : bool\n",
        "      If we do not include the bias term True\n",
        "    verbose : bool\n",
        "      When outputting the learning process True\n",
        "    \n",
        "    Attributes\n",
        "    ----------\n",
        "    self.coef_ : ndarray, shape (n_features,)\n",
        "      parameter\n",
        "    self.loss : ndarray, shape (self.iter,)\n",
        "      Record loss on training data\n",
        "    self.val_loss : ndarray, shape (self.iter,)\n",
        "      Record Loss Against Validation Data\n",
        "    \"\"\"\n",
        "    \n",
        "    def __init__(self, num_iter, lr, no_bias, verbose):\n",
        "        # Record hyperparameters as attributes\n",
        "        self.iter = num_iter\n",
        "        self.lr = lr\n",
        "        self.no_bias = no_bias\n",
        "        self.verbose = verbose\n",
        "        # Prepare an array to record losses\n",
        "        self.loss = np.zeros(self.iter)\n",
        "        self.val_loss = np.zeros(self.iter)\n",
        "\n",
        "    def _linear_hypothesis(self, X):\n",
        "        \"\"\"   \n",
        "        compute the linear hypothesis function\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        X : ndarray, shape (n_samples, n_features)\n",
        "          training data\n",
        "\n",
        "        Returns\n",
        "        -------\n",
        "        ndarray, shape (n_samples, 1)\n",
        "        Estimation result by linear hypothesis function\n",
        "\n",
        "        \"\"\"\n",
        "        pred = X @ self.theta\n",
        "        return pred\n",
        "\n",
        "    def _gradient_descent(self, X, y):\n",
        "      \"\"\"\n",
        "      gradient descent algorithm, update formula\n",
        "      \"\"\"\n",
        "      m = X.shape[0]\n",
        "      n = X.shape[1]\n",
        "      pred = self._linear_hypothesis(X)\n",
        "      for j in range(n):\n",
        "          gradient = 0\n",
        "          for i in range(m):\n",
        "              gradient += (pred[i] - y[i]) * X[i, j]\n",
        "          self.theta[j] = self.theta[j] - self.lr * (gradient / m)    \n",
        "        \n",
        "    def fit(self, X, y, X_val=None, y_val=None):\n",
        "        \"\"\"\n",
        "        Learn linear regression. If validation data is input, the loss and accuracy for it are also calculated for each iteration.\n",
        "        Parameters\n",
        "        ----------\n",
        "        X : ndarray, shape (n_samples, n_features)\n",
        "            Features of training data\n",
        "        y : ndarray, shape (n_samples, )\n",
        "            Correct value of training data\n",
        "        X_val : ndarray, shape (n_samples, n_features)\n",
        "            Features of validation data\n",
        "        y_val : ndarray, shape (n_samples, )\n",
        "            Correct value of validation data\n",
        "        \"\"\"\n",
        "        if self.verbose:\n",
        "            #Output learning process when verbose is set to True\n",
        "            print()\n",
        "        pass\n",
        "        \n",
        "    def predict(self, X):\n",
        "        \"\"\"\n",
        "        Estimate using linear regression。\n",
        "        Parameters\n",
        "        ----------\n",
        "        X : ndarray, shape (n_samples, n_features)\n",
        "            sample\n",
        "        Returns\n",
        "        -------\n",
        "            ndarray, shape (n_samples, 1)\n",
        "            Estimation result by linear regression\n",
        "        \"\"\"\n",
        "        if self.bias == True:\n",
        "            bias = np.ones(X.shape[0]).reshape(X.shape[0], 1)\n",
        "            X = np.hstack([bias, X])\n",
        "        pred_y = self._linear_hypothesis(X)\n",
        "        return pred_y"
      ],
      "metadata": {
        "id": "JnjOV2RDuuDN"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "[Question 4] Mean squared error"
      ],
      "metadata": {
        "id": "pXKPwHUAuxAt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class ScratchLinearRegression():\n",
        "    \"\"\"\n",
        "    Scratch implementation of linear regression\n",
        "    \n",
        "    Parameters\n",
        "    ----------\n",
        "    num_iter : int\n",
        "      number of iterations\n",
        "    lr : float      \n",
        "      learning rate\n",
        "    no_bias : bool\n",
        "      If we do not include the bias term True\n",
        "    verbose : bool\n",
        "      When outputting the learning process True\n",
        "    \n",
        "    Attributes\n",
        "    ----------\n",
        "    self.coef_ : ndarray, shape (n_features,)\n",
        "      parameter\n",
        "    self.loss : ndarray, shape (self.iter,)\n",
        "      Record loss on training data\n",
        "    self.val_loss : ndarray, shape (self.iter,)\n",
        "      Record Loss Against Validation Data\n",
        "    \"\"\"\n",
        "    \n",
        "    def __init__(self, num_iter, lr, no_bias, verbose):\n",
        "        # Record hyperparameters as attributes\n",
        "        self.iter = num_iter\n",
        "        self.lr = lr\n",
        "        self.no_bias = no_bias\n",
        "        self.verbose = verbose\n",
        "        # Prepare an array to record losses\n",
        "        self.loss = np.zeros(self.iter)\n",
        "        self.val_loss = np.zeros(self.iter)\n",
        "\n",
        "    def _linear_hypothesis(self, X):\n",
        "        \"\"\"   \n",
        "        compute the linear hypothesis function\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        X : ndarray, shape (n_samples, n_features)\n",
        "          training data\n",
        "\n",
        "        Returns\n",
        "        -------\n",
        "        ndarray, shape (n_samples, 1)\n",
        "        Estimation result by linear hypothesis function\n",
        "\n",
        "        \"\"\"\n",
        "        pred = X @ self.theta\n",
        "        return pred\n",
        "\n",
        "    def _gradient_descent(self, X, y):\n",
        "      \"\"\"\n",
        "      gradient descent algorithm, update formula\n",
        "      \"\"\"\n",
        "      m = X.shape[0]\n",
        "      n = X.shape[1]\n",
        "      pred = self._linear_hypothesis(X)\n",
        "      for j in range(n):\n",
        "          gradient = 0\n",
        "          for i in range(m):\n",
        "              gradient += (pred[i] - y[i]) * X[i, j]\n",
        "          self.theta[j] = self.theta[j] - self.lr * (gradient / m)    \n",
        "        \n",
        "    def fit(self, X, y, X_val=None, y_val=None):\n",
        "        \"\"\"\n",
        "        Learn linear regression. If validation data is input, the loss and accuracy for it are also calculated for each iteration.\n",
        "        Parameters\n",
        "        ----------\n",
        "        X : ndarray, shape (n_samples, n_features)\n",
        "            Features of training data\n",
        "        y : ndarray, shape (n_samples, )\n",
        "            Correct value of training data\n",
        "        X_val : ndarray, shape (n_samples, n_features)\n",
        "            Features of validation data\n",
        "        y_val : ndarray, shape (n_samples, )\n",
        "            Correct value of validation data\n",
        "        \"\"\"\n",
        "        if self.verbose:\n",
        "            #Output learning process when verbose is set to True\n",
        "            print()\n",
        "        pass\n",
        "        \n",
        "    def predict(self, X):\n",
        "        \"\"\"\n",
        "        Estimate using linear regression。\n",
        "        Parameters\n",
        "        ----------\n",
        "        X : ndarray, shape (n_samples, n_features)\n",
        "            sample\n",
        "        Returns\n",
        "        -------\n",
        "            ndarray, shape (n_samples, 1)\n",
        "            Estimation result by linear regression\n",
        "        \"\"\"\n",
        "        if self.bias == True:\n",
        "            bias = np.ones(X.shape[0]).reshape(X.shape[0], 1)\n",
        "            X = np.hstack([bias, X])\n",
        "        pred_y = self._linear_hypothesis(X)\n",
        "        return pred_y\n",
        "\n",
        "    def MSE(y_pred, y):\n",
        "        \"\"\"\n",
        "            Calculate Mean Squared Error\n",
        "\n",
        "            Parameters\n",
        "            ----------\n",
        "            y_pred : ndarray, shape (n_samples,)\n",
        "              estimated value\n",
        "            y : 次の形のndarray, shape (n_samples,)\n",
        "              correct answer\n",
        "\n",
        "            Returns\n",
        "            ----------\n",
        "            mse : numpy.float\n",
        "              mean squared error\n",
        "            \"\"\"\n",
        "        mse = ((y_pred - y) ** 2).sum() / X.shape[0]\n",
        "        return mse"
      ],
      "metadata": {
        "id": "lHIXM0yquwQl"
      },
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "[Question 5] Objective function"
      ],
      "metadata": {
        "id": "0qIwpw0uu4ed"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class ScratchLinearRegression():\n",
        "    \"\"\"\n",
        "    Scratch implementation of linear regression\n",
        "    \n",
        "    Parameters\n",
        "    ----------\n",
        "    num_iter : int\n",
        "      number of iterations\n",
        "    lr : float      \n",
        "      learning rate\n",
        "    no_bias : bool\n",
        "      If we do not include the bias term True\n",
        "    verbose : bool\n",
        "      When outputting the learning process True\n",
        "    \n",
        "    Attributes\n",
        "    ----------\n",
        "    self.coef_ : ndarray, shape (n_features,)\n",
        "      parameter\n",
        "    self.loss : ndarray, shape (self.iter,)\n",
        "      Record loss on training data\n",
        "    self.val_loss : ndarray, shape (self.iter,)\n",
        "      Record Loss Against Validation Data\n",
        "    \"\"\"\n",
        "    \n",
        "    def __init__(self, num_iter, lr, no_bias, verbose):\n",
        "        # Record hyperparameters as attributes\n",
        "        self.iter = num_iter\n",
        "        self.lr = lr\n",
        "        self.no_bias = no_bias\n",
        "        self.verbose = verbose\n",
        "        # Prepare an array to record losses\n",
        "        self.loss = np.zeros(self.iter)\n",
        "        self.val_loss = np.zeros(self.iter)\n",
        "\n",
        "    def _linear_hypothesis(self, X):\n",
        "        \"\"\"   \n",
        "        compute the linear hypothesis function\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        X : ndarray, shape (n_samples, n_features)\n",
        "          training data\n",
        "\n",
        "        Returns\n",
        "        -------\n",
        "        ndarray, shape (n_samples, 1)\n",
        "        Estimation result by linear hypothesis function\n",
        "\n",
        "        \"\"\"\n",
        "        pred = X @ self.theta\n",
        "        return pred\n",
        "\n",
        "    def _gradient_descent(self, X, y):\n",
        "      \"\"\"\n",
        "      gradient descent algorithm, update formula\n",
        "      \"\"\"\n",
        "      m = X.shape[0]\n",
        "      n = X.shape[1]\n",
        "      pred = self._linear_hypothesis(X)\n",
        "      for j in range(n):\n",
        "          gradient = 0\n",
        "          for i in range(m):\n",
        "              gradient += (pred[i] - y[i]) * X[i, j]\n",
        "          self.theta[j] = self.theta[j] - self.lr * (gradient / m)    \n",
        "        \n",
        "    def fit(self, X, y, X_val=None, y_val=None):\n",
        "        \"\"\"\n",
        "        Learn linear regression. If validation data is input, the loss and accuracy for it are also calculated for each iteration.\n",
        "        Parameters\n",
        "        ----------\n",
        "        X : ndarray, shape (n_samples, n_features)\n",
        "            Features of training data\n",
        "        y : ndarray, shape (n_samples, )\n",
        "            Correct value of training data\n",
        "        X_val : ndarray, shape (n_samples, n_features)\n",
        "            Features of validation data\n",
        "        y_val : ndarray, shape (n_samples, )\n",
        "            Correct value of validation data\n",
        "        \"\"\"\n",
        "        if self.verbose:\n",
        "            #Output learning process when verbose is set to True\n",
        "            print()\n",
        "        pass\n",
        "        \n",
        "    def predict(self, X):\n",
        "        \"\"\"\n",
        "        Estimate using linear regression。\n",
        "        Parameters\n",
        "        ----------\n",
        "        X : ndarray, shape (n_samples, n_features)\n",
        "            sample\n",
        "        Returns\n",
        "        -------\n",
        "            ndarray, shape (n_samples, 1)\n",
        "            Estimation result by linear regression\n",
        "        \"\"\"\n",
        "        if self.bias == True:\n",
        "            bias = np.ones(X.shape[0]).reshape(X.shape[0], 1)\n",
        "            X = np.hstack([bias, X])\n",
        "        pred_y = self._linear_hypothesis(X)\n",
        "        return pred_y\n",
        "\n",
        "    def MSE(y_pred, y):\n",
        "        \"\"\"\n",
        "            Calculate Mean Squared Error\n",
        "\n",
        "            Parameters\n",
        "            ----------\n",
        "            y_pred : ndarray, shape (n_samples,)\n",
        "              estimated value\n",
        "            y : 次の形のndarray, shape (n_samples,)\n",
        "              correct answer\n",
        "\n",
        "            Returns\n",
        "            ----------\n",
        "            mse : numpy.float\n",
        "              mean squared error\n",
        "            \"\"\"\n",
        "        mse = ((y_pred - y) ** 2).sum() / X.shape[0]\n",
        "        return mse\n",
        "\n",
        "    def _loss_func(self,y_pred, y):\n",
        "        loss = self.MSE(pred, y)/2\n",
        "        return loss"
      ],
      "metadata": {
        "id": "ohV5ZKEgu0mt"
      },
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "[Question 6] Learning and Estimation"
      ],
      "metadata": {
        "id": "bKY0UULcu79F"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class ScratchLinearRegression():\n",
        "    \"\"\"\n",
        "    Scratch implementation of linear regression\n",
        "    \n",
        "    Parameters\n",
        "    ----------\n",
        "    num_iter : int\n",
        "      number of iterations\n",
        "    lr : float      \n",
        "      learning rate\n",
        "    no_bias : bool\n",
        "      If we do not include the bias term True\n",
        "    verbose : bool\n",
        "      When outputting the learning process True\n",
        "    \n",
        "    Attributes\n",
        "    ----------\n",
        "    self.coef_ : ndarray, shape (n_features,)\n",
        "      parameter\n",
        "    self.loss : ndarray, shape (self.iter,)\n",
        "      Record loss on training data\n",
        "    self.val_loss : ndarray, shape (self.iter,)\n",
        "      Record Loss Against Validation Data\n",
        "    \"\"\"\n",
        "    \n",
        "    def __init__(self, num_iter, lr, no_bias, verbose):\n",
        "        # Record hyperparameters as attributes\n",
        "        self.iter = num_iter\n",
        "        self.lr = lr\n",
        "        self.no_bias = no_bias\n",
        "        self.verbose = verbose\n",
        "        # Prepare an array to record losses\n",
        "        #self.loss = np.zeros(self.iter)\n",
        "        #self.val_loss = np.zeros(self.iter)\n",
        "        self.theta = np.array([])\n",
        "        self.loss = np.array([])\n",
        "        self.val_loss = np.array([])\n",
        "\n",
        "    def _linear_hypothesis(self, X):\n",
        "        \"\"\"   \n",
        "        compute the linear hypothesis function\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        X : ndarray, shape (n_samples, n_features)\n",
        "          training data\n",
        "\n",
        "        Returns\n",
        "        -------\n",
        "        ndarray, shape (n_samples, 1)\n",
        "        Estimation result by linear hypothesis function\n",
        "\n",
        "        \"\"\"\n",
        "        y_pred = X @ self.theta\n",
        "        return y_pred\n",
        "\n",
        "    def _gradient_descent(self, X, y):\n",
        "      \"\"\"\n",
        "      gradient descent algorithm, update formula\n",
        "      \"\"\"\n",
        "      m = X.shape[0]\n",
        "      n = X.shape[1]\n",
        "      y_pred = self._linear_hypothesis(X)\n",
        "      for j in range(n):\n",
        "          gradient = 0\n",
        "          for i in range(m):\n",
        "              gradient += (y_pred[i] - y[i]) * X[i, j]\n",
        "          self.theta[j] = self.theta[j] - self.lr * (gradient / m)   \n",
        "\n",
        "    def MSE(self, y_pred, y):\n",
        "        \"\"\"\n",
        "            Calculate Mean Squared Error\n",
        "\n",
        "            Parameters\n",
        "            ----------\n",
        "            y_pred : ndarray, shape (n_samples,)\n",
        "              estimated value\n",
        "            y : 次の形のndarray, shape (n_samples,)\n",
        "              correct answer\n",
        "\n",
        "            Returns\n",
        "            ----------\n",
        "            mse : numpy.float\n",
        "              mean squared error\n",
        "            \"\"\"\n",
        "        mse = ((y_pred - y) ** 2).sum() / X.shape[0]\n",
        "        return mse\n",
        "\n",
        "    def _loss_func(self, y_pred, y):\n",
        "        loss = self.MSE(y_pred, y)/2\n",
        "        return loss \n",
        "        \n",
        "    def fit(self, X, y, X_val, y_val):\n",
        "        \"\"\"\n",
        "        Learn linear regression. If validation data is input, the loss and accuracy for it are also calculated for each iteration.\n",
        "        Parameters\n",
        "        ----------\n",
        "        X : ndarray, shape (n_samples, n_features)\n",
        "            Features of training data\n",
        "        y : ndarray, shape (n_samples, )\n",
        "            Correct value of training data\n",
        "        X_val : ndarray, shape (n_samples, n_features)\n",
        "            Features of validation data\n",
        "        y_val : ndarray, shape (n_samples, )\n",
        "            Correct value of validation data\n",
        "        \"\"\"        \n",
        "        if self.no_bias == True:\n",
        "            no_bias = np.ones((X.shape[0], 1))\n",
        "            X = np.hstack((no_bias, X))\n",
        "            no_bias = np.ones((X_val.shape[0], 1))\n",
        "            X_val = np.hstack((no_bias, X_val))\n",
        "        self.theta = np.zeros(X.shape[1])\n",
        "        self.theta = self.theta.reshape(X.shape[1], 1)\n",
        "        for i in range(self.iter):\n",
        "            pred = self._linear_hypothesis(X)\n",
        "            pred_val = self._linear_hypothesis(X_val)\n",
        "            self._gradient_descent(X, y)\n",
        "            loss = self._loss_func(pred, y)\n",
        "            self.loss = np.append(self.loss, loss)\n",
        "            loss_val = self._loss_func(pred_val, y_val)\n",
        "            self.val_loss = np.append(self.val_loss, loss_val)\n",
        "            if self.verbose:\n",
        "                #Output learning process when verbose is set to True\n",
        "                print('The {}th training loss is{}'.format(i,loss))    \n",
        "        \n",
        "    def predict(self, X):\n",
        "        \"\"\"\n",
        "        Estimate using linear regression。\n",
        "        Parameters\n",
        "        ----------\n",
        "        X : ndarray, shape (n_samples, n_features)\n",
        "            sample\n",
        "        Returns\n",
        "        -------\n",
        "            ndarray, shape (n_samples, 1)\n",
        "            Estimation result by linear regression\n",
        "        \"\"\"\n",
        "        if self.bias == True:\n",
        "            bias = np.ones(X.shape[0]).reshape(X.shape[0], 1)\n",
        "            X = np.hstack([bias, X])\n",
        "        pred_y = self._linear_hypothesis(X)\n",
        "        return pred_y\n",
        "\n",
        "    "
      ],
      "metadata": {
        "id": "esso1G72u-Os"
      },
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "dataset = pd.read_csv(\"train.csv\")\n",
        "X = dataset.loc[:, ['GrLivArea', 'YearBuilt']]\n",
        "y = dataset.loc[:, ['SalePrice']]\n",
        "X = X.values\n",
        "y = y.values\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, train_size=0.8)\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "scaler = StandardScaler()\n",
        "X_train = scaler.fit_transform(X_train)\n",
        "X_test = scaler.fit_transform(X_test)\n",
        "slr = ScratchLinearRegression(num_iter=100, lr=0.01, no_bias=True, verbose=True)\n",
        "slr.fit(X_train, y_train, X_test, y_test)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kXiuVrGxvEl8",
        "outputId": "58032258-1db0-450a-b436-b5b55b29b13b"
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The 0th training loss is15440190800.468151\n",
            "The 1th training loss is15143831716.671555\n",
            "The 2th training loss is14853531585.762093\n",
            "The 3th training loss is14569165895.376436\n",
            "The 4th training loss is14290612706.870934\n",
            "The 5th training loss is14017752601.77207\n",
            "The 6th training loss is13750468629.349209\n",
            "The 7th training loss is13488646255.285927\n",
            "The 8th training loss is13232173311.42676\n",
            "The 9th training loss is12980939946.576584\n",
            "The 10th training loss is12734838578.330515\n",
            "The 11th training loss is12493763845.91243\n",
            "The 12th training loss is12257612564.000988\n",
            "The 13th training loss is12026283677.522131\n",
            "The 14th training loss is11799678217.387846\n",
            "The 15th training loss is11577699257.161098\n",
            "The 16th training loss is11360251870.627462\n",
            "The 17th training loss is11147243090.25433\n",
            "The 18th training loss is10938581866.518959\n",
            "The 19th training loss is10734179028.087072\n",
            "The 20th training loss is10533947242.824064\n",
            "The 21th training loss is10337800979.621271\n",
            "The 22th training loss is10145656471.020157\n",
            "The 23th training loss is9957431676.617577\n",
            "The 24th training loss is9773046247.235672\n",
            "The 25th training loss is9592421489.840334\n",
            "The 26th training loss is9415480333.192429\n",
            "The 27th training loss is9242147294.216375\n",
            "The 28th training loss is9072348445.07101\n",
            "The 29th training loss is8906011380.907915\n",
            "The 30th training loss is8743065188.30276\n",
            "The 31th training loss is8583440414.345545\n",
            "The 32th training loss is8427069036.375827\n",
            "The 33th training loss is8273884432.349387\n",
            "The 34th training loss is8123821351.823102\n",
            "The 35th training loss is7976815887.544968\n",
            "The 36th training loss is7832805447.6366\n",
            "The 37th training loss is7691728728.355748\n",
            "The 38th training loss is7553525687.426632\n",
            "The 39th training loss is7418137517.9261875\n",
            "The 40th training loss is7285506622.714544\n",
            "The 41th training loss is7155576589.3982935\n",
            "The 42th training loss is7028292165.815395\n",
            "The 43th training loss is6903599236.030721\n",
            "The 44th training loss is6781444796.83159\n",
            "The 45th training loss is6661776934.712732\n",
            "The 46th training loss is6544544803.340469\n",
            "The 47th training loss is6429698601.486029\n",
            "The 48th training loss is6317189551.41816\n",
            "The 49th training loss is6206969877.74544\n",
            "The 50th training loss is6098992786.698813\n",
            "The 51th training loss is5993212445.84516\n",
            "The 52th training loss is5889583964.222855\n",
            "The 53th training loss is5788063372.890453\n",
            "The 54th training loss is5688607605.879902\n",
            "The 55th training loss is5591174481.545737\n",
            "The 56th training loss is5495722684.302023\n",
            "The 57th training loss is5402211746.738892\n",
            "The 58th training loss is5310602032.110756\n",
            "The 59th training loss is5220854717.188379\n",
            "The 60th training loss is5132931775.467235\n",
            "The 61th training loss is5046795960.724653\n",
            "The 62th training loss is4962410790.9185095\n",
            "The 63th training loss is4879740532.420252\n",
            "The 64th training loss is4798750184.575325\n",
            "The 65th training loss is4719405464.584114\n",
            "The 66th training loss is4641672792.696737\n",
            "The 67th training loss is4565519277.715075\n",
            "The 68th training loss is4490912702.795679\n",
            "The 69th training loss is4417821511.54723\n",
            "The 70th training loss is4346214794.4163885\n",
            "The 71th training loss is4276062275.356057\n",
            "The 72th training loss is4207334298.7700925\n",
            "The 73th training loss is4140001816.7287593\n",
            "The 74th training loss is4074036376.449224\n",
            "The 75th training loss is4009410108.035571\n",
            "The 76th training loss is3946095712.47294\n",
            "The 77th training loss is3884066449.8704557\n",
            "The 78th training loss is3823296127.947779\n",
            "The 79th training loss is3763759090.760186\n",
            "The 80th training loss is3705430207.657197\n",
            "The 81th training loss is3648284862.4698987\n",
            "The 82th training loss is3592298942.922178\n",
            "The 83th training loss is3537448830.261202\n",
            "The 84th training loss is3483711389.102574\n",
            "The 85th training loss is3431063957.485698\n",
            "The 86th training loss is3379484337.134958\n",
            "The 87th training loss is3328950783.922435\n",
            "The 88th training loss is3279441998.5279613\n",
            "The 89th training loss is3230937117.2923894\n",
            "The 90th training loss is3183415703.260076\n",
            "The 91th training loss is3136857737.406619\n",
            "The 92th training loss is3091243610.0479956\n",
            "The 93th training loss is3046554112.4273353\n",
            "The 94th training loss is3002770428.475621\n",
            "The 95th training loss is2959874126.7427044\n",
            "The 96th training loss is2917847152.495081\n",
            "The 97th training loss is2876671819.976974\n",
            "The 98th training loss is2836330804.831314\n",
            "The 99th training loss is2796807136.6773014\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "[Question 7] Learning curve plot"
      ],
      "metadata": {
        "id": "VfChFo6vvd1l"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "plt.plot(slr.loss)\n",
        "plt.plot(slr.val_loss)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 462
        },
        "id": "PIvBpBokvFNk",
        "outputId": "955eaea9-bf48-412c-b2cd-a44176457015"
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[<matplotlib.lines.Line2D at 0x7fe5ed42eeb0>]"
            ]
          },
          "metadata": {},
          "execution_count": 41
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAiMAAAGsCAYAAAAPJKchAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjcuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/bCgiHAAAACXBIWXMAAA9hAAAPYQGoP6dpAABPVUlEQVR4nO3deVxU9d4H8M8sMOzDvg+gIG4IoijiUpqWmnmvdStLy6W9tCxvt7RFb5vW7dZji2V5TbNyLbVF00xTsnABRVFEVFAQ2ZcZ9mXmPH8cHEQhGQUOM/N5v16/1zBnmflynq58nnN+i0wQBAFEREREEpFLXQARERFZN4YRIiIikhTDCBEREUmKYYSIiIgkxTBCREREkmIYISIiIkkxjBAREZGkGEaIiIhIUgwjREREJCmGESIiIpKUWYWR+Ph4TJw4Ef7+/pDJZNiyZYtJ59fU1GDGjBno168flEolJk2a1OJxe/bswYABA6BSqRAWFoZVq1bdcO1ERETUMrMKI5WVlYiKisLSpUuv63y9Xg97e3s888wzGDNmTIvHZGZmYsKECRg1ahSSk5Px7LPP4pFHHsGOHTtupHQiIiJqhcxcF8qTyWTYvHlzs7sbtbW1ePnll7F27VqUlZUhIiIC77zzDkaOHHnV+TNmzEBZWdlVd1defPFFbN26FcePHzduu++++1BWVobt27d30G9DRERkvczqzsi1zJ49GwkJCVi3bh2OHTuGe+65B+PGjcPp06fb/BkJCQlX3TUZO3YsEhIS2rtcIiIiggWFkaysLKxcuRIbN27EiBEjEBoaiueffx7Dhw/HypUr2/w5eXl58PHxabbNx8cHOp0O1dXV7V02ERGR1VNKXUB7SUlJgV6vR3h4eLPttbW18PDwkKgqIiIiuhaLCSMVFRVQKBRISkqCQqFots/JyanNn+Pr64v8/Pxm2/Lz8+Hi4gJ7e/t2qZWIiIiaWEwYiY6Ohl6vR0FBAUaMGHHdnxMXF4dt27Y127Zz507ExcXdaIlERETUArMKIxUVFThz5ozxfWZmJpKTk+Hu7o7w8HBMnToV06ZNw3vvvYfo6GgUFhZi165diIyMxIQJEwAAqampqKurQ0lJCcrLy5GcnAwA6N+/PwDgiSeewMcff4wXXngBDz30EHbv3o0NGzZg69atnf3rEhERWQWzGtq7Z88ejBo16qrt06dPx6pVq1BfX48333wTq1evRk5ODjw9PTFkyBC89tpr6NevHwAgJCQE58+fv+ozLr8Me/bswXPPPYfU1FQEBgbi1VdfxYwZMzrs9yIiIrJmZhVGiIiIyPJYzNBeIiIiMk8MI0RERCQps+jAajAYcPHiRTg7O0Mmk0ldDhEREbWBIAgoLy+Hv78/5PK/uP8hmGjv3r3CHXfcIfj5+QkAhM2bN1/znJqaGuGll14SgoKCBFtbWyE4OFhYsWJFm78zOztbAMDGxsbGxsZmhi07O/sv/86bfGfk0sq5Dz30EO666642nXPvvfciPz8fK1asQFhYGHJzc2EwGNr8nc7OzgCA7OxsuLi4mFoyERERSUCn00Gj0Rj/jrfG5DAyfvx4jB8/vs3Hb9++HXv37kVGRgbc3d0BiMNrTXHp0YyLiwvDCBERkZm5VheLDu/A+sMPPyAmJgb/+c9/EBAQgPDwcDz//PN/uehcbW0tdDpds0ZERESWqcM7sGZkZGDfvn2ws7PD5s2bUVRUhKeeegrFxcWtrqa7ePFivPbaax1dGhEREXUBHX5nxGAwQCaT4ZtvvsHgwYNx++234/3338eXX37Z6t2R+fPnQ6vVGlt2dnZHl0lEREQS6fA7I35+fggICIBarTZu6927NwRBwIULF9CjR4+rzlGpVFCpVB1dGhEREXUBHX5nZNiwYbh48SIqKiqM29LT0yGXyxEYGNjRX09ERERdnMlhpKKiAsnJycbVbi+tnJuVlQVAfMQybdo04/FTpkyBh4cHZs6cidTUVMTHx+Nf//oXHnroIdjb27fPb0FERERmy+QwkpiYiOjoaERHRwMA5s6di+joaCxYsAAAkJubawwmAODk5ISdO3eirKwMMTExmDp1KiZOnIgPP/ywnX4FIiIiMmdmsWqvTqeDWq2GVqvlPCNERERmoq1/v7lQHhEREUmKYYSIiIgkxTBCREREkmIYISIiIklZdRhJuaDF/Z/vR3FFrdSlEBERWS2rDSOCIGDepmNIyCjGgu9PSF0OERGR1bLaMCKTyfDOPyKhkMuwNSUXPx27KHVJREREVslqwwgARASoMWtkKABgwfcnUMTHNURERJ3OqsMIAMy+pQd6+TqjpLIOr245DjOYA46IiMiiWH0YsVXK8d69UVDKZfj5eB5+PJYrdUlERERWxerDCAD09Vdj9i1hAIAF3x9HQXmNxBURERFZD4aRRrNGhaGPnwvKqurxymY+riEiIuosDCONbBTi4xobhQy/pOZj85EcqUsiIiKyCgwjl+nt54Jnx4QDABZ+fwIXy6olroiIiMjyMYxc4fGbuiM6yBXltQ144dtjMBj4uIaIiKgjMYxcQamQ4717omBnI8e+M0X4+sB5qUsiIiKyaAwjLeju5YT543sDABZtO4mMwgqJKyIiIrJcDCOteHBIMIaFeaCm3oB/bjyKBr1B6pKIiIgsEsNIK+RyGd69OwrOKiWOZJXhs/gMqUsiIiKySAwjf8Hf1R7//ltfAMD/7UxHygWtxBURERFZHoaRa7hrQADGR/iiwSDg2fVHUFOvl7okIiIii8Iwcg0ymQyL7uwHb2cVzhZW4u2f06QuiYiIyKIwjLSBm6Mt3r0nCgCw6s9ziE8vlLgiIiIiy8Ew0kY3h3thelwwAOD5jUdRWlkncUVERESWgWHEBPPG90aolyMKymvx8pYULqZHRETUDhhGTGBvq8AH90VDKZdhW0oevk26IHVJREREZo9hxEQRAWrMva1xMb0fTuBcUaXEFREREZk3hpHr8PhNoYjt5o6qOj3mrE9GPWdnJSIium4MI9dBIZfh/yb3h4udEkezy/DBr6elLomIiMhsMYxcJ39Xeyy+KxIAsHTPGRzIKJa4IiIiIvPEMHIDJkT64Z6BgRAE4Ln1ydBW1UtdEhERkdlhGLlBC//WF8EeDriorcFLmzncl4iIyFQMIzfISaU0DvfdmpKLDYnZUpdERERkVhhG2kF/jSv+eVtPAOJw3zMF5RJXREREZD5MDiPx8fGYOHEi/P39IZPJsGXLljaf+8cff0CpVKJ///6mfm2X9/hN3TE8zBM19QbMXsPVfYmIiNrK5DBSWVmJqKgoLF261KTzysrKMG3aNIwePdrUrzQLcrkM798bBQ9HW6TllWPxtpNSl0RERGQWTA4j48ePx5tvvok777zTpPOeeOIJTJkyBXFxcaZ+pdnwdrHDe/eKq/t+mXAeO1PzJa6IiIio6+uUPiMrV65ERkYGFi5c2Kbja2trodPpmjVzMbKnNx4d0Q0A8K9vjyJXWy1xRURERF1bh4eR06dPY968efj666+hVCrbdM7ixYuhVquNTaPRdHCV7etfY3uhX4AaZVX1eGbtETRwungiIqJWdWgY0ev1mDJlCl577TWEh4e3+bz58+dDq9UaW3a2eQ2XtVXK8fGUaDiplDh0rhRLOF08ERFRqzo0jJSXlyMxMRGzZ8+GUqmEUqnE66+/jqNHj0KpVGL37t0tnqdSqeDi4tKsmZtgD0e8/Y9+AMTp4n8/XShxRURERF1Th4YRFxcXpKSkIDk52dieeOIJ9OzZE8nJyYiNje3Ir5fcHZH+mBIbZJwuvkBXI3VJREREXU7bOnFcpqKiAmfOnDG+z8zMRHJyMtzd3REUFIT58+cjJycHq1evhlwuR0RERLPzvb29YWdnd9V2S7Xgjj44fL4UaXnlmLMuGV8/EguFXCZ1WURERF2GyXdGEhMTER0djejoaADA3LlzER0djQULFgAAcnNzkZWV1b5VmjE7GwU+njIADrYKJGQU46Pd7D9CRER0OZlgBiu76XQ6qNVqaLVas+w/AgCbDl/A3A1HIZMBXz8ci2FhnlKXRERE1KHa+veba9N0krsGBOLemEAIAjBn3RHks/8IERERAIaRTvX63yPQy9cZRRV1eJrzjxAREQFgGOlUdjYKfDJ1AJxUShzMLMH7O9OlLomIiEhyDCOdrLuXk3H+kU/2nMVvaQUSV0RERCQthhEJ3BHpj+lxwQCA5zYk40JplcQVERERSYdhRCIvTeiNqEBx/ZpZ3xxGbYNe6pKIiIgkwTAiEZVSgaVTB8DVwQZHL2jx+o+pUpdEREQkCYYRCQW6OWDJ5P6QyYBvDmThu6QLUpdERETU6RhGJDaypzfmjO4BAHh5SwpO5uokroiIiKhzMYx0Ac/c0gM3h3uhpt6AJ79Ogq6mXuqSiIiIOg3DSBcgl8uwZHJ/BLja41xxFf654SgMhi4/Sz8REVG7YBjpItwcbfHpAwNgq5BjZ2o+Ptlz5tonERERWQCGkS4kMtAVb0zqCwB4b2c69pzihGhERGT5GEa6mMmDgjAlNqhxQb1kZBVzQjQiIrJsDCNd0MKJfdBf4wptdT0e+yoRVXUNUpdERETUYRhGuiCVUoFPHxgATydbpOWVY/6mFAgCO7QSEZFlYhjpovzU9lg6ZQAUchm+T76IFfsypS6JiIioQzCMdGGx3T3wyoTeAIBF205i3+kiiSsiIiJqfwwjXdyMoSH4x4BAGARg9trD7NBKREQWh2Gki5PJZHjrzgjjCr+PfZWIylp2aCUiIsvBMGIG7GwUWPbgQHg6qZCWV45/fXuUHVqJiMhiMIyYCT+1PT57cABsFDJsS8nD0t84QysREVkGhhEzMjDYHa//PQKAOEPrztR8iSsiIiK6cQwjZub+wUF4cEgwBAF4dt0RpOXppC6JiIjohjCMmKEFE/sgrrsHKuv0eOTLRJRU1kldEhER0XVjGDFDNgo5Ppk6AMEeDrhQWo0nv05CXYNB6rKIiIiuC8OImXJztMX/psXASaXEgcwS/PvHExxhQ0REZolhxIz18HHGB/f1h0wGrDmQhdUJ56UuiYiIyGQMI2ZudG8fvDiuFwDgtR9PYM+pAokrIiIiMg3DiAV4/Kbuxinjn15zBKfzy6UuiYiIqM0YRiyATCbDorsiMDjEHeW1DXjoy0MorqiVuiwiIqI2YRixECqlOGV8kLsDskuq8fhXSaht0EtdFhER0TUxjFgQd0dbfDEjBs52SiSeL8X8TSkcYUNERF0ew4iFCfN2xtIpA6CQy7DpcA4+3s01bIiIqGtjGLFAN4V74bW/9QUgrmHzfXKOxBURERG1zuQwEh8fj4kTJ8Lf3x8ymQxbtmz5y+M3bdqEW2+9FV5eXnBxcUFcXBx27NhxvfVSGz0wJBiPjugGAPjXxmM4dK5E4oqIiIhaZnIYqaysRFRUFJYuXdqm4+Pj43Hrrbdi27ZtSEpKwqhRozBx4kQcOXLE5GLJNPPH98bYvj6o0xvw2OpEnCuqlLokIiKiq8iEG+jhKJPJsHnzZkyaNMmk8/r27YvJkydjwYIFbTpep9NBrVZDq9XCxcXlOiq1XtV1etz3eQKOXtCim6cjNj05FG6OtlKXRUREVqCtf787vc+IwWBAeXk53N3dWz2mtrYWOp2uWaPrY2+rwPLpMQhwtUdmUSUe+yoRNfUc8ktERF1Hp4eR//73v6ioqMC9997b6jGLFy+GWq02No1G04kVWh5vZzt8MWMQnFVKHDpXin9uPAqDgUN+iYioa+jUMLJmzRq89tpr2LBhA7y9vVs9bv78+dBqtcaWnZ3diVVapp6+zvjswYGwUciw9Vgu3tmeJnVJREREADoxjKxbtw6PPPIINmzYgDFjxvzlsSqVCi4uLs0a3bihYZ74z92RAIDP4jOwOuGctAURERGhk8LI2rVrMXPmTKxduxYTJkzojK+kVtwZHYjnbwsHAPz7hxP45USexBUREZG1MzmMVFRUIDk5GcnJyQCAzMxMJCcnIysrC4D4iGXatGnG49esWYNp06bhvffeQ2xsLPLy8pCXlwetVts+vwGZbNaoMNw3SAODADyz7ggOZ5VKXRIREVkxk8NIYmIioqOjER0dDQCYO3cuoqOjjcN0c3NzjcEEAD7//HM0NDRg1qxZ8PPzM7Y5c+a0069AppLJZHhzUgRG9vRCTb0BD686hLOFFVKXRUREVuqG5hnpLJxnpGNU1jZgyvL9OHpBi0A3e2x6cii8XeykLouIiCxEl51nhLoOR5USX8wYhBAPB1worcaMlYdQXlMvdVlERGRlGEasnIeTCqsfioWnky1Sc3V44usk1DUYpC6LiIisCMMIIcjDAStnDIajrQJ/nCnmpGhERNSpGEYIANAvUI1PHxgIpVyGH49exOs/pcIMuhMREZEFYBgho5vCvfDevVEAgFV/nsPS385IXBEREVkDhhFq5u/9A7BwYh8AwH9/SceaA1nXOIOIiOjGMIzQVWYO64bZo8IAAK9sScHPKbkSV0RERJaMYYRa9M/bwnH/YHGW1jnrkvHnmSKpSyIiIgvFMEItEmdp7YdxfX1RpzfgkdWJSM4uk7osIiKyQAwj1CqFXIYl9/XHsDAPVNXpMWPlQZzKK5e6LCIisjAMI/SX7GwU+PzBGPTXuKKsqh4PrjiArOIqqcsiIiILwjBC1+SoUmLVzEHo6eOMgvJaPLDiAAp0NVKXRUREFoJhhNrE1cEWXz08GEHuDsgqqcIDKw6gtLJO6rKIiMgCMIxQm3m72OGbR2Lh7axCen4Fpn1xEDourEdERDeIYYRMonF3wDePxMLd0RYpOVo8vOoQquoapC6LiIjMGMMImayHjzNWPzQYznZKHDpXise/SkJtg17qsoiIyEwxjNB1iQhQY9XMwXCwVeD300WYveYI6vUGqcsiIiIzxDBC121gsBv+Ny0Gtko5dqbm47n1yWhgICEiIhMxjNANGRrmiWUPDICNQoafjuXihW+PQW8QpC6LiIjMCMMI3bBbevngo/sHQCGXYdORHLy0KQUGBhIiImojhhFqF+MifPHBff0hlwHrE7Ox4IfjEAQGEiIiujaGEWo3d0T64717oyCTAV/vz8LrP6UykBAR0TUxjFC7ujM6EO/cFQkAWPnHOby59SQDCRER/SWGEWp39w7SYNGd/QAAK/ZlYtE2BhIiImodwwh1iCmxQXhzUgQAYPnvmXj75zQGEiIiahHDCHWYB4YE442/9wUAfBafgXe2n2IgISKiqzCMUId6MC4Er/1NDCTL9p7Ff3YwkBARUXMMI9Thpg8NwcKJfQAAn+45y0c2RETUDMMIdYqZw7oZ75B8Fp/BUTZERGTEMEKdZvrQELzR2Kl1xb5MzkNCREQAGEaokz04JNg47HflH+ew8IcTnDqeiMjKMYxQp5sSG4R3/tEPMhmwOuE8Xt7CtWyIiKwZwwhJYvKgILx7dxTkMmDtwWw8v/EoGvQGqcsiIiIJMIyQZO4eGIgP7os2rvY7Z30y6hlIiIisDsMISWpilD+WThkAG4UMW4/l4qlvDqO2QS91WURE1IlMDiPx8fGYOHEi/P39IZPJsGXLlmues2fPHgwYMAAqlQphYWFYtWrVdZRKlmpchC8+fzAGtko5dqbm49HVSaiuYyAhIrIWJoeRyspKREVFYenSpW06PjMzExMmTMCoUaOQnJyMZ599Fo888gh27NhhcrFkuUb18sYX0wfB3kaB+PRCTP/iIMpr6qUui4iIOoFMuIGJHmQyGTZv3oxJkya1esyLL76IrVu34vjx48Zt9913H8rKyrB9+/Y2fY9Op4NarYZWq4WLi8v1lktmIPFcCWauPITy2gZEBqrx5czBcHO0lbosIiK6Dm39+93hfUYSEhIwZsyYZtvGjh2LhISEVs+pra2FTqdr1sg6xIS4Y+1jQ+DmYINjF7SY/HkCCnQ1UpdFREQdqMPDSF5eHnx8fJpt8/HxgU6nQ3V1dYvnLF68GGq12tg0Gk1Hl0ldSESAGhsej4OPiwrp+RW457MEZJdUSV0WERF1kC45mmb+/PnQarXGlp2dLXVJ1Ml6+Dhj4+NDoXG3x/niKty97E+czi+XuiwiIuoAHR5GfH19kZ+f32xbfn4+XFxcYG9v3+I5KpUKLi4uzRpZnyAPB2x8fCh6eDshX1eLez5LQHJ2mdRlERFRO+vwMBIXF4ddu3Y127Zz507ExcV19FeTBfBV22HD43GI0riirKoeU5bvxx9niqQui4iI2pHJYaSiogLJyclITk4GIA7dTU5ORlZWFgDxEcu0adOMxz/xxBPIyMjACy+8gLS0NHzyySfYsGEDnnvuufb5DcjiuTnaYs0jsRge5omqOj1mrjyE7cdzpS6LiIjaiclhJDExEdHR0YiOjgYAzJ07F9HR0ViwYAEAIDc31xhMAKBbt27YunUrdu7ciaioKLz33nv43//+h7Fjx7bTr0DWwFGlxIoZMRgf4Ys6vQFPfXMYaw5kXftEIiLq8m5onpHOwnlG6BK9QcArW1Kw9qDYqXnureF4+pYwyGQyiSsjIqIrdZl5Rojak0Iuw6I7++GZW8IAAO/vTMfCH05Ab+jymZqIiFrBMEJmRyaTYe5tPfHa3/pCJgNWJ5zHM+uOcIE9IiIzxTBCZmv60BB8eF+0ccXf6V8chLaa69kQEZkbhhEyaxOj/LFq5mA4qZTYn1GCe5clIFfb8sy+RETUNTGMkNkbFuaJ9Y8PgZezCqfyy3HXJ38inbO1EhGZDYYRsgh9/dXY9ORQhHo5Ildbg7s//RMHMoqlLouIiNqAYYQshsbdAd89ORQDg92gq2nAgysO4oejF6Uui4iIroFhhCyKq4MtvnkkFuP6ipOjPbP2CD7dcxZmMJ0OEZHVYhghi2Nno8DSqQPw8PBuAIB3tqfh5S3H0aA3SFwZERG1hGGELJJCLsOrd/TBwol9IJMBaw5k4dHViaisbZC6NCIiugLDCFm0mcO6YdkDA2FnI8dvpwpxD4f+EhF1OQwjZPHG9vXF2keHwNPJFqm5Okxa+geO52ilLouIiBoxjJBViA5yw+anhqGHtxPydbW4Z1kCfk3Nl7osIiICwwhZEY27A757aihG9PBEdb0ej36ViBX7MjnShohIYgwjZFVc7GzwxYxBmBIbBEEA3vgpFS9tPo56jrQhIpIMwwhZHRuFHG9NisArE3pDJgPWHszCtBUHUVZVJ3VpRERWiWGErJJMJsMjI7pjxfQYONoqkJBRjElL/8DZwgqpSyMisjoMI2TVbunlg++eGooAV3ucK67CpKV/ID69UOqyiIisCsMIWb1evi74fvYwDAx2Q3lNA2asPIgv2LGViKjTMIwQAfB0UmHNo7G4e2AgDALw+k+pePG7Y6ht0EtdGhGRxWMYIWqkUirw7t2RePWOPpDLgA2JFzBl+QEUltdKXRoRkUVjGCG6jEwmw8PDu2HVzMFwsVMi6Xwp/vbxPhy7UCZ1aUREFothhKgFN4V7YcusYeju5YhcbQ3uXpaA75IuSF0WEZFFYhghakV3LydsmTUMo3t5o67BgH9uPIrXfjzBCdKIiNoZwwjRX3Cxs8HyaTF4ZnQPAMDKP87hwRUHUFzBfiRERO2FYYToGuRyGebeGo7PHhwIR1sF9meUYOJH7EdCRNReGEaI2mhsX1+xH4mnIy429iNZfyhL6rKIiMwewwiRCXr4OGPL7GG4tY8P6hoMePG7FMzflML5SIiIbgDDCJGJXOxs8NkDA/H8beHGhfbu/Ww/csqqpS6NiMgsMYwQXQe5XIbZt/TAqpmDoba3wdHsMtzx4e/Yy3VtiIhMxjBCdANuDvfCT08PR78ANUqr6jFj5UEs+TUdBgPXtSEiaiuGEaIbpHF3wMYn4jAlNgiCACz59TRmrDqEkso6qUsjIjILDCNE7cDORoFFd/bDe/dEwc5Gjvj0Qkz48HcknS+RujQioi6PYYSoHf1jYCA2PzUM3TzFaeQnf7Yfy+MzIAh8bENE1BqGEaJ21tvPBT/MHoY7Iv3QYBDw1raTeHR1ErRV9VKXRkTUJV1XGFm6dClCQkJgZ2eH2NhYHDx48C+PX7JkCXr27Al7e3toNBo899xzqKmpua6CicyBs50NPro/Gm/8vS9sFXL8ejIfEz76HUeySqUujYioyzE5jKxfvx5z587FwoULcfjwYURFRWHs2LEoKCho8fg1a9Zg3rx5WLhwIU6ePIkVK1Zg/fr1eOmll264eKKuTCaT4cG4EHz35FAEuTvgQmk17lmWgOXxGRxtQ0R0GZlg4sPs2NhYDBo0CB9//DEAwGAwQKPR4Omnn8a8efOuOn727Nk4efIkdu3aZdz2z3/+EwcOHMC+ffva9J06nQ5qtRparRYuLi6mlEvUJehq6jH/uxRsTckFAIzq6YX37u0Pd0dbiSsjIuo4bf37bdKdkbq6OiQlJWHMmDFNHyCXY8yYMUhISGjxnKFDhyIpKcn4KCcjIwPbtm3D7bff3ur31NbWQqfTNWtE5szFzgYfT4nGW3dGQKWU47dThRj/QTz2ZxRLXRoRkeRMCiNFRUXQ6/Xw8fFptt3Hxwd5eXktnjNlyhS8/vrrGD58OGxsbBAaGoqRI0f+5WOaxYsXQ61WG5tGozGlTKIuSSaTYWpsML6fPQyhXo7I19ViyvL9eP+XU2jQG6Quj4hIMh0+mmbPnj1YtGgRPvnkExw+fBibNm3C1q1b8cYbb7R6zvz586HVao0tOzu7o8sk6jS9fF3w49PDcc/AQBgE4MPdZzD58/24UFoldWlERJJQmnKwp6cnFAoF8vPzm23Pz8+Hr69vi+e8+uqrePDBB/HII48AAPr164fKyko89thjePnllyGXX52HVCoVVCqVKaURmRUHWyXevScKN4V74aVNKUg6X4rxH/yOt++KxIRIP6nLIyLqVCbdGbG1tcXAgQObdUY1GAzYtWsX4uLiWjynqqrqqsChUCgAgBNBkdWbGOWPbXNGIDrIFeU1DZi15jBe+PYoKmsbpC6NiKjTmPyYZu7cuVi+fDm+/PJLnDx5Ek8++SQqKysxc+ZMAMC0adMwf/584/ETJ07Ep59+inXr1iEzMxM7d+7Eq6++iokTJxpDCZE107g7YMPjcZg9KgwyGbAh8QImfPg7krPLpC6NiKhTmPSYBgAmT56MwsJCLFiwAHl5eejfvz+2b99u7NSalZXV7E7IK6+8AplMhldeeQU5OTnw8vLCxIkT8dZbb7Xfb0Fk5mwUcjw/tidG9PDE3A1Hca64Cv/49E88O7oHnhoVBoVcJnWJREQdxuR5RqTAeUbImmir6/HKluP48ehFAEBMsBvev7c/gjwcJK6MiMg0HTLPCBF1PLW9DT68rz/+b3IUnFRKJJ4vxfgP4rH+UBb7WRGRRWIYIeqCZDIZ7owOxM9zRmBwiDsq6/R48bsUPPZVEooqaqUuj4ioXTGMEHVhGncHrH1sCOaP7wUbhQw7U/Mxbkk8fjnR8iSDRETmiGGEqItTyGV4/OZQfD9rOHr6OKOoog6PfZWEf244Cm11vdTlERHdMIYRIjPRx98F388ehsdv7g6ZDPju8AWMWxKPfaeLpC6NiOiGMIwQmRE7GwXmj++NjY/HIdjDAbnaGjyw4gBe3XKcE6URkdliGCEyQzEh7vh5zgg8OCQYAPDV/vMY90E8Es5yFWAiMj8MI0RmysFWiTcmReDrh2MR4GqP7JJq3L98PxZ8z7skRGReGEaIzNzwHp7Y/uwI3D84CACwOkG8S/LnWfYlISLzwDBCZAGc7Wyw+K5+ze6STFl+AC9tTkF5DUfcEFHXxjBCZEEu3SWZEiveJVlzIAu3/V88fksrkLgyIqLWMYwQWRhnOxssurMf1j46BEHu4oibmasOYe76ZJRW1kldHhHRVRhGiCxUXKgHtj87Ag8P7waZDNh0JAdj3t+L75NzuMYNEXUpDCNEFszBVolX7+iD754cinAfJxRX1mHOumQ8tOoQcsqqpS6PiAgAwwiRVRgQ5Iafnh6BubeGw1Yhx2+nCnHb+3ux8o9M6A28S0JE0mIYIbIStko5nhndA9vmDEdMsBsq6/R47cdU3PnJHzieo5W6PCKyYgwjRFYmzNsZGx6Pw5uTIuBsp8SxC1r8fekfeGtrKqrqOFkaEXU+hhEiKySXy/DAkGDsmnszJkT6QW8QsPz3TNz6fjx+Tc2XujwisjIMI0RWzNvFDkunDMDKGYMQ4GqPnLJqPLI6EY+tTmQHVyLqNAwjRIRRvbyxc+5NeOLmUCjlMvySmo8x7+3F5/FnUa83SF0eEVk4hhEiAiAOA543vhe2PjMCg0PcUV2vx6JtaZjw4e/Yn8HVgImo4zCMEFEzPX2dsf7xIXj37ki4OdggPb8C932+H8+tT0ZBeY3U5RGRBWIYIaKryGQy3BOjwW/Pj8TU2CDIZMDmIzkY/d+9+GJfJhr46IaI2pFMMIN5oXU6HdRqNbRaLVxcXKQuh8jqHM0uw6vfH8exC+J8JD19nPHvv/VFXKiHxJURUVfW1r/fDCNE1CZ6g4B1h7Lw7o5TKKuqBwBMiPTDy7f3hr+rvcTVEVFXxDBCRB2itLIO7+08hTUHsmAQAHsbBZ4aGYpHb+oOOxuF1OURURfCMEJEHerERS3+/cMJHDpXCgAIdLPHy7f3xrgIX8hkMomrI6KugGGEiDqcIAj44ehFLN6WhjydONJmSHd3LJzYF739+L9VImvHMEJEnaaqrgHL9mbgs71nUdtggFwGTB4UhH/eFg5PJ5XU5RGRRBhGiKjTXSitwuJtadiakgsAcFIpMWtUGGYOC2F/EiIrxDBCRJI5dK4Eb/yUahwKrHG3x7xxvXF7P/YnIbImDCNEJCmDQcCW5Bz8Z/spY3+SAUGueHlCHwwMdpO4OiLqDAwjRNQlVNU14PP4DHy2NwPV9XoAwO39fPHiuF4I9nCUuDoi6kgMI0TUpRToavD+znRsSMyGQQBsFDI8MCQYT9/SA+6OtlKXR0QdgGGEiLqktDwdFm9Lw970QgCAs0qJJ0aG4qFh3WBvy06uRJakrX+/r2uhvKVLlyIkJAR2dnaIjY3FwYMH//L4srIyzJo1C35+flCpVAgPD8e2bduu56uJyMz18nXBlw8NxtcPx6KvvwvKaxvw7o5TGPnf37DuYBYX4SOyQiaHkfXr12Pu3LlYuHAhDh8+jKioKIwdOxYFBQUtHl9XV4dbb70V586dw7fffotTp05h+fLlCAgIuOHiich8De/hiR9nD8cH9/VHoJs98nW1mLcpBbcticfPKbkwg5u2RNROTH5MExsbi0GDBuHjjz8GABgMBmg0Gjz99NOYN2/eVccvW7YM7777LtLS0mBjY3NdRfIxDZFlq23Q46uE81j62xmUNi7CFxWoxovjemFomKfE1RHR9eqQxzR1dXVISkrCmDFjmj5ALseYMWOQkJDQ4jk//PAD4uLiMGvWLPj4+CAiIgKLFi2CXq9v9Xtqa2uh0+maNSKyXCqlAo+M6I74F0bhmdE94GCrwNELWkz53wE88L8DSM4uk7pEIupAJoWRoqIi6PV6+Pj4NNvu4+ODvLy8Fs/JyMjAt99+C71ej23btuHVV1/Fe++9hzfffLPV71m8eDHUarWxaTQaU8okIjPlbGeDubeGY++/RmHG0BDYKGTYd6YIk5b+gcdWJ+JUXrnUJRJRB7iuDqymMBgM8Pb2xueff46BAwdi8uTJePnll7Fs2bJWz5k/fz60Wq2xZWdnd3SZRNSFeDmr8O+/9cXuf47E3QMDIZcBv6TmY9wH8Ziz7ggyCiukLpGI2pHSlIM9PT2hUCiQn5/fbHt+fj58fX1bPMfPzw82NjZQKJqG7PXu3Rt5eXmoq6uDre3V8wuoVCqoVFxci8jaadwd8N97ovDEzd3xfztPY2tKLr5PvoifjuXizugAzBndAxp3B6nLJKIbZNKdEVtbWwwcOBC7du0ybjMYDNi1axfi4uJaPGfYsGE4c+YMDIam4Xrp6enw8/NrMYgQEV0pzNsZS6cOwE9PD8foXt7QGwR8m3QBo/67B/M3pSCnrFrqEonoBpj8mGbu3LlYvnw5vvzyS5w8eRJPPvkkKisrMXPmTADAtGnTMH/+fOPxTz75JEpKSjBnzhykp6dj69atWLRoEWbNmtV+vwURWYWIADVWzBiEzU8NxYgenmgwCFh7MAsj3/0Nr2xJwUWGEiKzZNJjGgCYPHkyCgsLsWDBAuTl5aF///7Yvn27sVNrVlYW5PKmjKPRaLBjxw4899xziIyMREBAAObMmYMXX3yx/X4LIrIq0UFu+OrhWBw6V4L3f0lHQkYxvt6fhfWHsjF5kAZPjQyDv6u91GUSURtxOngiMnv7M4rxwa+nkZBRDEBc9+aeGA2evDmUfUqIJMS1aYjI6uzPKMaSX9OxP6MEAKCUy3DXgAA8NTIMIZ5cIZioszGMEJHVOpBRjI92n8G+M0UAALkM+Hv/ADw1MhQ9fJwlro7IejCMEJHVSzpfio92n8aeU+IKwTIZMK6vL2aNCkNEgFri6ogsH8MIEVGjYxfKsPS3M9hxommOpJvDvTBrVBgGd3OXsDIiy8YwQkR0hfT8cnzy2xn8cPQiDI3/8sUEu+HJkaG4pZc3ZDKZtAUSWRiGESKiVpwvrsSyvRn4LukC6vTihIw9fZzx5MhQTIj0g42iw1fKILIKDCNERNdQoKvBin2Z+Hr/eVTWiSuJB7ja4+Hh3TB5kAaOKpOnYiKiyzCMEBG1kbaqHl/tP4dVf55DUUUdAEBtb4NpccGYFhcCL2eulUV0PRhGiIhMVFOvx3eHL2B5fAbOFVcBAGyVcvxjQAAeHt4dYd5OEldIZF4YRoiIrpPeIGBnah6W7c1AcnaZcfvoXt54ZER3DOnuzs6uRG3AMEJEdIMEQUDS+VJ8Hp+BnSfzcelfy77+Lnh4eDfcEekPWyU7uxK1hmGEiKgdZRRWYMW+THx3+AJq6sURON7OKkyLC8aU2GC4O9pKXCFR18MwQkTUAUor67DmYBa+/PMcCsprAQAqpRyT+gdg5vAQ9PLlv1FElzCMEBF1oLoGA7amXMSKfZk4nqMzbo/r7oGZw0IwurcPFHL2KyHrxjBCRNQJBEFA4vlSrPwjE9uP5xlndg10s8eDQ4IxeZAGrg58hEPWiWGEiKiT5ZRVY3XCOaw/lI2yqnoAgJ2N+AhnWlwI+vjz3y+yLgwjREQSqa7T44ejOVj153mczG16hBMT7IYH44IxLsIXKqVCwgqJOgfDCBGRxC49wvnyz3PYfjwPDY3PcDydbDF5kAb3Dw5CoJuDxFUSdRyGESKiLqRAV4N1h7Kx5kAW8nQ1AACZDBjV0xsPDAnCzeHe7PBKFodhhIioC6rXG/Braj6+PnAef5wpNm4PcLXH/YM1uCdGAx8XOwkrJGo/DCNERF1cRmEF1h7MwsakC8YOrwq5DGN6e+P+wUEY0cOLd0vIrDGMEBGZiZp6Pbal5GLtwSwcOldq3B7gao/JgzS4JyYQfmp7CSskuj4MI0REZig9vxxrD2bhu6QL0NU0AADkMuDmcC9MHhSE0b29YaPgejhkHhhGiIjMWE29Hj8fz8X6Q9nYn1Fi3O7pZIu7BgTi3phAhHk7S1gh0bUxjBARWYjMokpsSMzGxsQLKKqoNW6PDnLFvTEa3BHpB2c7GwkrJGoZwwgRkYWp1xuw51QhNiRmY3daAfSN85bY2cgxrq8v7h6owdBQD8jZ6ZW6CIYRIiILVlBegy1HcrD+UDbOFlYat/ur7XDXgEDcNSAA3b2cJKyQiGGEiMgqCIKAoxe0+DYpGz8kXzR2egXExzh3DQjExEg/LtZHkmAYISKyMjX1euxMzcemwxcQf7rI+BjHViHHLb28MSk6AKN6eXFdHOo0DCNERFasoLwGPyRfxHeHc5ot1udip8SESH/cGR2AmGA39i+hDsUwQkREAICTuTpsOZKDLck5yNc1jcYJcLXHxCh//L2/P3r78d9Wan8MI0RE1IzeIGB/RjE2Hc7BjhN5qKht6l/S08cZf+vvj4mR/gjy4ErC1D4YRoiIqFU19XrsTivAliM52HOqEHV6g3Fff40rJkb5445IPy7aRzeEYYSIiNpEW1WP7Sdy8cPRi0g4W4zGfq+QyYDBIe64I8of4yN84emkkrZQMjsMI0REZLKC8hpsOyYGk8NZZcbtchkwNNQTEyL9cFsfH3gwmFAbtPXv93WttrR06VKEhITAzs4OsbGxOHjwYJvOW7duHWQyGSZNmnQ9X9v+Ss8BucekroKIqMvwdrbDjGHdsOmpYdj34ii8dHsvRAWqYRCAfWeKMH9TCgYv2oWp/9uPbw6cR/Fl09MTXS+T74ysX78e06ZNw7JlyxAbG4slS5Zg48aNOHXqFLy9vVs979y5cxg+fDi6d+8Od3d3bNmypc3f2WF3RrY+DxxaDvhGAtEPAv3uBhzc2+/ziYgsRFZxFX5KuYhtKbk4ntM0VFguA2K7eWB8P1+M7evLPibUTIc9pomNjcWgQYPw8ccfAwAMBgM0Gg2efvppzJs3r8Vz9Ho9brrpJjz00EP4/fffUVZW1jXCyI/PAsnfAPo68b3CFuh1BxA9Feg+CpBzYiAioiudL67EtpQ8bEvJRUqO1rhdJgMGBrlhXIQYTDTuHJVj7TokjNTV1cHBwQHffvtts0ct06dPR1lZGb7//vsWz1u4cCGOHTuGzZs3Y8aMGdcMI7W1taitbbr1p9PpoNFoOqbPSFUJkLIROPIVkJfStN3ZH4iaDERNAbzC2/c7iYgsRHZJFX4+noufj+fhyGV9TACgr78LxvX1xdgIX/TwdoJMxgnWrE1bw4jSlA8tKiqCXq+Hj49Ps+0+Pj5IS0tr8Zx9+/ZhxYoVSE5ObvP3LF68GK+99poppV0/B3cg9nGx5R4FjnwDpGwAyi8C+/5PbAExQP/7gb538TEOEdFlNO4OeOymUDx2UyhytdXYfjwPO07k4WBmCU5c1OHERR3e25mObp6OuK2PD27r64NoDWd+peZMujNy8eJFBAQE4M8//0RcXJxx+wsvvIC9e/fiwIEDzY4vLy9HZGQkPvnkE4wfPx4Aut6dkZY01ALpO4DkNcDpXwBBL25X2ALh44Co+4GwMYCSC08REbWkuKIWv57Mx44T+dh3uqjZPCaeTiqM6e2NW/v4YFiYJ+xs+EjcUnWJxzTJycmIjo6GQtH0H5rBIP4HKZfLcerUKYSGhrbbL9MhKgqAYxuAo+uA/Mse49i7AxH/ACInA4Ex4sNSIiK6SkVtA/aeKsQvqXnYnVaA8stWFra3UWBED0+M6eODW3p5cy4TC9OhHVgHDx6Mjz76CIAYLoKCgjB79uyrOrDW1NTgzJkzzba98sorKC8vxwcffIDw8HDY2l777kKXmWckL0UMJSkbgYr8pu1u3cRQEnkv4HHtcEVEZK3qGgw4kFmMX1Pz8evJAuSUVRv3yWTi7K+je3ljdG8f9PJ1Zj8TM9dhYWT9+vWYPn06PvvsMwwePBhLlizBhg0bkJaWBh8fH0ybNg0BAQFYvHhxi+e35THN9f4ynUbfAGTuEe+YnPwRqK9q2uc/AOh3DxBxF+DsK1mJRERdnSAISM3V4dfUAvx6Mr/ZyBxAXMhvVC8v3NLLG0ND+TjHHHVIB1YAmDx5MgoLC7FgwQLk5eWhf//+2L59u7FTa1ZWFuTy65pLzXwolGKfkbAxQG0FcGqbeMckYw9w8bDYfnkZCBkhzl3SeyJg7yZ11UREXYpMJkNffzX6+qsxZ0wP5GlrsDutALtO5mPfmSLklFXj6/1Z+Hp/FlRKOYaGeuCWXt4Y2dObw4YtDKeDb08VhUDqFvExTvZlnXnlNmJw6Xe32AFW5SRZiURE5qC6To+EjCLsTivA7pMFuKitabY/1MsRI3t6Y1RPbwzq5gaVkndNuiKuTSO10nPA8e+A45uA/ONN25X2QPht4jDhHrcBtkz3RER/RRAEnMovx+60Auw5VYik86XQG5r+dNnbKDA01AM39/TCzeFeCPZwlLBauhzDSFdSkAYc/1YMJyUZTdttHIGe44A+k4AetwI29pKVSERkLrTV9fjjTBF+SyvAnvRCFJY3Xx8nxMMBI3p44aZwL8SFesBJZXKPBGonDCNdkSCIE6ud2ASc2AyUZTXts3EEwscCfScBYbfyjgkRURsIgoCTueXYm16IvekFSDxXiobL7poo5TIMCHbDiDBPjAj3Qr8ANRSccK3TMIx0dYIA5CSJoST1e0Cb3bTPxkG8U9Ln7+KjHJWzdHUSEZmRitoGJJwtRnx6IeJPF+J8cVWz/Wp7GwwN9cDwHp4YEeaFIA/+P34diWHEnAgCkHMYSG0MJpffMVGogLDR4oic8HGcjp6IyATniysRf7oI+04X4s+zxc0mXAOAQDd7DA/zxLAwT8SFenDStXbGMGKuBAHITQZSfxCDScnZpn0yBRAyXAwmve4AXPwkK5OIyNw06A04lqPFvtNF2HemCEeySlGvb/4nsJevM4aGemJoqAcGd3eHi52NRNVaBoYRSyAIQEGqGEzSfmo+KgcAAgYCvSaIwcSrpzQ1EhGZqcraBhw8V4I/GsNJWl55s/1yGdAvQI0hoR6I6+6BQSHucGRnWJMwjFii4rNiKDn5I3DhUPN9HmFAz9vFcBI4CJBzzD0RkSmKK2qRkFGMP88W488zRTh3RX8TpVyGyEA1Yrt7YEh3D8QEuzGcXAPDiKUrzwNO/QykbQUy9wL6uqZ9Dp5i/5Ke44DuozjJGhHRdbhYVo39GcVIOFuMhIxiXCitbrZfIZehX4Aasd3dMaSbBwaGuPGxzhUYRqxJjQ4486sYTk7vAGouW99BoQK6jRDDSfg4wFUjXZ1ERGYsu6QKBzJLsD+jGPtbCCdyGdDH3wWDQzwwuJsbYkLcrb5DLMOItdLXA1kJQNo2IP1ncSbYy/lEiMOFw8fycQ4R0Q3ILqnCwcwSHMgsxsHMkqse6wBAdy9HDA5xR0yIOwaFuCHI3cGqViJmGCGxA2zhKSB9u9iyDwCCoWm/vbu4Zk6P28Thwxw2TER03fK0NTh4rgSHMktwMLMEp/LLrzrGy1mFQSFuiAl2x8BgN/Txd4GNwnIXl2UYoatVlYiPc9J3AGd2Nn+cAxkQGNMYTMYAfv0BS199mYioA5VV1SHxXCkOnSvBoXMlSMnRXjWU2N5GgSiN2hhOooNc4epgK1HF7Y9hhP6avgG4cBA4/QtweufVw4YdPIDQ0eJMsKG3AI6e0tRJRGQhaur1OJpdhsTzpUg8V4Kk86XQXTEJGwCEeTthQJArBgS5YUCwG8K8nCA30ynsGUbINNoc8a7J6V+AjL1A3eW3F2WAX5T4KCd0NKAZDCjYY5yI6EYYDALOFFYg6XwpEs+V4khWKTKKKq86zlmlRJTGFdFBYuuvcYO7o3ncPWEYoevXUCfeNTnzK3D6VyA/pfl+W2dxhE7oLeLQYY9QwIo6ZBERdZSSyjocySpF0vlSHMkqw9ELZaiq0191XLCHA/prXNFf44oojSv6+LnAzqbrDUhgGKH2U54HnP0NOLsLOLsbqCpuvl8dBISOFINJt5sBRw9JyiQisjQNegPS8ytwJLsUh8+XITm7FGcLr757YqOQoZevC6I0akQGuiIq0BVh3k6Sr1DMMEIdw2AA8o42hpPd4gidyydcAwDffkD3kUC3kUBwHGDrKEGhRESWSVtdj6PZZUjOLjO+FlfWXXWcg60CEf5qRAaqEalxRWSAGsEenTu0mGGEOkddJXD+TzGcZOwBCk403y+3Eecz6X6zeNckYCCgNI9nnURE5kAQBOSUVeNothbJ2aU4ekGL4znaFh/vuNgp0S9QjYgANfo1to6c+4RhhKRRUQBkxovhJHMvoM1uvt/GAQgaAoSMALrdJA4hVnBtByKi9qQ3CDhbWIGj2WVIydHi2AUtUnN1qGswXHWsi50SEQFqPHZTd4zs6d2udTCMkPQEASjNFEfnZMaLraqo+TG2zuKjnJDhYvONYjghIuoA9XoD0vPLceyCFik54t2TtNxy1OnFgLLsgQEYF+HXrt/JMEJdjyAABSfFUHLud+DcPqCmrPkxKhdAEwuEDAOChwP+/TmMmIiog9Q1GHC6oBzHc7S4pZcPvJzbdy0dhhHq+gx6cbK1c/sa2x9Arbb5MTaO4rwmwUPFFjAQsLGXpl4iIjIJwwiZH4MeyEsRO8Se/0Ns1aXNj1HYAv4DxEc7QXFiULF3k6ZeIiL6SwwjZP4MBqDwZGM4aWwVeVcf591H7BSrGSK+ugZxEjYioi6AYYQsjyAAJRlAVoLYzicAJWevPs7ZT+x3EjREvHPiG8l+J0REEmAYIetQUQBk7Rdb9n4g9yhguGLhKaU9EDBADCaBg8V5T5y8pKmXiMiKMIyQdaqrAi4ebgwnB8U1dq7sdwIAbiFNwSQwBvCJ4GRsRETtjGGECBD7nRSfEUNJ9gEg+xBQmAbgiv/sFSpxGHFADBA4UHxl3xMiohvCMELUmhotcCERuHCo6fXK+U4AwNFLHEocEAMERIujeBzcO71cIiJzxTBC1FaCABSfBXIuCyj5x6/uewIAbt3E/if+AwD/aMAvClA5dX7NRERmgGGE6EbU1wB5x8RgcvEwkJMkjuS5igzw6ikGE/9oca0d336ArUNnV0xE1OUwjBC1t+pS4OIRMZhcTBZ/1uVcfZxMDnj1EoOJX5TYfPvxDgoRWR2GEaLOUJ4v3jm5mAzkJosBpSK/hQNlgGcPcc4TvyjAL1L8mX1QiMiCMYwQSUWX2xhMksV5T3KTgfLclo9VB4l3TXz7NQaUfoBaw1E8RGQROjSMLF26FO+++y7y8vIQFRWFjz76CIMHD27x2OXLl2P16tU4fvw4AGDgwIFYtGhRq8e3hGGEzF55vtgHJfeo2PKOAaXnWj7WTg349AN8I8Rw4hMhPvaxsevUkomIblSHhZH169dj2rRpWLZsGWJjY7FkyRJs3LgRp06dgre391XHT506FcOGDcPQoUNhZ2eHd955B5s3b8aJEycQEBDQrr8MkVmpLhNH7eSlNLZjQEEaYKi/+liZQnzM49O3sUWIa/KoA3kXhYi6rA4LI7GxsRg0aBA+/vhjAIDBYIBGo8HTTz+NefPmXfN8vV4PNzc3fPzxx5g2bVqbvpNhhKxGQx1QdArIawwp+Sniz9UlLR+vUgM+fcRg4tMH8O4LePcG7F07tWwiopa09e+30pQPraurQ1JSEubPn2/cJpfLMWbMGCQkJLTpM6qqqlBfXw9399Y77tXW1qK2ttb4XqfTmVImkflS2jb1IcH94jZBAMrzxLso+cfFcFKQChSlA7XapoUDL+cSIIYS796A16XXXhxyTERdkklhpKioCHq9Hj4+Ps22+/j4IC0trU2f8eKLL8Lf3x9jxoxp9ZjFixfjtddeM6U0IsslkwEufmLrcWvT9oY6MZDknxDDSUEqkJ8K6C6IQ451OcCZXy//IHGKe+8+gHcvMaR49QQ8wxlSiEhSJoWRG/X2229j3bp12LNnD+zsWu+MN3/+fMydO9f4XqfTQaPRdEaJROZDadvYyTWi+fbqMqDgJFB4UuyDUpAqvq8qAsrOiy3958tOaAwpXr3EcOLVE/DsCXiFi51piYg6mElhxNPTEwqFAvn5zedRyM/Ph6+v71+e+9///hdvv/02fv31V0RGRv7lsSqVCiqVypTSiOgSe1cgOE5sl6ssagwpac1fq0uaQsrpHc3PcfIVQ4ln4x0Uzx5iWHH2Y8dZImo3JoURW1tbDBw4ELt27cKkSZMAiB1Yd+3ahdmzZ7d63n/+8x+89dZb2LFjB2JiYm6oYCK6To6eQLcRYrtcZZEYTArTgMJTYitKF+dGqcgTW2Z883NsnQCPMDGceIY3/eweykc+RGQykx/TzJ07F9OnT0dMTAwGDx6MJUuWoLKyEjNnzgQATJs2DQEBAVi8eDEA4J133sGCBQuwZs0ahISEIC8vDwDg5OQEJydOj00kOUdPwHE4EDK8+fYaLVCYLgaTonSg6LQ40qckE6irECdzy02++vPUGjGcNGuh4qMguaIzfiMiMjMmh5HJkyejsLAQCxYsQF5eHvr374/t27cbO7VmZWVBLpcbj//0009RV1eHu+++u9nnLFy4EP/+979vrHoi6jh2akAzSGyXa6gDSjMbw0ljSCk+Lb7WlAHabLFl/Nb8PIUt4BYi3j3xaGzuoYB7d3H0z2X/bhCRdeF08ETUPgQBqCppCiYlZ4HiM0DRGXHFY31t6+cqVIB7t8Zw0q2xdW8MKoGAolP72hNRO+mQeUaIiFolkwGOHmILGtJ8n0EPaC80BpSzYjgpbgwppefEoHKp38qV5ErxEY9bY0i5/NUtGLB17JRfj4g6DsMIEXU8uUIMDm7BQOgtzffpG8THOiVnxf4oJZliSLk8qFx6f7aFz3byER//uAaLr27BTe9d/NlPhcgMMIwQkbQUyqZHM1cyGIDyi2JAKc1sei09J/5cUwZU5Ist+8DV58ttxPV73IIbw0rjq2uQ+OrkzSHKRF0AwwgRdV1yuRgm1IFXD0kGgOpSMZiUnm98vaxps8VFB0sbA0xLlHbi6B9XTWNACQLUQeJ7tQZw9uWdFaJOwDBCRObL3k1s/tFX7zPoAd1FcTK30vNAWVbj5G5ZYtPlAA01Yofb4tMtf77cRnzU4xrUGIo0TeFIrQHUAeyzQtQOGEaIyDLJFY13PDRXz6ECAPp6MZBcCidlWUBZ47DksvNikDHUN81O2xp7dzGUqDXiEGXjz/7iexd/QGHTcb8nkQVgGCEi66SwaezwGtLyfoNenIVWe6EppGizxfeXttWVi9PpV5cAeSmtfJFM7GTr4i8GlUsBxSVAnFbfxV9sSi6BQdaLYYSIqCVyRdMjmSuHKl9SoxVDiS5HDCiXXrU5jasnXwT0dU3T6l883Pr3OXiKKzM7+1/x6tcUWuzd2OGWLBLDCBHR9bJTA77qq1dOvsRgAKqKm4KJ7qIYWHQXxcBS3ritoUZcVbmq6C/usECcHM7ZtzGgXPnqI746+Yh1MbSQGWEYISLqKHI54OQltpY62QLizLXVpY0hJbcxoFz+2tiqisU5V67VhwUAlPbisGVnXzGcNHv1bdrn4MHRQtQlMIwQEUlJJgMc3MXm26/14xpqG4NJfuNrXlNQKc8T51opzxUfHTVUty20yOSAo5cYTpx8AEfvpp+dGn++tI2PiKgDMYwQEZkDpeqvO9xeUl99WTi54rUiXwwzFXlAZREgGJq24y8eDwHitPyOXk3hpaWfjc2TI4jIJAwjRESWxMa+9RltL6dvEPuoVOQDFYWNnWwLGls+UFnYuK9AnOnW0NB0J6Yt7FwvCyce4quDpxhUHD3Fnx08Gn/2YHixcgwjRETWSKFs7Pzqe+1jG+rEcFJZIAaXysbQUlkovlYViXdaLv0sGMQAU1PW+oRyV7JTN4UVB4+mduV7B3fxVeXCx0YWhGGEiIj+mtK2cTK3gGsfazCIHXKrihoDTKEYVCob318KLpWNo4eqSgAIYl+XGq24YGJbyJViKLF3b+pzY3+NVztXMYRRl8P/qxARUfuRyxsfy3gAXj2vfbxBL4aXyiJxxFBV42tlcVNYqSq+rJUA9ZXiYyNjfxcTqNSAQ+MyAvbuTUsKtNhcxVc7VzGQUYdhGCEiIunIFU39SNqqvkac9fbygFJV3HhHpvF9dcllr6VArVY8t1YrttJzptVp49g8nNg3NrvLXxv32akbt6kZZNqIYYSIiMyLjR1g0ziNflvpG8Q+LJcCSnVZ42tpY4gpEfdfel9d2jzE1FeKTZdjer1K+8vCyV80lUvL723sTP9OM8MwQkRElk+hNP0ODCA+RqrRXhZUylr4+YrXGi1QrW0KMg3VQHl120ciXVW77WVBxUX8WeV8WWBxuezVuXH/Fe9tHbt0h1+GESIiotbIFU0dZE1l0AO1OjGk1OoaQ0pZY2DRNW27tL1WJ26/tK1WB0AQ1ze6tFzA9ZLJAVvnxnByeXNqCiyR97Y+U3AHYxghIiLqCHJFU2fY62EwAHUVTcHEGGB04l2XZu/Lr/hZK77W6ABBLw63rr3sbk1LAmMYRoiIiOgycrn4qMXO5fo/QxCA+iqgtqIpsNTqLnt/aVs54NW7/Wo3EcMIERGRpZLJxP4ito7iys5dlFzqAoiIiMi6MYwQERGRpBhGiIiISFIMI0RERCQphhEiIiKSFMMIERERSYphhIiIiCTFMEJERESSYhghIiIiSTGMEBERkaQYRoiIiEhSDCNEREQkKYYRIiIikpRZrNorCAIAQKfTSVwJERERtdWlv9uX/o63xizCSHl5OQBAo9FIXAkRERGZqry8HGq1utX9MuFacaULMBgMuHjxIpydnSGTydrtc3U6HTQaDbKzs+Hi4tJun0tX47XuXLzenYfXuvPwWnee9rrWgiCgvLwc/v7+kMtb7xliFndG5HI5AgMDO+zzXVxc+B92J+G17ly83p2H17rz8Fp3nva41n91R+QSdmAlIiIiSTGMEBERkaSsOoyoVCosXLgQKpVK6lIsHq915+L17jy81p2H17rzdPa1NosOrERERGS5rPrOCBEREUmPYYSIiIgkxTBCREREkmIYISIiIklZdRhZunQpQkJCYGdnh9jYWBw8eFDqksze4sWLMWjQIDg7O8Pb2xuTJk3CqVOnmh1TU1ODWbNmwcPDA05OTvjHP/6B/Px8iSq2HG+//TZkMhmeffZZ4zZe6/aTk5ODBx54AB4eHrC3t0e/fv2QmJho3C8IAhYsWAA/Pz/Y29tjzJgxOH36tIQVmye9Xo9XX30V3bp1g729PUJDQ/HGG280W9uE1/r6xMfHY+LEifD394dMJsOWLVua7W/LdS0pKcHUqVPh4uICV1dXPPzww6ioqLjx4gQrtW7dOsHW1lb44osvhBMnTgiPPvqo4OrqKuTn50tdmlkbO3assHLlSuH48eNCcnKycPvttwtBQUFCRUWF8ZgnnnhC0Gg0wq5du4TExERhyJAhwtChQyWs2vwdPHhQCAkJESIjI4U5c+YYt/Nat4+SkhIhODhYmDFjhnDgwAEhIyND2LFjh3DmzBnjMW+//bagVquFLVu2CEePHhX+9re/Cd26dROqq6slrNz8vPXWW4KHh4fw008/CZmZmcLGjRsFJycn4YMPPjAew2t9fbZt2ya8/PLLwqZNmwQAwubNm5vtb8t1HTdunBAVFSXs379f+P3334WwsDDh/vvvv+HarDaMDB48WJg1a5bxvV6vF/z9/YXFixdLWJXlKSgoEAAIe/fuFQRBEMrKygQbGxth48aNxmNOnjwpABASEhKkKtOslZeXCz169BB27twp3HzzzcYwwmvdfl588UVh+PDhre43GAyCr6+v8O677xq3lZWVCSqVSli7dm1nlGgxJkyYIDz00EPNtt11113C1KlTBUHgtW4vV4aRtlzX1NRUAYBw6NAh4zE///yzIJPJhJycnBuqxyof09TV1SEpKQljxowxbpPL5RgzZgwSEhIkrMzyaLVaAIC7uzsAICkpCfX19c2ufa9evRAUFMRrf51mzZqFCRMmNLumAK91e/rhhx8QExODe+65B97e3oiOjsby5cuN+zMzM5GXl9fsWqvVasTGxvJam2jo0KHYtWsX0tPTAQBHjx7Fvn37MH78eAC81h2lLdc1ISEBrq6uiImJMR4zZswYyOVyHDhw4Ia+3ywWymtvRUVF0Ov18PHxabbdx8cHaWlpElVleQwGA5599lkMGzYMERERAIC8vDzY2trC1dW12bE+Pj7Iy8uToErztm7dOhw+fBiHDh26ah+vdfvJyMjAp59+irlz5+Kll17CoUOH8Mwzz8DW1hbTp083Xs+W/k3htTbNvHnzoNPp0KtXLygUCuj1erz11luYOnUqAPBad5C2XNe8vDx4e3s3269UKuHu7n7D194qwwh1jlmzZuH48ePYt2+f1KVYpOzsbMyZMwc7d+6EnZ2d1OVYNIPBgJiYGCxatAgAEB0djePHj2PZsmWYPn26xNVZlg0bNuCbb77BmjVr0LdvXyQnJ+PZZ5+Fv78/r7UFs8rHNJ6enlAoFFeNKsjPz4evr69EVVmW2bNn46effsJvv/2GwMBA43ZfX1/U1dWhrKys2fG89qZLSkpCQUEBBgwYAKVSCaVSib179+LDDz+EUqmEj48Pr3U78fPzQ58+fZpt6927N7KysgDAeD35b8qN+9e//oV58+bhvvvuQ79+/fDggw/iueeew+LFiwHwWneUtlxXX19fFBQUNNvf0NCAkpKSG772VhlGbG1tMXDgQOzatcu4zWAwYNeuXYiLi5OwMvMnCAJmz56NzZs3Y/fu3ejWrVuz/QMHDoSNjU2za3/q1ClkZWXx2pto9OjRSElJQXJysrHFxMRg6tSpxp95rdvHsGHDrhqinp6ejuDgYABAt27d4Ovr2+xa63Q6HDhwgNfaRFVVVZDLm/9pUigUMBgMAHitO0pbrmtcXBzKysqQlJRkPGb37t0wGAyIjY29sQJuqPurGVu3bp2gUqmEVatWCampqcJjjz0muLq6Cnl5eVKXZtaefPJJQa1WC3v27BFyc3ONraqqynjME088IQQFBQm7d+8WEhMThbi4OCEuLk7Cqi3H5aNpBIHXur0cPHhQUCqVwltvvSWcPn1a+OabbwQHBwfh66+/Nh7z9ttvC66ursL3338vHDt2TPj73//O4abXYfr06UJAQIBxaO+mTZsET09P4YUXXjAew2t9fcrLy4UjR44IR44cEQAI77//vnDkyBHh/PnzgiC07bqOGzdOiI6OFg4cOCDs27dP6NGjB4f23qiPPvpICAoKEmxtbYXBgwcL+/fvl7okswegxbZy5UrjMdXV1cJTTz0luLm5CQ4ODsKdd94p5ObmSle0BbkyjPBat58ff/xRiIiIEFQqldCrVy/h888/b7bfYDAIr776quDj4yOoVCph9OjRwqlTpySq1nzpdDphzpw5QlBQkGBnZyd0795dePnll4Xa2lrjMbzW1+e3335r8d/n6dOnC4LQtutaXFws3H///YKTk5Pg4uIizJw5UygvL7/h2mSCcNm0dkRERESdzCr7jBAREVHXwTBCREREkmIYISIiIkkxjBAREZGkGEaIiIhIUgwjREREJCmGESIiIpIUwwgRERFJimGEiIiIJMUwQkRERJJiGCEiIiJJMYwQERGRpP4fXyz8mtVRzMkAAAAASUVORK5CYII=\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "0uFch29Bvgcl"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}